{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Agent\n",
    "- 作者： [chaofa用代码打点酱油](https://bruceyuan.com/)，全平台同名\n",
    "    - [公众号](https://bruceyuan.com/llms-zero-to-hero/chaofa-wechat-official-account.png)/视频号/[YouTube](https://www.youtube.com/@bbruceyuan)/小红书/Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function Call 的基础知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "key_value_env = dotenv_values(\".env\")\n",
    "# print(key_value_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='0de36954-c060-40cc-ba53-66cf1b0785c7', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0_e7d57cac-d0e1-4069-9ab9-82d12a7b372c', function=Function(arguments='{\"location\":\"深圳\",\"country\":\"中国\"}', name='get_weather'), type='function', index=0)]))], created=1744549828, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3d5141a69a_prod0225', usage=CompletionUsage(completion_tokens=23, prompt_tokens=158, total_tokens=181, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=158))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    api_key=key_value_env[\"API_KEY\"]\n",
    ")\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"获取当前城市的天气信息\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"城市名字 e.g. 北京\"\n",
    "                },\n",
    "                \"country\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"国家名字 e.g. 中国\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\", \"coutry\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        }   \n",
    "    }\n",
    "},\n",
    "]\n",
    "\n",
    "\n",
    "messages =[{\"role\": \"user\", \"content\": \"深圳的天气怎么样?\"}]\n",
    "\n",
    "# step 1: 获取 function call 的结果\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\", # auto, required, none\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_0_e7d57cac-d0e1-4069-9ab9-82d12a7b372c', function=Function(arguments='{\"location\":\"深圳\",\"country\":\"中国\"}', name='get_weather'), type='function', index=0)]\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather\n",
      "{'location': '深圳', 'country': '中国'}\n"
     ]
    }
   ],
   "source": [
    "## step3: 执行 function call\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "tool_name = tool_call.function.name\n",
    "tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "print(tool_name, )\n",
    "print(tool_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_0_e7d57cac-d0e1-4069-9ab9-82d12a7b372c\n"
     ]
    }
   ],
   "source": [
    "print(tool_call.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深圳 的天气是：晴天\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_weather(location, country):\n",
    "    return (f\"{location} 的天气是：晴天\")\n",
    "\n",
    "function_call_result=get_weather(tool_args[\"location\"], tool_args[\"country\"])\n",
    "print(function_call_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: 将 function call 的结果返回给 LLM\n",
    "mesages = messages.append(response.choices[0].message)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": function_call_result,\n",
    "    \"tool_call_id\": tool_call.id\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='7fadab18-4364-4db8-9a55-44ea66f43c96', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='深圳目前是晴天，适合外出活动。如果需要更详细的天气信息，比如温度或湿度，可以告诉我哦！', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1744549833, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3d5141a69a_prod0225', usage=CompletionUsage(completion_tokens=24, prompt_tokens=43, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=43))\n",
      "深圳目前是晴天，适合外出活动。如果需要更详细的天气信息，比如温度或湿度，可以告诉我哦！\n"
     ]
    }
   ],
   "source": [
    "## step5: 再次调用 LLM\n",
    "\n",
    "\n",
    "res = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(res)\n",
    "print(res.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练一个自己的 function Call 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n这里主要讲解如何处理数据，\\nSFT 训练由于前期的视频讲过，因此不过多赘述，可以参考\\n视频：https://www.bilibili.com/video/BV1NM1tY3Eu5/\\n代码： https://github.com/bbruceyuan/Hands-On-Large-Language-Models-CN/tree/master/chapter12\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "这里主要讲解如何处理数据，\n",
    "SFT 训练由于前期的视频讲过，因此不过多赘述，可以参考\n",
    "视频：https://www.bilibili.com/video/BV1NM1tY3Eu5/\n",
    "代码： https://github.com/bbruceyuan/Hands-On-Large-Language-Models-CN/tree/master/chapter12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# 这是 aistackdc 用于 Github/huggingface 下载加速的方式\n",
    "result = subprocess.run('bash -c \"source /etc/network/turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "dataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 备注：这里的 model \n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] | trim + '<|im_end|>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\"\n",
    "\n",
    "\n",
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    first_message = messages[0]\n",
    "\n",
    "    # Instead of adding a system message, we merge the content into the first user message\n",
    "    if first_message[\"role\"] == \"system\":\n",
    "        system_message_content = first_message[\"content\"]\n",
    "        # Merge system content with the first user message\n",
    "        messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
    "        # Remove the system message from the conversation\n",
    "        messages.pop(0)\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversations\", \"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Hi, can you tell me the current stock price of Apple?',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'},\n",
       "  {'content': \"That's great. Can you also tell me the stock price for Microsoft?\",\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Microsoft is $210.22.',\n",
       "   'role': 'model'},\n",
       "  {'content': 'Thank you for the information.', 'role': 'human'},\n",
       "  {'content': \"You're welcome! If you have any other questions, feel free to ask.\",\n",
       "   'role': 'model'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bc3331bbeb4b9f93584ed62e84770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 改成和 qwen 对应 assistant / user 的格式\n",
    "def convert_model_to_assistant(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"model\":\n",
    "            message[\"role\"] = \"assistant\"\n",
    "        if message[\"role\"] == \"human\":\n",
    "            message[\"role\"] = \"user\"\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(convert_model_to_assistant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Hi, can you tell me the current stock price of Apple?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Apple is $150.75.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"That's great. Can you also tell me the stock price for Microsoft?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Microsoft is $210.22.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Thank you for the information.', 'role': 'user'},\n",
       "  {'content': \"You're welcome! If you have any other questions, feel free to ask.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3213\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 357\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Can you get me the latest news headlines for the United States?<|im_end|>\n",
      "<|im_start|>model\n",
      "<think>Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n",
      "\n",
      "Looking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n",
      "\n",
      "Since the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n",
      "\n",
      "I should make sure to format the response correctly as a JSON object with the function name and arguments.\n",
      "</think><tool_call>\n",
      "{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>tool\n",
      "<tool_response>\n",
      "{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>model\n",
      "Here are the latest news headlines for the United States:\n",
      "1. US President signs new bill into law\n",
      "2. Major earthquake hits California\n",
      "3. NASA announces new Mars mission\n",
      "4. Tech giant releases new smartphone\n",
      "5. US economy shows signs of recovery<|im_end|>\n",
      "<|im_start|>human\n",
      "That's interesting. Can you get me the news for France now?<|im_end|>\n",
      "<|im_start|>model\n",
      "<think>Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n",
      "</think><tool_call>\n",
      "{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>tool\n",
      "<tool_response>\n",
      "{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>model\n",
      "Here are the latest news headlines for France:\n",
      "1. French President announces new environmental policy\n",
      "2. Paris Fashion Week highlights\n",
      "3. France wins World Cup qualifier\n",
      "4. New culinary trend sweeps across France\n",
      "5. French tech startup raises millions in funding<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call=\"<tool_call>\"\n",
    "    eotool_call=\"</tool_call>\"\n",
    "    tool_response=\"<tool_reponse>\"\n",
    "    eotool_response=\"</tool_reponse>\"\n",
    "    pad_token = \"<|endoftext|>\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "        additional_special_tokens=ChatmlSpecialTokens.list()\n",
    "    )\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] | trim + '<|im_end|>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们得到了可以使用的 训练数据，接下来就按照：https://github.com/bbruceyuan/Hands-On-Large-Language-Models-CN/tree/master/chapter12 中的代码，就可以做 SFT 训练，然后模型就可以调用工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user: input\n",
    "# model: 一段话 + tool_call\n",
    "# role: tool + tool_result\n",
    "# model: 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: 用户想要了解深圳的天气情况，我需要调用天气查询功能来获取深圳的天气信息。\n",
      "\n",
      "Action: get_weather\n",
      "Action Input: {\"location\": \"深圳\", \"country\": \"中国\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare system message with tool information\n",
    "system_message = f\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{json.dumps(tools)}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be function name\n",
    "Action Input: the input to the action. eg. {{\"param1\": \"value1\"}}\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\"\"\"\n",
    "\n",
    "react_messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "react_messages.append({\"role\": \"user\", \"content\": \"帮我查看一下深圳的天气怎么样？\"})\n",
    "        \n",
    "# Call LLM API\n",
    "completion = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=react_messages,\n",
    "        temperature=0.3,\n",
    "        stop=\"Observation:\"\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
