
## ğŸ“‹ è„šæœ¬æ¦‚è§ˆ

è¿™ä¸ªè„šæœ¬å®ç°äº†åŸºäº Transformers åº“çš„åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰æ¨¡å‹è®­ç»ƒï¼Œä¸“é—¨ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚å®ƒæ”¯æŒå¤šç§ç¿»è¯‘æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬ T5ã€BARTã€mBARTã€MarianMT ç­‰ã€‚

## ï¿½ï¿½ï¸ è„šæœ¬ç»“æ„åˆ†æ

### **1. å¯¼å…¥å’Œä¾èµ–**
```python
import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import load_dataset

import transformers
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    HfArgumentParser,
    # ... å…¶ä»–å¯¼å…¥
)
```

**æ ¸å¿ƒç»„ä»¶**ï¼š
- **dataclasses**ï¼šç”¨äºå®šä¹‰å‘½ä»¤è¡Œå‚æ•°çš„æ•°æ®ç±»
- **transformers**ï¼šHugging Face çš„æ ¸å¿ƒåº“
- **datasets**ï¼šæ•°æ®é›†åŠ è½½å’Œå¤„ç†
- **evaluate**ï¼šè¯„ä¼°æŒ‡æ ‡è®¡ç®—

### **2. å‚æ•°å®šä¹‰**

#### **ModelArguments ç±»**
```python
@dataclass
class ModelArguments:
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(...)
    tokenizer_name: Optional[str] = field(...)
    cache_dir: Optional[str] = field(...)
    use_fast_tokenizer: bool = field(default=True, ...)
    # ... å…¶ä»–å‚æ•°
```

**ä¸»è¦å‚æ•°**ï¼š
- `model_name_or_path`ï¼šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æˆ–æ ‡è¯†ç¬¦
- `config_name`ï¼šæ¨¡å‹é…ç½®æ–‡ä»¶
- `tokenizer_name`ï¼šåˆ†è¯å™¨åç§°
- `cache_dir`ï¼šæ¨¡å‹ç¼“å­˜ç›®å½•
- `use_fast_tokenizer`ï¼šæ˜¯å¦ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨

#### **DataTrainingArguments ç±»**
```python
@dataclass
class DataTrainingArguments:
    source_lang: str = field(default=None, metadata={"help": "Source language id for translation."})
    target_lang: str = field(default=None, metadata={"help": "Target language id for translation."})
    dataset_name: Optional[str] = field(...)
    train_file: Optional[str] = field(...)
    validation_file: Optional[str] = field(...)
    max_source_length: Optional[int] = field(default=1024, ...)
    max_target_length: Optional[int] = field(default=128, ...)
    # ... å…¶ä»–å‚æ•°
```

**å…³é”®å‚æ•°**ï¼š
- `source_lang`/`target_lang`ï¼šæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
- `dataset_name`ï¼šæ•°æ®é›†åç§°
- `max_source_length`ï¼šè¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦
- `max_target_length`ï¼šç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦
- `source_prefix`ï¼šT5æ¨¡å‹çš„å‰ç¼€ï¼ˆå¦‚"translate English to German: "ï¼‰

### **3. ä¸»å‡½æ•°æµç¨‹**

#### **A. å‚æ•°è§£æå’Œåˆå§‹åŒ–**
```python
def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(...)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(training_args.seed)
```

#### **B. æ•°æ®é›†åŠ è½½**
```python
if data_args.dataset_name is not None:
    # ä» Hugging Face Hub ä¸‹è½½æ•°æ®é›†
    raw_datasets = load_dataset(
        data_args.dataset_name,
        data_args.dataset_config_name,
        cache_dir=model_args.cache_dir,
        token=model_args.token,
    )
else:
    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†
    data_files = {}
    if data_args.train_file is not None:
        data_files["train"] = data_args.train_file
    # ... å¤„ç†éªŒè¯å’Œæµ‹è¯•æ–‡ä»¶
    raw_datasets = load_dataset(builder_name, data_files=data_files, ...)
```

**æ”¯æŒçš„æ•°æ®æ ¼å¼**ï¼š
- Hugging Face Hub ä¸Šçš„å…¬å¼€æ•°æ®é›†
- æœ¬åœ° JSON/JSONL æ–‡ä»¶
- è‡ªå®šä¹‰æ ¼å¼çš„æ•°æ®

#### **C. æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½**
```python
# åŠ è½½é…ç½®
config = AutoConfig.from_pretrained(...)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(...)

# åŠ è½½æ¨¡å‹
model = AutoModelForSeq2SeqLM.from_pretrained(...)

# è°ƒæ•´è¯åµŒå…¥å¤§å°
embedding_size = model.get_input_embeddings().weight.shape[0]
if len(tokenizer) > embedding_size:
    model.resize_token_embeddings(len(tokenizer))
```

#### **D. å¤šè¯­è¨€æ¨¡å‹ç‰¹æ®Šå¤„ç†**
```python
# å¤šè¯­è¨€åˆ†è¯å™¨éœ€è¦è®¾ç½®æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast, M2M100Tokenizer]

if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):
    tokenizer.src_lang = data_args.source_lang
    tokenizer.tgt_lang = data_args.target_lang
    
    # è®¾ç½®å¼ºåˆ¶å¼€å§‹æ ‡è®°
    forced_bos_token_id = tokenizer.lang_code_to_id[data_args.forced_bos_token] if data_args.forced_bos_token else None
    model.config.forced_bos_token_id = forced_bos_token_id
```

### **4. æ•°æ®é¢„å¤„ç†**

#### **é¢„å¤„ç†å‡½æ•°**
```python
def preprocess_function(examples):
    # æå–æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ–‡æœ¬
    inputs = [ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    
    # æ·»åŠ å‰ç¼€ï¼ˆT5æ¨¡å‹éœ€è¦ï¼‰
    inputs = [prefix + inp for inp in inputs]
    
    # å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯
    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)
    
    # å¯¹ç›®æ ‡è¿›è¡Œåˆ†è¯
    labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)
    
    # å¤„ç†å¡«å……æ ‡è®°
    if padding == "max_length" and data_args.ignore_pad_token_for_loss:
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

**é¢„å¤„ç†æ­¥éª¤**ï¼š
1. **æ–‡æœ¬æå–**ï¼šä»ç¿»è¯‘å¯¹ä¸­æå–æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ–‡æœ¬
2. **å‰ç¼€æ·»åŠ **ï¼šä¸ºT5æ¨¡å‹æ·»åŠ ä»»åŠ¡å‰ç¼€
3. **åˆ†è¯å¤„ç†**ï¼šå¯¹è¾“å…¥å’Œç›®æ ‡æ–‡æœ¬è¿›è¡Œåˆ†è¯
4. **é•¿åº¦æ§åˆ¶**ï¼šæˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
5. **æ ‡ç­¾å¤„ç†**ï¼šå°†å¡«å……æ ‡è®°æ›¿æ¢ä¸º-100ï¼ˆå¿½ç•¥æŸå¤±è®¡ç®—ï¼‰

#### **æ•°æ®é›†å¤„ç†**
```python
if training_args.do_train:
    train_dataset = raw_datasets["train"]
    if data_args.max_train_samples is not None:
        max_train_samples = min(len(train_dataset), data_args.max_train_samples)
        train_dataset = train_dataset.select(range(max_train_samples))
    
    with training_args.main_process_first(desc="train dataset map pre-processing"):
        train_dataset = train_dataset.map(
            preprocess_function,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on train dataset",
        )
```

### **5. æ•°æ®æ•´ç†å™¨**

```python
# æ•°æ®æ•´ç†å™¨
label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id
if data_args.pad_to_max_length:
    data_collator = default_data_collator
else:
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        label_pad_token_id=label_pad_token_id,
        pad_to_multiple_of=8 if training_args.fp16 else None,
    )
```

**æ•°æ®æ•´ç†å™¨ä½œç”¨**ï¼š
- å°†æ‰¹æ¬¡æ•°æ®æ•´ç†æˆæ¨¡å‹è¾“å…¥æ ¼å¼
- å¤„ç†åŠ¨æ€å¡«å……
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ

### **6. è¯„ä¼°æŒ‡æ ‡**

```python
# åŠ è½½BLEUè¯„ä¼°æŒ‡æ ‡
metric = evaluate.load("sacrebleu", cache_dir=model_args.cache_dir)

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]
    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    
    # æ›¿æ¢å¡«å……æ ‡è®°
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # åå¤„ç†
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    
    # è®¡ç®—BLEUåˆ†æ•°
    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}
    
    # è®¡ç®—ç”Ÿæˆé•¿åº¦
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result
```

### **7. è®­ç»ƒå™¨åˆå§‹åŒ–**

```python
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset if training_args.do_train else None,
    eval_dataset=eval_dataset if training_args.do_eval else None,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics if training_args.predict_with_generate else None,
)
```

### **8. è®­ç»ƒå’Œè¯„ä¼°**

#### **è®­ç»ƒé˜¶æ®µ**
```python
if training_args.do_train:
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    trainer.save_model()  # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
    
    # è®°å½•è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()
```

#### **è¯„ä¼°é˜¶æ®µ**
```python
if training_args.do_eval:
    logger.info("*** Evaluate ***")
    
    max_length = training_args.generation_max_length or data_args.val_max_target_length
    num_beams = data_args.num_beams or training_args.generation_num_beams
    
    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix="eval")
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)
```

#### **é¢„æµ‹é˜¶æ®µ**
```python
if training_args.do_predict:
    logger.info("*** Predict ***")
    
    predict_results = trainer.predict(
        predict_dataset, 
        metric_key_prefix="predict", 
        max_length=max_length, 
        num_beams=num_beams
    )
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if trainer.is_world_process_zero():
        if training_args.predict_with_generate:
            predictions = predict_results.predictions
            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
            predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            predictions = [pred.strip() for pred in predictions]
            
            output_prediction_file = os.path.join(training_args.output_dir, "generated_predictions.txt")
            with open(output_prediction_file, "w", encoding="utf-8") as writer:
                writer.write("\n".join(predictions))
```

## ï¿½ï¿½ è„šæœ¬ç‰¹ç‚¹

### **1. é«˜åº¦å¯é…ç½®**
- æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„
- çµæ´»çš„æ•°æ®æºé…ç½®
- ä¸°å¯Œçš„è®­ç»ƒå‚æ•°

### **2. ç”Ÿäº§å°±ç»ª**
- å®Œæ•´çš„æ—¥å¿—è®°å½•
- æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
- åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### **3. å¤šè¯­è¨€æ”¯æŒ**
- è‡ªåŠ¨å¤„ç†å¤šè¯­è¨€åˆ†è¯å™¨
- æ”¯æŒè¯­è¨€ä»£ç è®¾ç½®
- å¼ºåˆ¶å¼€å§‹æ ‡è®°é…ç½®

### **4. è¯„ä¼°å®Œå–„**
- BLEUåˆ†æ•°è®¡ç®—
- ç”Ÿæˆé•¿åº¦ç»Ÿè®¡
- é¢„æµ‹ç»“æœä¿å­˜

## ï¿½ï¿½ ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬è®­ç»ƒå‘½ä»¤
python run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang de \
    --source_prefix "translate English to German: " \
    --dataset_name wmt16 \
    --dataset_config_name de-en \
    --output_dir ./output \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

è¿™ä¸ªè„šæœ¬æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ä¼ä¸šçº§çš„ç¿»è¯‘æ¨¡å‹è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨æµç¨‹ã€‚
