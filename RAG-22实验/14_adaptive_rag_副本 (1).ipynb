{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "RyWQZtatgLMR"
      },
      "source": [
        "# Adaptive Retrieval for Enhanced RAG Systems\n",
        "\n",
        "In this notebook, I implement an Adaptive Retrieval system that dynamically selects the most appropriate retrieval strategy based on the type of query. This approach significantly enhances our RAG system's ability to provide accurate and relevant responses across a diverse range of questions.\n",
        "\n",
        "Different questions demand different retrieval strategies. Our system:\n",
        "\n",
        "1. Classifies the query type (Factual, Analytical, Opinion, or Contextual)\n",
        "2. Selects the appropriate retrieval strategy\n",
        "3. Executes specialized retrieval techniques\n",
        "4. Generates a tailored response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 增强型RAG系统的自适应检索  \n",
        "\n",
        "在本笔记本中，我实现了一个自适应检索系统，该系统可基于查询类型动态选择最合适的检索策略。这种方法显著提升了RAG系统在处理各类问题时提供准确且相关回答的能力。  \n",
        "\n",
        "不同类型的问题需要不同的检索策略。我们的系统能够：  \n",
        "1. 对查询类型进行分类（事实型、分析型、观点型或上下文型）  \n",
        "2. 选择合适的检索策略  \n",
        "3. 执行专门的检索技术  \n",
        "4. 生成定制化的回答  \n",
        "\n",
        "\n",
        "### 关键概念解析  \n",
        "- **自适应检索（Adaptive Retrieval）**：根据查询特征动态调整检索策略，而非采用固定方法。  \n",
        "- **查询分类（Query Classification）**：将用户问题划分为不同类别，例如：  \n",
        "  - **事实型（Factual）**：寻求具体事实答案（如“谁发明了蒸汽机？”）。  \n",
        "  - **分析型（Analytical）**：需要因果分析或逻辑推理（如“为什么气候变化会影响农业？”）。  \n",
        "  - **观点型（Opinion）**：涉及主观评价或建议（如“如何评价某部电影？”）。  \n",
        "  - **上下文型（Contextual）**：依赖对话历史或特定场景（如“基于之前的讨论，下一步该怎么做？”）。  \n",
        "\n",
        "\n",
        "### 系统优势  \n",
        "- **精准性提升**：针对不同问题类型采用最优检索策略，避免“一刀切”带来的误差。  \n",
        "- **泛化能力增强**：支持从简单事实查询到复杂分析型问题的全场景覆盖。  \n",
        "- **用户体验优化**：根据问题特性生成更贴合需求的回答，减少信息冗余或缺失。  \n",
        "\n",
        "通过这种自适应机制，RAG系统能够像人类一样理解问题的深层意图，并匹配最恰当的知识检索方式，从而显著提升整体问答质量。"
      ],
      "metadata": {
        "id": "kWyFxXuKgb7E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENnOUlW2gLMS"
      },
      "source": [
        "## Setting Up the Environment\n",
        "We begin by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PymuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMyh7JMhgODo",
        "outputId": "286aafdb-b3de-4a8a-e301-14a5a0950655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PymuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PymuPDF\n",
            "Successfully installed PymuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqHgFFXrgLMT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk7bXtUgLMT"
      },
      "source": [
        "## Extracting Text from a PDF File\n",
        "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeWFLNFrgLMU"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVOBAYB-gLMU"
      },
      "source": [
        "## Chunking the Extracted Text\n",
        "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM1T820egLMU"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86b68OAOgLMV"
      },
      "source": [
        "## Setting Up the OpenAI API Client\n",
        "We initialize the OpenAI client to generate embeddings and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfdLEagngLMV"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"xxxxx1/\",\n",
        "    api_key=\"skxxxxxx\" # Retrieve the API key from environment variables\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ49CRfpgLMV"
      },
      "source": [
        "## Simple Vector Store Implementation\n",
        "We'll create a basic vector store to manage document chunks and their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTRYEJKXgLMV"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "        filter_func (callable, optional): Function to filter results.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Apply filter if provided\n",
        "            if filter_func and not filter_func(self.metadata[i]):\n",
        "                continue\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the text\n",
        "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码实现了一个简单但功能完整的向量数据库（Vector Store），它是基于NumPy数组实现的。向量数据库是现代AI系统中的核心组件，主要用于存储文本、图像或其他数据的向量表示（embeddings），并能根据向量相似度快速检索相关内容。\n",
        "\n",
        "下面我将逐行解释这个向量数据库的工作原理：\n",
        "\n",
        "\n",
        "### 类结构与初始化\n",
        "```python\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "```\n",
        "\n",
        "这个类有三个核心属性：\n",
        "1. `vectors`：存储所有文本的向量表示（embeddings）\n",
        "2. `texts`：存储对应的原始文本\n",
        "3. `metadata`：存储额外信息（如文本来源、创建时间等）\n",
        "\n",
        "这三个列表通过索引位置关联，例如`vectors[0]`、`texts[0]`和`metadata[0]`对应同一条记录。\n",
        "\n",
        "\n",
        "### 添加数据项\n",
        "```python\n",
        "def add_item(self, text, embedding, metadata=None):\n",
        "    \"\"\"\n",
        "    Add an item to the vector store.\n",
        "\n",
        "    Args:\n",
        "    text (str): The original text.\n",
        "    embedding (List[float]): The embedding vector.\n",
        "    metadata (dict, optional): Additional metadata.\n",
        "    \"\"\"\n",
        "    self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "    self.texts.append(text)  # Add the original text to texts list\n",
        "    self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
        "```\n",
        "\n",
        "这个方法用于向向量数据库添加新记录：\n",
        "1. 将输入的embedding转换为NumPy数组（提高计算效率）\n",
        "2. 保存原始文本\n",
        "3. 保存元数据（如果没有提供则使用空字典）\n",
        "\n",
        "三个列表同步增长，保持索引位置的一致性。\n",
        "\n",
        "\n",
        "### 相似度搜索\n",
        "```python\n",
        "def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
        "    \"\"\"\n",
        "    Find the most similar items to a query embedding.\n",
        "\n",
        "    Args:\n",
        "    query_embedding (List[float]): Query embedding vector.\n",
        "    k (int): Number of results to return.\n",
        "    filter_func (callable, optional): Function to filter results.\n",
        "\n",
        "    Returns:\n",
        "    List[Dict]: Top k most similar items with their texts and metadata.\n",
        "    \"\"\"\n",
        "    if not self.vectors:\n",
        "        return []  # Return empty list if no vectors are stored\n",
        "    \n",
        "    # Convert query embedding to numpy array\n",
        "    query_vector = np.array(query_embedding)\n",
        "    \n",
        "    # Calculate similarities using cosine similarity\n",
        "    similarities = []\n",
        "    for i, vector in enumerate(self.vectors):\n",
        "        # Apply filter if provided\n",
        "        if filter_func and not filter_func(self.metadata[i]):\n",
        "            continue\n",
        "            \n",
        "        # Calculate cosine similarity\n",
        "        similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "        similarities.append((i, similarity))  # Append index and similarity score\n",
        "    \n",
        "    # Sort by similarity (descending)\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # Return top k results\n",
        "    results = []\n",
        "    for i in range(min(k, len(similarities))):\n",
        "        idx, score = similarities[i]\n",
        "        results.append({\n",
        "            \"text\": self.texts[idx],  # Add the text\n",
        "            \"metadata\": self.metadata[idx],  # Add the metadata\n",
        "            \"similarity\": score  # Add the similarity score\n",
        "        })\n",
        "    \n",
        "    return results  # Return the list of top k results\n",
        "```\n",
        "\n",
        "这是向量数据库的核心方法，实现了基于向量相似度的检索功能：\n",
        "\n",
        "#### 输入参数：\n",
        "- `query_embedding`：查询向量（表示用户的查询）\n",
        "- `k`：返回结果的数量（默认5）\n",
        "- `filter_func`：可选的过滤函数（用于筛选符合条件的记录）\n",
        "\n",
        "#### 执行流程：\n",
        "1. **输入检查**：如果数据库为空，直接返回空列表\n",
        "2. **向量转换**：将查询向量转换为NumPy数组\n",
        "3. **相似度计算**：\n",
        "   - 遍历所有存储的向量\n",
        "   - 对每个向量应用过滤条件（如果有）\n",
        "   - 计算查询向量与存储向量的余弦相似度\n",
        "   - 余弦相似度公式：`dot(a, b) / (norm(a) * norm(b))`\n",
        "   - 保存结果（索引和相似度分数）\n",
        "\n",
        "4. **结果排序**：按相似度分数降序排列\n",
        "5. **结果收集**：提取前k个结果，包含原始文本、元数据和相似度分数\n",
        "\n",
        "#### 余弦相似度：\n",
        "这是向量检索中最常用的相似度度量方法，取值范围为[-1, 1]：\n",
        "- 1表示两个向量方向完全相同（最相似）\n",
        "- 0表示两个向量正交（不相关）\n",
        "- -1表示两个向量方向完全相反\n",
        "\n",
        "在自然语言处理中，余弦相似度越高，表示两个文本的语义越相近。\n",
        "\n",
        "\n",
        "### 过滤功能\n",
        "```python\n",
        "if filter_func and not filter_func(self.metadata[i]):\n",
        "    continue\n",
        "```\n",
        "\n",
        "这个条件语句允许用户通过自定义函数过滤结果。例如：\n",
        "- 只检索特定来源的文档：`lambda meta: meta['source'] == 'book1'`\n",
        "- 只检索最近一个月内添加的文档：`lambda meta: meta['timestamp'] > one_month_ago`\n",
        "\n",
        "这为向量检索增加了灵活性，可以结合语义相似度和其他条件进行更精准的筛选。\n",
        "\n",
        "\n",
        "### 为什么这个实现很重要？\n",
        "向量数据库是现代RAG（检索增强生成）系统的核心组件，它解决了两个关键问题：\n",
        "1. **语义检索**：能够理解用户查询的语义，而不仅仅是关键词匹配\n",
        "2. **高效匹配**：通过向量相似度，可以快速找到相关内容（即使表述不同）\n",
        "\n",
        "这个简单实现虽然使用了朴素的遍历算法（适用于小规模数据），但它展示了向量数据库的核心原理。在实际应用中，大规模向量数据库通常会使用更高效的数据结构（如KD树、HNSW图等）来加速检索过程。"
      ],
      "metadata": {
        "id": "nbEOs11EmxdU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK2oPdFVgLMW"
      },
      "source": [
        "## Creating Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8sTCrCPgLMW"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text.\n",
        "\n",
        "    Args:\n",
        "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "\n",
        "    Returns:\n",
        "    List[float] or List[List[float]]: The embedding vector(s).\n",
        "    \"\"\"\n",
        "    # Handle both string and list inputs by converting string input to a list\n",
        "    input_text = text if isinstance(text, list) else [text]\n",
        "\n",
        "    # Create embeddings for the input text using the specified model\n",
        "    response = client.embeddings.create(\n",
        "        model=model,\n",
        "        input=input_text\n",
        "    )\n",
        "\n",
        "    # If the input was a single string, return just the first embedding\n",
        "    if isinstance(text, str):\n",
        "        return response.data[0].embedding\n",
        "\n",
        "    # Otherwise, return all embeddings for the list of texts\n",
        "    return [item.embedding for item in response.data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "28ssKueSniR1",
        "outputId": "46dbfa8f-6b61-47f4-d849-471c73ea9243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'response' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-496027881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qAHqnXxbniC8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH1b2U4ngLMW"
      },
      "source": [
        "## Document Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRJYrwbRgLMW"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for use with adaptive retrieval.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the extracted text\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Create embeddings for the text chunks\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Initialize the vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add each chunk and its embedding to the vector store with metadata\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "\n",
        "    # Return the chunks and the vector store\n",
        "    return chunks, store"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码实现了文档处理的完整流程，将PDF文档转换为适合自适应检索系统使用的格式。它主要完成三个核心任务：**文本提取**、**文本分块**和**向量嵌入**，最终构建一个可用于语义检索的向量数据库。\n",
        "\n",
        "下面我将逐行解释这个文档处理函数的工作原理：\n",
        "\n",
        "\n",
        "### 函数概述与参数\n",
        "```python\n",
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for use with adaptive retrieval.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "这个函数接收三个参数：\n",
        "- `pdf_path`：PDF文件的路径\n",
        "- `chunk_size`：每个文本块的大小（默认1000个字符）\n",
        "- `chunk_overlap`：相邻文本块之间的重叠长度（默认200个字符）\n",
        "\n",
        "返回值是一个元组：\n",
        "- 包含所有文本块的列表\n",
        "- 已经填充好数据的向量数据库实例\n",
        "\n",
        "\n",
        "### 文本提取与分块\n",
        "```python\n",
        "# Extract text from the PDF file\n",
        "print(\"Extracting text from PDF...\")\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Chunk the extracted text\n",
        "print(\"Chunking text...\")\n",
        "chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "print(f\"Created {len(chunks)} text chunks\")\n",
        "```\n",
        "\n",
        "这部分代码完成两个关键步骤：\n",
        "1. **文本提取**：调用`extract_text_from_pdf`函数从PDF文件中提取原始文本。这个函数通常使用OCR技术或直接解析PDF格式来获取文本内容。\n",
        "2. **文本分块**：使用`chunk_text`函数将提取的完整文本分割成多个小块。分块时采用**滑动窗口**策略，相邻块之间有一定的重叠，这样可以避免重要信息被分割在两个块中而导致信息丢失。\n",
        "\n",
        "\n",
        "### 向量嵌入生成\n",
        "```python\n",
        "# Create embeddings for the text chunks\n",
        "print(\"Creating embeddings for chunks...\")\n",
        "chunk_embeddings = create_embeddings(chunks)\n",
        "```\n",
        "\n",
        "这行代码调用`create_embeddings`函数，将所有文本块转换为向量表示（embeddings）。向量嵌入是一种将文本转换为多维空间中向量的技术，使得语义相似的文本在向量空间中距离更近。\n",
        "\n",
        "这里的`create_embeddings`函数通常会调用大型语言模型（如OpenAI的text-embedding-ada-002）来生成向量表示。注意，这个函数支持批量处理，可以同时为多个文本生成嵌入向量。\n",
        "\n",
        "\n",
        "### 构建向量数据库\n",
        "```python\n",
        "# Initialize the vector store\n",
        "store = SimpleVectorStore()\n",
        "\n",
        "# Add each chunk and its embedding to the vector store with metadata\n",
        "for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "    store.add_item(\n",
        "        text=chunk,\n",
        "        embedding=embedding,\n",
        "        metadata={\"index\": i, \"source\": pdf_path}\n",
        "    )\n",
        "\n",
        "print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "```\n",
        "\n",
        "这部分代码将处理好的文本块及其向量表示存入向量数据库：\n",
        "1. **初始化向量库**：创建`SimpleVectorStore`类的实例\n",
        "2. **批量添加数据**：遍历所有文本块及其对应的向量，调用`add_item`方法将它们存入向量库\n",
        "3. **添加元数据**：为每个文本块添加元数据，包括块索引和来源文件路径\n",
        "\n",
        "这里的元数据很重要，它为后续的检索和结果解释提供了额外信息。例如，当系统返回某个文本块时，我们可以知道它来自哪个文件以及在文件中的位置。\n",
        "\n",
        "\n",
        "### 函数返回\n",
        "```python\n",
        "# Return the chunks and the vector store\n",
        "return chunks, store\n",
        "```\n",
        "\n",
        "最终返回两个对象：\n",
        "1. **文本块列表**：包含所有分割后的文本片段\n",
        "2. **向量数据库**：存储了所有文本块的向量表示，可以用于语义检索\n",
        "\n",
        "这两个对象是后续自适应检索系统的基础：文本块提供了原始内容，而向量数据库则支持基于语义的高效检索。\n",
        "\n",
        "\n",
        "### 为什么这样设计很重要？\n",
        "这个文档处理流程是整个自适应检索系统的基础，它解决了几个关键问题：\n",
        "\n",
        "1. **处理长文档**：大多数LLM模型有输入长度限制，直接处理整个文档不可行。分块技术将长文档转换为适合模型处理的小块。\n",
        "\n",
        "2. **保留上下文**：通过设置块之间的重叠，确保相邻块之间有语义连续性，避免重要信息丢失。\n",
        "\n",
        "3. **语义检索**：将文本转换为向量表示后，可以使用余弦相似度等方法进行语义检索，比传统的关键词匹配更强大。\n",
        "\n",
        "4. **可追溯性**：通过元数据记录每个文本块的来源和位置，使得检索结果可以追溯到原始文档，增强回答的可信度。\n",
        "\n",
        "这个实现虽然简单，但包含了文档处理的核心要素，是构建高效RAG系统的关键一步。"
      ],
      "metadata": {
        "id": "xFLxK6DPoIwZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2RsHp8zgLMW"
      },
      "source": [
        "## Query Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FheiyWAgLMX"
      },
      "outputs": [],
      "source": [
        "def classify_query(query, model=\"o1\"):\n",
        "    \"\"\"\n",
        "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Query category\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's classification\n",
        "    system_prompt = \"\"\"You are an expert at classifying questions.\n",
        "        Classify the given query into exactly one of these categories:\n",
        "        - Factual: Queries seeking specific, verifiable information.\n",
        "        - Analytical: Queries requiring comprehensive analysis or explanation.\n",
        "        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
        "        - Contextual: Queries that depend on user-specific context.\n",
        "\n",
        "        Return ONLY the category name, without any explanation or additional text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the query to be classified\n",
        "    user_prompt = f\"Classify this query: {query}\"\n",
        "\n",
        "    # Generate the classification response from the AI model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and strip the category from the response\n",
        "    category = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Define the list of valid categories\n",
        "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
        "\n",
        "    # Ensure the returned category is valid\n",
        "    for valid in valid_categories:\n",
        "        if valid in category:\n",
        "            return valid\n",
        "\n",
        "    # Default to \"Factual\" if classification fails\n",
        "    return \"Factual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRyngkFjgLMX"
      },
      "source": [
        "## Implementing Specialized Retrieval Strategies\n",
        "### 1. Factual Strategy - Focus on Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS8jydbvgLMX"
      },
      "outputs": [],
      "source": [
        "def factual_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    Retrieval strategy for factual queries focusing on precision.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # Use LLM to enhance the query for better precision\n",
        "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
        "        Your task is to reformulate the given factual query to make it more precise and\n",
        "        specific for information retrieval. Focus on key entities and their relationships.\n",
        "\n",
        "        Provide ONLY the enhanced query without any explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Enhance this factual query: {query}\"\n",
        "\n",
        "    # Generate the enhanced query using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and print the enhanced query\n",
        "    enhanced_query = response.choices[0].message.content.strip()\n",
        "    print(f\"Enhanced query: {enhanced_query}\")\n",
        "\n",
        "    # Create embeddings for the enhanced query\n",
        "    query_embedding = create_embeddings(enhanced_query)\n",
        "\n",
        "    # Perform initial similarity search to retrieve documents\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
        "\n",
        "    # Initialize a list to store ranked results\n",
        "    ranked_results = []\n",
        "\n",
        "    # Score and rank documents by relevance using LLM\n",
        "    for doc in initial_results:\n",
        "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
        "        ranked_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"similarity\": doc[\"similarity\"],\n",
        "            \"relevance_score\": relevance_score\n",
        "        })\n",
        "\n",
        "    # Sort the results by relevance score in descending order\n",
        "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top k results\n",
        "    return ranked_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事实型检索策略详解：精准定位信息的\"智能放大镜\"\n",
        "\n",
        "这个函数实现了针对事实型问题的检索策略，核心目标是像\"放大镜\"一样精准定位具体答案。事实型问题通常寻求具体、可验证的信息（如\"谁发明了电灯？\"），因此策略设计聚焦于提高检索的精确性。下面逐行解析其工作原理：\n",
        "\n",
        "\n",
        "### 策略整体流程概述\n",
        "```python\n",
        "def factual_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"针对事实型查询的检索策略（注重精确性）\"\"\"\n",
        "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
        "    # ...（中间代码）\n",
        "    return ranked_results[:k]\n",
        "```\n",
        "\n",
        "这个策略遵循\"**优化查询→初步检索→二次筛选**\"的三层逻辑：\n",
        "1. 先用LLM优化查询，使其更精准\n",
        "2. 检索出较多候选结果（k*2）\n",
        "3. 再用LLM对结果进行相关性评分，筛选出最相关的k个\n",
        "\n",
        "\n",
        "### 第一步：查询优化——让问题更\"锋利\"\n",
        "```python\n",
        "# 使用LLM优化查询以提高精确性\n",
        "system_prompt = \"\"\"你是优化搜索查询的专家。\n",
        "    你的任务是重新表述给定的事实型查询，使其更精确、更具体，以用于信息检索。\n",
        "    关注关键实体及其关系。\n",
        "\n",
        "    只返回优化后的查询，不做任何解释。\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"Enhance this factual query: {query}\"\n",
        "\n",
        "# 调用LLM生成优化后的查询\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ],\n",
        "    temperature=0  # 确保输出确定性\n",
        ")\n",
        "\n",
        "enhanced_query = response.choices[0].message.content.strip()\n",
        "print(f\"优化后的查询: {enhanced_query}\")\n",
        "```\n",
        "\n",
        "**核心技巧**：通过LLM将模糊问题转化为精准查询。例如：\n",
        "- 输入查询：\"相对论的提出者\"\n",
        "- 优化后：\"阿尔伯特·爱因斯坦在哪一年提出了相对论？\"\n",
        "\n",
        "**为什么有效**：\n",
        "1. 明确关键实体（爱因斯坦）和关系（提出相对论）\n",
        "2. 添加具体维度（时间）使查询更精准\n",
        "3. `temperature=0`确保LLM生成最确定的答案，避免发散\n",
        "\n",
        "\n",
        "### 第二步：初步检索——撒网捞鱼\n",
        "```python\n",
        "# 为优化后的查询创建嵌入向量\n",
        "query_embedding = create_embeddings(enhanced_query)\n",
        "\n",
        "# 执行初始检索（获取k*2个结果）\n",
        "initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
        "```\n",
        "\n",
        "**策略要点**：\n",
        "1. 用优化后的查询生成向量，确保检索语义精准\n",
        "2. 检索数量设为`k*2`（如k=4时检索8条），为后续筛选留有余地\n",
        "3. 这里使用向量数据库的`similarity_search`方法，基于余弦相似度检索语义相近的文本块\n",
        "\n",
        "\n",
        "### 第三步：二次筛选——LLM当\"裁判\"\n",
        "```python\n",
        "# 初始化排序结果列表\n",
        "ranked_results = []\n",
        "\n",
        "# 使用LLM对每个文档进行相关性评分\n",
        "for doc in initial_results:\n",
        "    relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
        "    ranked_results.append({\n",
        "        \"text\": doc[\"text\"],\n",
        "        \"metadata\": doc[\"metadata\"],\n",
        "        \"similarity\": doc[\"similarity\"],\n",
        "        \"relevance_score\": relevance_score\n",
        "    })\n",
        "\n",
        "# 按相关性分数降序排序\n",
        "ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "# 返回前k个结果\n",
        "return ranked_results[:k]\n",
        "```\n",
        "\n",
        "**关键函数`score_document_relevance`解析**：\n",
        "```python\n",
        "def score_document_relevance(query, document):\n",
        "    \"\"\"让LLM判断文档与查询的相关性（0-10分）\"\"\"\n",
        "    system_prompt = \"\"\"你是评估相关性的专家，给文档打0-10分：\n",
        "    0=完全不相关，10=完美匹配查询\"\"\"\n",
        "    # ...（构造提示词并调用LLM）\n",
        "    # 提取分数并返回\n",
        "```\n",
        "\n",
        "**二次筛选的价值**：\n",
        "1. 向量相似度是\"语义相近\"的度量，但不一定完全匹配问题\n",
        "2. LLM能理解自然语言的细微差别，例如：\n",
        "   - 查询：\"Python 3.8的新特性\"\n",
        "   - 文档1：\"Python 3.8新增海象运算符\"（相关度10分）\n",
        "   - 文档2：\"Python 3.9的性能优化\"（向量相似度可能高，但LLM会打低分）\n",
        "3. 结合`similarity`和`relevance_score`双重排序，确保结果既语义相关又精准匹配问题\n",
        "\n",
        "\n",
        "### 策略核心优势：精准性超越传统检索\n",
        "1. **查询优化层**：将自然语言问题转化为信息检索友好的表达式，类似人类将问题转化为数据库查询\n",
        "2. **双层筛选机制**：\n",
        "   - 向量相似度：快速过滤出语义相关的候选\n",
        "   - LLM相关性评分：在候选中精准定位最匹配的答案\n",
        "3. **可解释性**：每个结果都带有`relevance_score`，便于理解为什么选中该文档\n",
        "\n",
        "\n",
        "### 实际应用案例\n",
        "**用户查询**：\"世界上最长的河流是哪条？\"\n",
        "\n",
        "#### 执行流程：\n",
        "1. **查询优化**：\n",
        "   - LLM将查询转化为：\"世界上长度最长的河流及其确切长度是多少？\"\n",
        "2. **初步检索**：\n",
        "   - 检索出8个与\"河流长度\"相关的文本块，包括：\n",
        "     - \"尼罗河全长约6650公里\"\n",
        "     - \"亚马逊河长度争议\"\n",
        "     - \"长江亚洲第一长\"\n",
        "3. **二次筛选**：\n",
        "   - LLM对每个文档打分：\n",
        "     - 尼罗河文档：10分（直接回答问题）\n",
        "     - 亚马逊河文档：7分（涉及长度但有争议）\n",
        "     - 长江文档：5分（仅提及亚洲范围）\n",
        "4. **返回结果**：\n",
        "   - 前4个结果中，尼罗河相关文档排在最前，确保用户获得准确答案\n",
        "\n",
        "\n",
        "### 与普通检索的对比\n",
        "| 对比维度        | 普通检索（仅向量相似度）        | 事实型检索策略                |\n",
        "|-----------------|---------------------------------|-----------------------------|\n",
        "| **查询处理**    | 直接使用原始查询                | 先优化查询，明确关键实体关系  |\n",
        "| **结果数量**    | 直接返回k个结果                | 先检索k*2个，再筛选出k个     |\n",
        "| **相关性判断**  | 仅依赖向量距离                  | 向量距离+LLM语义理解        |\n",
        "| **精准性**      | 可能返回语义相关但不直接的答案  | 精准定位直接回答问题的内容  |\n",
        "\n",
        "这个策略通过引入LLM进行查询优化和结果重排序，将事实型问题的检索精准性提升到了新的高度，尤其适合需要具体答案的场景。"
      ],
      "metadata": {
        "id": "_JEsoqFto8A8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ4T3yhRgLMX"
      },
      "source": [
        "### 2. Analytical Strategy - Comprehensive Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_GOPv8ugLMY"
      },
      "outputs": [],
      "source": [
        "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # Define the system prompt to guide the AI in generating sub-questions\n",
        "    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n",
        "    Generate sub-questions that explore different aspects of the main analytical query.\n",
        "    These sub-questions should cover the breadth of the topic and help retrieve\n",
        "    comprehensive information.\n",
        "\n",
        "    Return a list of exactly 3 sub-questions, one per line.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the main query\n",
        "    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
        "\n",
        "    # Generate the sub-questions using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Extract and clean the sub-questions\n",
        "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
        "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
        "    print(f\"Generated sub-queries: {sub_queries}\")\n",
        "\n",
        "    # Retrieve documents for each sub-query\n",
        "    all_results = []\n",
        "    for sub_query in sub_queries:\n",
        "        # Create embeddings for the sub-query\n",
        "        sub_query_embedding = create_embeddings(sub_query)\n",
        "        # Perform similarity search for the sub-query\n",
        "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Ensure diversity by selecting from different sub-query results\n",
        "    # Remove duplicates (same text content)\n",
        "    unique_texts = set()\n",
        "    diverse_results = []\n",
        "\n",
        "    for result in all_results:\n",
        "        if result[\"text\"] not in unique_texts:\n",
        "            unique_texts.add(result[\"text\"])\n",
        "            diverse_results.append(result)\n",
        "\n",
        "    # If we need more results to reach k, add more from initial results\n",
        "    if len(diverse_results) < k:\n",
        "        # Direct retrieval for the main query\n",
        "        main_query_embedding = create_embeddings(query)\n",
        "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
        "\n",
        "        for result in main_results:\n",
        "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
        "                unique_texts.add(result[\"text\"])\n",
        "                diverse_results.append(result)\n",
        "\n",
        "    # Return the top k diverse results\n",
        "    return diverse_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分析型检索策略详解：多维视角拆解复杂问题\n",
        "\n",
        "这个函数实现了针对分析型问题的检索策略，核心思路是将复杂问题拆解为多个维度，从不同视角检索信息，从而获得全面的分析依据。分析型问题通常需要综合多方面因素（如\"人工智能对就业市场的影响\"），因此策略设计聚焦于\"全面覆盖\"和\"多维度分析\"。\n",
        "\n",
        "\n",
        "### 策略整体流程概述\n",
        "```python\n",
        "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"针对分析型查询的检索策略（注重全面性）\"\"\"\n",
        "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
        "    # ...（中间代码）\n",
        "    return diverse_results[:k]\n",
        "```\n",
        "\n",
        "这个策略遵循\"**问题拆解→多维度检索→结果整合与去重**\"的三层逻辑：\n",
        "1. 先用LLM将复杂问题拆解为3个子问题（覆盖不同维度）\n",
        "2. 针对每个子问题分别检索相关文档\n",
        "3. 整合结果并确保多样性，避免重复和片面性\n",
        "\n",
        "\n",
        "### 第一步：问题拆解——把复杂问题\"切片\"\n",
        "```python\n",
        "# 引导LLM生成子问题的系统提示词\n",
        "system_prompt = \"\"\"你是拆解复杂问题的专家。\n",
        "为给定的分析型查询生成子问题，这些子问题应涵盖主题的不同方面，\n",
        "有助于检索全面的信息。\n",
        "\n",
        "精确返回3个子问题，每行一个。\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
        "\n",
        "# 调用LLM生成子问题\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ],\n",
        "    temperature=0.3  # 稍微增加随机性，获取多样化的子问题\n",
        ")\n",
        "\n",
        "# 提取并清理子问题\n",
        "sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
        "sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
        "print(f\"生成的子问题: {sub_queries}\")\n",
        "```\n",
        "\n",
        "**核心技巧**：通过精心设计的提示词，引导LLM从不同维度拆解问题。例如：\n",
        "- 原始查询：\"人工智能对就业市场的影响\"\n",
        "- 生成的子问题：\n",
        "  1. \"哪些行业的工作岗位最容易被AI自动化取代？\"（影响维度）\n",
        "  2. \"AI创造了哪些新的就业机会？\"（机遇维度）\n",
        "  3. \"政府和企业应采取哪些措施应对就业结构变化？\"（对策维度）\n",
        "\n",
        "**为什么`temperature=0.3`**：\n",
        "- 较低的temperature确保生成的子问题围绕主题展开，不会偏离太远\n",
        "- 适度的随机性避免每次生成相同的子问题，增加多样性\n",
        "\n",
        "\n",
        "### 第二步：多维度检索——每个子问题\"撒一网\"\n",
        "```python\n",
        "# 为每个子问题检索相关文档\n",
        "all_results = []\n",
        "for sub_query in sub_queries:\n",
        "    sub_query_embedding = create_embeddings(sub_query)\n",
        "    results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
        "    all_results.extend(results)\n",
        "```\n",
        "\n",
        "**策略要点**：\n",
        "1. 每个子问题生成独立的向量表示\n",
        "2. 为每个子问题检索2个相关文档（共3个子问题，最多6个文档）\n",
        "3. 这种方式确保从不同维度收集信息，避免只关注单一视角\n",
        "\n",
        "**举例说明**：\n",
        "- 对于\"哪些行业易被AI取代\"子问题，可能检索到：\n",
        "  - \"制造业自动化程度及岗位流失预测\"\n",
        "  - \"客服行业AI聊天机器人的应用现状\"\n",
        "- 对于\"AI创造的新机会\"子问题，可能检索到：\n",
        "  - \"AI训练师和数据标注员的职业发展\"\n",
        "  - \"机器学习工程师的市场需求分析\"\n",
        "\n",
        "\n",
        "### 第三步：结果整合与去重——确保信息多样性\n",
        "```python\n",
        "# 去重（相同文本内容只保留一个）\n",
        "unique_texts = set()\n",
        "diverse_results = []\n",
        "\n",
        "for result in all_results:\n",
        "    if result[\"text\"] not in unique_texts:\n",
        "        unique_texts.add(result[\"text\"])\n",
        "        diverse_results.append(result)\n",
        "\n",
        "# 如果结果不足k个，从主查询直接检索补充\n",
        "if len(diverse_results) < k:\n",
        "    main_query_embedding = create_embeddings(query)\n",
        "    main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
        "    \n",
        "    for result in main_results:\n",
        "        if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
        "            unique_texts.add(result[\"text\"])\n",
        "            diverse_results.append(result)\n",
        "\n",
        "# 返回前k个多样化结果\n",
        "return diverse_results[:k]\n",
        "```\n",
        "\n",
        "**去重与补充机制**：\n",
        "1. **去重逻辑**：使用集合（set）存储已选文本内容，避免重复\n",
        "2. **补充机制**：如果去重后结果不足k个，直接使用原始查询进行检索\n",
        "3. **多样性保障**：优先保留来自不同子问题的结果，确保覆盖多维度视角\n",
        "\n",
        "\n",
        "### 策略核心优势：全面性与多维度分析\n",
        "1. **问题拆解能力**：利用LLM的理解能力，自动识别复杂问题的关键维度\n",
        "2. **多维度检索**：针对每个维度独立检索，避免单一检索可能导致的视角盲区\n",
        "3. **智能去重**：既保证信息全面性，又避免冗余内容干扰分析\n",
        "4. **灵活性**：当某维度信息不足时，能从主查询补充相关内容\n",
        "\n",
        "\n",
        "### 实际应用案例\n",
        "**用户查询**：\"如何评估一家科技公司的投资价值？\"\n",
        "\n",
        "#### 执行流程：\n",
        "1. **问题拆解**：\n",
        "   - LLM生成3个子问题：\n",
        "     1. \"评估科技公司投资价值的关键财务指标有哪些？\"\n",
        "     2. \"科技公司的技术创新能力如何衡量？\"\n",
        "     3. \"哪些市场因素影响科技公司的长期增长潜力？\"\n",
        "2. **多维度检索**：\n",
        "   - 针对财务指标：检索PE、PS等估值方法的文档\n",
        "   - 针对技术创新：检索专利数量、研发投入等内容\n",
        "   - 针对市场因素：检索行业竞争格局、政策环境等资料\n",
        "3. **结果整合**：\n",
        "   - 从每个维度筛选最相关的文档，去重后形成最终结果集\n",
        "\n",
        "#### 返回结果示例：\n",
        "1. \"科技股估值的五大核心指标解析\"（财务维度）\n",
        "2. \"专利分析在科技公司评估中的应用\"（技术维度）\n",
        "3. \"全球半导体行业竞争格局与未来趋势\"（市场维度）\n",
        "4. \"科技公司研发投入与市场表现的相关性研究\"（综合维度）\n",
        "\n",
        "\n",
        "### 与事实型检索的对比\n",
        "| 对比维度        | 事实型检索策略                | 分析型检索策略                |\n",
        "|-----------------|-----------------------------|-----------------------------|\n",
        "| **问题类型**    | 寻求具体、明确答案          | 需要综合分析和多维度视角    |\n",
        "| **查询处理**    | 优化查询以提高精确性        | 拆解问题以覆盖多方面        |\n",
        "| **检索方式**    | 单次检索+LLM重排序          | 多次检索（每个子问题一次）  |\n",
        "| **结果特点**    | 精准定位直接答案            | 全面覆盖不同维度的信息      |\n",
        "| **多样性控制**  | 无特殊处理                  | 严格去重并确保多维度覆盖    |\n",
        "\n",
        "这个策略特别适合需要综合分析的场景，如商业决策、学术研究、政策制定等。通过将复杂问题拆解为多个可检索的子问题，它能够系统性地收集全面信息，为深入分析提供坚实基础。"
      ],
      "metadata": {
        "id": "UXDJ5I_FqAEL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdK3lyypgLMY"
      },
      "source": [
        "### 3. Opinion Strategy - Diverse Perspectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD1GvuNEgLMY"
      },
      "outputs": [],
      "source": [
        "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    Retrieval strategy for opinion queries focusing on diverse perspectives.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # Define the system prompt to guide the AI in identifying different perspectives\n",
        "    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n",
        "        For the given query about opinions or viewpoints, identify different perspectives\n",
        "        that people might have on this topic.\n",
        "\n",
        "        Return a list of exactly 3 different viewpoint angles, one per line.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the main query\n",
        "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
        "\n",
        "    # Generate the different perspectives using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Extract and clean the viewpoints\n",
        "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
        "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
        "    print(f\"Identified viewpoints: {viewpoints}\")\n",
        "\n",
        "    # Retrieve documents representing each viewpoint\n",
        "    all_results = []\n",
        "    for viewpoint in viewpoints:\n",
        "        # Combine the main query with the viewpoint\n",
        "        combined_query = f\"{query} {viewpoint}\"\n",
        "        # Create embeddings for the combined query\n",
        "        viewpoint_embedding = create_embeddings(combined_query)\n",
        "        # Perform similarity search for the combined query\n",
        "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
        "\n",
        "        # Mark results with the viewpoint they represent\n",
        "        for result in results:\n",
        "            result[\"viewpoint\"] = viewpoint\n",
        "\n",
        "        # Add the results to the list of all results\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Select a diverse range of opinions\n",
        "    # Ensure we get at least one document from each viewpoint if possible\n",
        "    selected_results = []\n",
        "    for viewpoint in viewpoints:\n",
        "        # Filter documents by viewpoint\n",
        "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
        "        if viewpoint_docs:\n",
        "            selected_results.append(viewpoint_docs[0])\n",
        "\n",
        "    # Fill remaining slots with highest similarity docs\n",
        "    remaining_slots = k - len(selected_results)\n",
        "    if remaining_slots > 0:\n",
        "        # Sort remaining docs by similarity\n",
        "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
        "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "        selected_results.extend(remaining_docs[:remaining_slots])\n",
        "\n",
        "    # Return the top k results\n",
        "    return selected_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 观点型检索策略详解：多元视角的\"辩论会组织者\"\n",
        "\n",
        "这个函数实现了针对观点型问题的检索策略，核心目标是像\"辩论会组织者\"一样，收集并呈现关于某一主题的多元观点。观点型问题通常涉及主观判断（如\"元宇宙是否值得投资\"），因此策略设计聚焦于\"视角多样性\"和\"无偏呈现\"。\n",
        "\n",
        "\n",
        "### 策略整体流程概述\n",
        "```python\n",
        "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"针对观点型查询的检索策略（注重多元视角）\"\"\"\n",
        "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
        "    # ...（中间代码）\n",
        "    return selected_results[:k]\n",
        "```\n",
        "\n",
        "这个策略遵循\"**视角识别→定向检索→平衡选择**\"的三层逻辑：\n",
        "1. 先用LLM识别关于该主题的不同观点角度\n",
        "2. 针对每个观点角度定向检索支持文档\n",
        "3. 平衡选择不同观点的文档，确保无偏呈现\n",
        "\n",
        "\n",
        "### 第一步：视角识别——发现辩论的\"正反方\"\n",
        "```python\n",
        "# 引导LLM识别不同观点的系统提示词\n",
        "system_prompt = \"\"\"你是识别主题不同观点的专家。\n",
        "对于给定的关于观点或看法的查询，识别人们可能持有的不同观点。\n",
        "\n",
        "精确返回3个不同的观点角度，每行一个。\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"Identify different perspectives on: {query}\"\n",
        "\n",
        "# 调用LLM生成不同观点\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ],\n",
        "    temperature=0.3  # 适度随机，获取多样化视角\n",
        ")\n",
        "\n",
        "# 提取并清理观点\n",
        "viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
        "viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
        "print(f\"识别的观点: {viewpoints}\")\n",
        "```\n",
        "\n",
        "**核心技巧**：通过提示词引导LLM扮演\"中立观察者\"，发现辩论的不同立场。例如：\n",
        "- 原始查询：\"社交媒体对青少年的影响\"\n",
        "- 生成的观点角度：\n",
        "  1. \"社交媒体对青少年认知发展的积极影响\"（支持方）\n",
        "  2. \"社交媒体导致青少年焦虑和注意力分散\"（反对方）\n",
        "  3. \"需要平衡使用的中立观点\"（中立方）\n",
        "\n",
        "**为什么`temperature=0.3`**：\n",
        "- 既保证观点围绕主题展开（非完全随机）\n",
        "- 又能生成稍有差异的视角，避免重复或片面\n",
        "\n",
        "\n",
        "### 第二步：定向检索——为每个观点\"找论据\"\n",
        "```python\n",
        "# 为每个观点检索支持文档\n",
        "all_results = []\n",
        "for viewpoint in viewpoints:\n",
        "    combined_query = f\"{query} {viewpoint}\"  # 组合主查询和观点\n",
        "    viewpoint_embedding = create_embeddings(combined_query)\n",
        "    results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
        "    \n",
        "    # 标记每个结果属于哪个观点\n",
        "    for result in results:\n",
        "        result[\"viewpoint\"] = viewpoint\n",
        "    \n",
        "    all_results.extend(results)\n",
        "```\n",
        "\n",
        "**策略要点**：\n",
        "1. 将主查询与观点组合（如\"元宇宙是否值得投资 支持观点\"）\n",
        "2. 为每个组合查询生成向量并检索2个相关文档\n",
        "3. 为检索结果标记所属观点，便于后续分类\n",
        "\n",
        "**举例说明**：\n",
        "- 观点1：\"元宇宙具有巨大投资潜力\"\n",
        "  - 检索到：\"元宇宙用户增长数据与市场规模预测\"\n",
        "  - 检索到：\"科技巨头在元宇宙的布局与投资\"\n",
        "- 观点2：\"元宇宙存在泡沫和不确定性\"\n",
        "  - 检索到：\"元宇宙项目失败案例分析\"\n",
        "  - 检索到：\"虚拟资产监管风险解读\"\n",
        "\n",
        "\n",
        "### 第三步：平衡选择——做无偏的\"观点整合者\"\n",
        "```python\n",
        "# 确保每个观点至少有一个代表文档\n",
        "selected_results = []\n",
        "for viewpoint in viewpoints:\n",
        "    viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
        "    if viewpoint_docs:\n",
        "        selected_results.append(viewpoint_docs[0])  # 选第一个匹配文档\n",
        "\n",
        "# 用高相似度文档填充剩余位置\n",
        "remaining_slots = k - len(selected_results)\n",
        "if remaining_slots > 0:\n",
        "    remaining_docs = [r for r in all_results if r not in selected_results]\n",
        "    remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "    selected_results.extend(remaining_docs[:remaining_slots])\n",
        "\n",
        "# 返回前k个平衡结果\n",
        "return selected_results[:k]\n",
        "```\n",
        "\n",
        "**平衡选择机制**：\n",
        "1. **优先保证视角多样性**：每个观点至少选一个文档（如果有）\n",
        "2. **相似度补充**：当某观点无足够文档时，用高相似度文档填充\n",
        "3. **去重处理**：通过`selected_results`自然去重（同文档不会被选多次）\n",
        "\n",
        "**关键逻辑**：\n",
        "- 避免某一观点占据过多份额（如k=4时，至少3个来自不同观点，第4个选最相似的）\n",
        "- 通过`viewpoint`标记确保结果可追溯，便于理解每个文档的立场\n",
        "\n",
        "\n",
        "### 策略核心优势：无偏性与视角多样性\n",
        "1. **自动视角发现**：无需人工标注，LLM自动识别潜在的辩论角度\n",
        "2. **定向检索能力**：针对每个观点精准匹配支持文档\n",
        "3. **平衡选择机制**：通过算法确保多元观点被公平呈现\n",
        "4. **可解释性**：每个结果都明确标注所属观点，便于用户理解立场\n",
        "\n",
        "\n",
        "### 实际应用案例\n",
        "**用户查询**：\"是否应该全面禁止塑料包装？\"\n",
        "\n",
        "#### 执行流程：\n",
        "1. **视角识别**：\n",
        "   - LLM生成3个观点：\n",
        "     1. \"环境保护角度的支持观点\"\n",
        "     2. \"经济影响角度的反对观点\"\n",
        "     3. \"替代方案可行性的中立观点\"\n",
        "2. **定向检索**：\n",
        "   - 支持观点：\"塑料污染对海洋生态的影响研究\"\n",
        "   - 反对观点：\"塑料包装对中小企业成本的影响分析\"\n",
        "   - 中立观点：\"可降解材料的研发进展与挑战\"\n",
        "3. **平衡选择**：\n",
        "   - 先为每个观点选1个文档（共3个）\n",
        "   - 再选1个相似度最高的文档（如\"全球各国塑料禁令效果对比\"）\n",
        "\n",
        "#### 返回结果示例：\n",
        "1. \"塑料微粒对海洋生物的危害及案例\"（支持禁塑）\n",
        "2. \"发展中国家塑料包装依赖的经济原因\"（反对全面禁塑）\n",
        "3. \"玉米淀粉基可降解包装的成本与性能分析\"（中立视角）\n",
        "4. \"欧盟塑料税政策的实施效果评估\"（综合视角）\n",
        "\n",
        "\n",
        "### 与其他策略的对比\n",
        "| 对比维度        | 事实型策略          | 分析型策略          | 观点型策略          |\n",
        "|-----------------|-------------------|-------------------|-------------------|\n",
        "| **问题类型**    | 具体事实          | 复杂分析          | 主观观点          |\n",
        "| **核心目标**    | 精准定位          | 全面覆盖          | 多元呈现          |\n",
        "| **查询处理**    | 优化查询          | 拆解问题          | 识别视角          |\n",
        "| **结果特点**    | 唯一正确答案      | 多维度分析依据    | 多元观点论据      |\n",
        "| **价值取向**    | 客观性            | 系统性            | 平衡性            |\n",
        "\n",
        "这个策略特别适合需要决策参考、舆论分析或学术讨论的场景。通过系统性地收集多元观点，它帮助用户超越单一视角，在充分了解各方立场后做出更明智的判断，避免信息茧房带来的认知偏差。"
      ],
      "metadata": {
        "id": "DI1KbwJbq3Fq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMKj0IGwgLMY"
      },
      "source": [
        "### 4. Contextual Strategy - User Context Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAHQt_AIgLMY"
      },
      "outputs": [],
      "source": [
        "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    Retrieval strategy for contextual queries integrating user context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "        user_context (str): Additional user context\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # If no user context provided, try to infer it from the query\n",
        "    if not user_context:\n",
        "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
        "For the given query, infer what contextual information might be relevant or implied\n",
        "but not explicitly stated. Focus on what background would help answering this query.\n",
        "\n",
        "Return a brief description of the implied context.\"\"\"\n",
        "\n",
        "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
        "\n",
        "        # Generate the inferred context using the LLM\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"o1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Extract and print the inferred context\n",
        "        user_context = response.choices[0].message.content.strip()\n",
        "        print(f\"Inferred context: {user_context}\")\n",
        "\n",
        "    # Reformulate the query to incorporate context\n",
        "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
        "    Given a query and some contextual information, create a more specific query that\n",
        "    incorporates the context to get more relevant information.\n",
        "\n",
        "    Return ONLY the reformulated query without explanation.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "    Context: {user_context}\n",
        "\n",
        "    Reformulate the query to incorporate this context:\"\"\"\n",
        "\n",
        "    # Generate the contextualized query using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and print the contextualized query\n",
        "    contextualized_query = response.choices[0].message.content.strip()\n",
        "    print(f\"Contextualized query: {contextualized_query}\")\n",
        "\n",
        "    # Retrieve documents based on the contextualized query\n",
        "    query_embedding = create_embeddings(contextualized_query)\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
        "\n",
        "    # Rank documents considering both relevance and user context\n",
        "    ranked_results = []\n",
        "\n",
        "    for doc in initial_results:\n",
        "        # Score document relevance considering the context\n",
        "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
        "        ranked_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"similarity\": doc[\"similarity\"],\n",
        "            \"context_relevance\": context_relevance\n",
        "        })\n",
        "\n",
        "    # Sort by context relevance and return top k results\n",
        "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
        "    return ranked_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3IGNU3gLMY"
      },
      "source": [
        "## Helper Functions for Document Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hk4edAKgLMZ"
      },
      "outputs": [],
      "source": [
        "def score_document_relevance(query, document, model=\"o1\"):\n",
        "    \"\"\"\n",
        "    Score document relevance to a query using LLM.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        document (str): Document text\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        float: Relevance score from 0-10\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the model on how to rate relevance\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
        "        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
        "        0 = Completely irrelevant\n",
        "        10 = Perfectly addresses the query\n",
        "\n",
        "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate document if it's too long\n",
        "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
        "\n",
        "    # User prompt containing the query and document preview\n",
        "    user_prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "\n",
        "        Document: {doc_preview}\n",
        "\n",
        "        Relevance score (0-10):\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the score from the model's response\n",
        "    score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract numeric score using regex\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
        "    if match:\n",
        "        score = float(match.group(1))\n",
        "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
        "    else:\n",
        "        # Default score if extraction fails\n",
        "        return 5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ_hjRKbgLMZ"
      },
      "outputs": [],
      "source": [
        "def score_document_context_relevance(query, context, document, model=\"o1\"):\n",
        "    \"\"\"\n",
        "    Score document relevance considering both query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): User context\n",
        "        document (str): Document text\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        float: Relevance score from 0-10\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the model on how to rate relevance considering context\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
        "        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
        "        when considering the provided context, where:\n",
        "        0 = Completely irrelevant\n",
        "        10 = Perfectly addresses the query in the given context\n",
        "\n",
        "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate document if it's too long\n",
        "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
        "\n",
        "    # User prompt containing the query, context, and document preview\n",
        "    user_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "    Context: {context}\n",
        "\n",
        "    Document: {doc_preview}\n",
        "\n",
        "    Relevance score considering context (0-10):\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the score from the model's response\n",
        "    score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract numeric score using regex\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
        "    if match:\n",
        "        score = float(match.group(1))\n",
        "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
        "    else:\n",
        "        # Default score if extraction fails\n",
        "        return 5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxJ5YnargLMZ"
      },
      "source": [
        "## The Core Adaptive Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEXwy4LfgLMZ"
      },
      "outputs": [],
      "source": [
        "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to retrieve\n",
        "        user_context (str): Optional user context for contextual queries\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    # Classify the query to determine its type\n",
        "    query_type = classify_query(query)\n",
        "    print(f\"Query classified as: {query_type}\")\n",
        "\n",
        "    # Select and execute the appropriate retrieval strategy based on the query type\n",
        "    if query_type == \"Factual\":\n",
        "        # Use the factual retrieval strategy for precise information\n",
        "        results = factual_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Analytical\":\n",
        "        # Use the analytical retrieval strategy for comprehensive coverage\n",
        "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Opinion\":\n",
        "        # Use the opinion retrieval strategy for diverse perspectives\n",
        "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Contextual\":\n",
        "        # Use the contextual retrieval strategy, incorporating user context\n",
        "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
        "    else:\n",
        "        # Default to factual retrieval strategy if classification fails\n",
        "        results = factual_retrieval_strategy(query, vector_store, k)\n",
        "\n",
        "    return results  # Return the retrieved documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msiCCk5sgLMZ"
      },
      "source": [
        "## Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b9j7_S7gLMZ"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, results, query_type, model=\"o1\"):\n",
        "    \"\"\"\n",
        "    Generate a response based on query, retrieved documents, and query type.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Retrieved documents\n",
        "        query_type (str): Type of query\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Prepare context from retrieved documents by joining their texts with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
        "\n",
        "    # Create custom system prompt based on query type\n",
        "    if query_type == \"Factual\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
        "    Answer the question based on the provided context. Focus on accuracy and precision.\n",
        "    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    elif query_type == \"Analytical\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
        "    Based on the provided context, offer a comprehensive analysis of the topic.\n",
        "    Cover different aspects and perspectives in your explanation.\n",
        "    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
        "\n",
        "    elif query_type == \"Opinion\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
        "    Based on the provided context, present different perspectives on the topic.\n",
        "    Ensure fair representation of diverse opinions without showing bias.\n",
        "    Acknowledge where the context presents limited viewpoints.\"\"\"\n",
        "\n",
        "    elif query_type == \"Contextual\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
        "    Answer the question considering both the query and its context.\n",
        "    Make connections between the query context and the information in the provided documents.\n",
        "    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    else:\n",
        "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    # Create user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Please provide a helpful response based on the context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response using the OpenAI client\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yywkqo5ggLMZ"
      },
      "source": [
        "## Complete RAG Pipeline with Adaptive Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms9GFldfgLMZ"
      },
      "outputs": [],
      "source": [
        "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with adaptive retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        k (int): Number of documents to retrieve\n",
        "        user_context (str): Optional user context\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, retrieved documents, query type, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Classify the query to determine its type\n",
        "    query_type = classify_query(query)\n",
        "    print(f\"Query classified as: {query_type}\")\n",
        "\n",
        "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
        "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
        "\n",
        "    # Generate a response based on the query, retrieved documents, and query type\n",
        "    response = generate_response(query, retrieved_docs, query_type)\n",
        "\n",
        "    # Compile the results into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"query_type\": query_type,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SW79Yf_gLMa"
      },
      "source": [
        "## Evaluation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfcUIu-4gLMa"
      },
      "outputs": [],
      "source": [
        "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Compare adaptive retrieval with standard retrieval on a set of test queries.\n",
        "\n",
        "    This function processes a document, runs both standard and adaptive retrieval methods\n",
        "    on each test query, and compares their performance. If reference answers are provided,\n",
        "    it also evaluates the quality of responses against these references.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document to be processed as the knowledge source\n",
        "        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n",
        "        reference_answers (List[str], optional): Reference answers for evaluation metrics\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results containing individual query results and overall comparison\n",
        "    \"\"\"\n",
        "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
        "\n",
        "    # Process document to extract text, create chunks and build the vector store\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Initialize collection for storing comparison results\n",
        "    results = []\n",
        "\n",
        "    # Process each test query with both retrieval methods\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # --- Standard retrieval approach ---\n",
        "        print(\"\\n--- Standard Retrieval ---\")\n",
        "        # Create embedding for the query\n",
        "        query_embedding = create_embeddings(query)\n",
        "        # Retrieve documents using simple vector similarity\n",
        "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
        "        # Generate response using a generic approach\n",
        "        standard_response = generate_response(query, standard_docs, \"General\")\n",
        "\n",
        "        # --- Adaptive retrieval approach ---\n",
        "        print(\"\\n--- Adaptive Retrieval ---\")\n",
        "        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
        "        query_type = classify_query(query)\n",
        "        # Retrieve documents using the strategy appropriate for this query type\n",
        "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
        "        # Generate a response tailored to the query type\n",
        "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
        "\n",
        "        # Store complete results for this query\n",
        "        result = {\n",
        "            \"query\": query,\n",
        "            \"query_type\": query_type,\n",
        "            \"standard_retrieval\": {\n",
        "                \"documents\": standard_docs,\n",
        "                \"response\": standard_response\n",
        "            },\n",
        "            \"adaptive_retrieval\": {\n",
        "                \"documents\": adaptive_docs,\n",
        "                \"response\": adaptive_response\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add reference answer if available for this query\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            result[\"reference_answer\"] = reference_answers[i]\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Display preview of both responses for quick comparison\n",
        "        print(\"\\n--- Responses ---\")\n",
        "        print(f\"Standard: {standard_response[:200]}...\")\n",
        "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
        "\n",
        "    # Calculate comparative metrics if reference answers are available\n",
        "    if reference_answers:\n",
        "        comparison = compare_responses(results)\n",
        "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "        print(comparison)\n",
        "\n",
        "    # Return the complete evaluation results\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLtGK3BFgLMa"
      },
      "outputs": [],
      "source": [
        "def compare_responses(results):\n",
        "    \"\"\"\n",
        "    Compare standard and adaptive responses against reference answers.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results containing both types of responses\n",
        "\n",
        "    Returns:\n",
        "        str: Comparison analysis\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI in comparing responses\n",
        "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
        "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
        "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
        "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
        "\n",
        "    # Initialize the comparison text with a header\n",
        "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
        "\n",
        "    # Iterate through each result to compare responses\n",
        "    for i, result in enumerate(results):\n",
        "        # Skip if there is no reference answer for the query\n",
        "        if \"reference_answer\" not in result:\n",
        "            continue\n",
        "\n",
        "        # Add query details to the comparison text\n",
        "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
        "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
        "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
        "\n",
        "        # Add standard retrieval response to the comparison text\n",
        "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
        "\n",
        "        # Add adaptive retrieval response to the comparison text\n",
        "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
        "\n",
        "        # Create the user prompt for the AI to compare the responses\n",
        "        user_prompt = f\"\"\"\n",
        "        Reference Answer: {result['reference_answer']}\n",
        "\n",
        "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
        "\n",
        "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
        "\n",
        "        Provide a detailed comparison of the two responses.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate the comparison analysis using the OpenAI client\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"o1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        # Add the AI's comparison analysis to the comparison text\n",
        "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
        "\n",
        "    return comparison_text  # Return the complete comparison analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MHSygvjgLMa"
      },
      "source": [
        "## Evaluating the Adaptive Retrieval System (Customized Queries)\n",
        "\n",
        "The final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys0be5XYgLMa"
      },
      "outputs": [],
      "source": [
        "# Path to your knowledge source document\n",
        "# This PDF file contains the information that the RAG system will use\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Define test queries covering different query types to demonstrate\n",
        "# how adaptive retrieval handles various query intentions\n",
        "test_queries = [\n",
        "    \"What is Explainable AI (XAI)?\",                                              # Factual query - seeking definition/specific information\n",
        "    # \"How do AI ethics and governance frameworks address potential societal impacts?\",  # Analytical query - requiring comprehensive analysis\n",
        "    # \"Is AI development moving too fast for proper regulation?\",                   # Opinion query - seeking diverse perspectives\n",
        "    # \"How might explainable AI help in healthcare decisions?\",                     # Contextual query - benefits from context-awareness\n",
        "]\n",
        "\n",
        "# Reference answers for more thorough evaluation\n",
        "# These can be used to objectively assess response quality against a known standard\n",
        "reference_answers = [\n",
        "    \"Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\",\n",
        "    # \"AI ethics and governance frameworks address potential societal impacts by establishing guidelines and principles to ensure AI systems are developed and used responsibly. These frameworks focus on fairness, accountability, transparency, and the protection of human rights to mitigate risks and promote beneficial output.5.\",\n",
        "    # \"Opinions on whether AI development is moving too fast for proper regulation vary. Some argue that rapid advancements outpace regulatory efforts, leading to potential risks and ethical concerns. Others believe that innovation should continue at its current pace, with regulations evolving alongside to address emerging challenges.\",\n",
        "    # \"Explainable AI can significantly aid healthcare decisions by providing transparent and understandable insights into AI-driven recommendations. This transparency helps healthcare professionals trust AI systems, make informed decisions, and improve patient output by understanding the rationale behind AI suggestions.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KoVaFb0gLMd",
        "outputId": "c11c0459-2e39-4edb-e690-18cf4e52d5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "\n",
            "\n",
            "Query 1: What is Explainable AI (XAI)?\n",
            "\n",
            "--- Standard Retrieval ---\n",
            "\n",
            "--- Adaptive Retrieval ---\n",
            "Query classified as: Factual\n",
            "Executing Factual retrieval strategy for: 'What is Explainable AI (XAI)?'\n",
            "Enhanced query: What are the core principles, methodologies, and real-world applications of Explainable AI (XAI) in improving transparency and interpretability of machine learning models?\n",
            "\n",
            "--- Responses ---\n",
            "Standard: Explainable AI (XAI) refers to techniques and approaches designed to make an AI system’s decision-making process more transparent and understandable. It helps users see how and why a particular outcom...\n",
            "Adaptive: Explainable AI (XAI) refers to methods and techniques designed to make AI systems and their decision-making processes more transparent and understandable to humans. By offering insights into how AI mo...\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "# Evaluation of Standard vs. Adaptive Retrieval\n",
            "\n",
            "## Query 1: What is Explainable AI (XAI)?\n",
            "*Query Type: Factual*\n",
            "\n",
            "**Reference Answer:**\n",
            "Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\n",
            "\n",
            "**Standard Retrieval Response:**\n",
            "Explainable AI (XAI) refers to techniques and approaches designed to make an AI system’s decision-making process more transparent and understandable. It helps users see how and why a particular outcome was reached, enabling them to assess the system’s reliability, fairness, and accountability. By providing insights into AI decisions, XAI fosters greater trust, facilitates better oversight, and supports responsible deployment of AI technologies.\n",
            "\n",
            "**Adaptive Retrieval Response:**\n",
            "Explainable AI (XAI) refers to methods and techniques designed to make AI systems and their decision-making processes more transparent and understandable to humans. By offering insights into how AI models arrive at specific outcomes, XAI helps users assess the fairness, accuracy, and reliability of AI-driven decisions. This transparency is essential for building trust, ensuring accountability, and addressing ethical concerns such as bias, privacy, and responsible data handling.\n",
            "\n",
            "**Comparison Analysis:**\n",
            "Below is a comparative analysis of the two responses—Standard Retrieval and Adaptive Retrieval—relative to the reference answer. The assessment considers accuracy, relevance, comprehensiveness, and alignment with the reference.\n",
            "\n",
            "1. Accuracy and Alignment with the Reference Answer  \n",
            "   • Both responses accurately convey the core purpose of Explainable AI (XAI): to make AI systems transparent and understandable so that users can trust and manage them effectively.  \n",
            "   • Each explanation aligns well with the reference answer’s emphasis on how decisions are made and the importance of transparency and trust.  \n",
            "\n",
            "2. Relevance to the Topic  \n",
            "   • Standard Retrieval: Concentrates more on the direct benefits of XAI in terms of transparency and oversight. It succinctly covers decision-making transparency, trust, fairness, reliability, and accountability.  \n",
            "   • Adaptive Retrieval: Also includes these central concepts but extends further by explicitly mentioning “ethical concerns such as bias, privacy, and responsible data handling.” This additional level of detail demonstrates broader relevance to current concerns in the field of AI ethics.  \n",
            "\n",
            "3. Comprehensiveness  \n",
            "   • Standard Retrieval:  \n",
            "     – Strengths: Offers a concise yet clear view of why XAI is important. It appropriately highlights trust, reliability, fairness, and accountability.  \n",
            "     – Weaknesses: It does not directly address certain specifics, such as bias or privacy. These considerations are implicit in responsible AI discussions but are not spelled out.  \n",
            "   • Adaptive Retrieval:  \n",
            "     – Strengths: Covers a wider range of ethical and practical aspects, including fairness, accuracy, reliability, accountability, bias, privacy, and responsible data handling. This gives it a more holistic angle, reflecting a deeper understanding of why XAI is critical.  \n",
            "     – Weaknesses: Could be seen as slightly more general rather than deeply focused on a specific concern, because it references multiple high-level ethical topics. However, this does not detract significantly from its clarity.  \n",
            "\n",
            "4. Strengths and Weaknesses  \n",
            "   • Strengths of Standard Retrieval:  \n",
            "     – Direct, concise, and immediately highlights the trust-building and oversight benefits of XAI.  \n",
            "     – Closely aligns with the reference answer’s main theme of transparency and trust.  \n",
            "   • Weaknesses of Standard Retrieval:  \n",
            "     – Less explicit mention of broader ethical issues (bias, privacy, etc.) that are increasingly important in discussions of AI explainability.  \n",
            "\n",
            "   • Strengths of Adaptive Retrieval:  \n",
            "     – More comprehensive by addressing fairness, bias, privacy, and responsible data handling, thus offering a rounder perspective on XAI.  \n",
            "     – Aligns with the reference answer’s intent and expands on important ethical and accountability factors.  \n",
            "   • Weaknesses of Adaptive Retrieval:  \n",
            "     – The additional detail may be more general than the strictly core explanation. Some readers might prefer the brevity of the Standard Retrieval approach.  \n",
            "\n",
            "5. Overall Evaluation  \n",
            "   • Both responses are accurate, relevant, and closely aligned with the reference answer.  \n",
            "   • The Standard Retrieval response is succinct and directly addresses key points: transparency, trust, and responsibility.  \n",
            "   • The Adaptive Retrieval response covers the same ground but adds depth on ethical and social considerations.  \n",
            "\n",
            "In summary, the Adaptive Retrieval response is more expansive in scope, covering ethical and legal dimensions like bias and privacy, while the Standard Retrieval response is more concise but still covers the main points of XAI’s purpose. Both interpretations fulfill the core requirement of explaining XAI in a way that emphasizes transparency, trust, and responsible AI deployment, with the Adaptive Retrieval providing a slightly richer and more contextualized view.\n",
            "\n",
            "\n",
            "# Evaluation of Standard vs. Adaptive Retrieval\n",
            "\n",
            "## Query 1: What is Explainable AI (XAI)?\n",
            "*Query Type: Factual*\n",
            "\n",
            "**Reference Answer:**\n",
            "Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\n",
            "\n",
            "**Standard Retrieval Response:**\n",
            "Explainable AI (XAI) refers to techniques and approaches designed to make an AI system’s decision-making process more transparent and understandable. It helps users see how and why a particular outcome was reached, enabling them to assess the system’s reliability, fairness, and accountability. By providing insights into AI decisions, XAI fosters greater trust, facilitates better oversight, and supports responsible deployment of AI technologies.\n",
            "\n",
            "**Adaptive Retrieval Response:**\n",
            "Explainable AI (XAI) refers to methods and techniques designed to make AI systems and their decision-making processes more transparent and understandable to humans. By offering insights into how AI models arrive at specific outcomes, XAI helps users assess the fairness, accuracy, and reliability of AI-driven decisions. This transparency is essential for building trust, ensuring accountability, and addressing ethical concerns such as bias, privacy, and responsible data handling.\n",
            "\n",
            "**Comparison Analysis:**\n",
            "Below is a comparative analysis of the two responses—Standard Retrieval and Adaptive Retrieval—relative to the reference answer. The assessment considers accuracy, relevance, comprehensiveness, and alignment with the reference.\n",
            "\n",
            "1. Accuracy and Alignment with the Reference Answer  \n",
            "   • Both responses accurately convey the core purpose of Explainable AI (XAI): to make AI systems transparent and understandable so that users can trust and manage them effectively.  \n",
            "   • Each explanation aligns well with the reference answer’s emphasis on how decisions are made and the importance of transparency and trust.  \n",
            "\n",
            "2. Relevance to the Topic  \n",
            "   • Standard Retrieval: Concentrates more on the direct benefits of XAI in terms of transparency and oversight. It succinctly covers decision-making transparency, trust, fairness, reliability, and accountability.  \n",
            "   • Adaptive Retrieval: Also includes these central concepts but extends further by explicitly mentioning “ethical concerns such as bias, privacy, and responsible data handling.” This additional level of detail demonstrates broader relevance to current concerns in the field of AI ethics.  \n",
            "\n",
            "3. Comprehensiveness  \n",
            "   • Standard Retrieval:  \n",
            "     – Strengths: Offers a concise yet clear view of why XAI is important. It appropriately highlights trust, reliability, fairness, and accountability.  \n",
            "     – Weaknesses: It does not directly address certain specifics, such as bias or privacy. These considerations are implicit in responsible AI discussions but are not spelled out.  \n",
            "   • Adaptive Retrieval:  \n",
            "     – Strengths: Covers a wider range of ethical and practical aspects, including fairness, accuracy, reliability, accountability, bias, privacy, and responsible data handling. This gives it a more holistic angle, reflecting a deeper understanding of why XAI is critical.  \n",
            "     – Weaknesses: Could be seen as slightly more general rather than deeply focused on a specific concern, because it references multiple high-level ethical topics. However, this does not detract significantly from its clarity.  \n",
            "\n",
            "4. Strengths and Weaknesses  \n",
            "   • Strengths of Standard Retrieval:  \n",
            "     – Direct, concise, and immediately highlights the trust-building and oversight benefits of XAI.  \n",
            "     – Closely aligns with the reference answer’s main theme of transparency and trust.  \n",
            "   • Weaknesses of Standard Retrieval:  \n",
            "     – Less explicit mention of broader ethical issues (bias, privacy, etc.) that are increasingly important in discussions of AI explainability.  \n",
            "\n",
            "   • Strengths of Adaptive Retrieval:  \n",
            "     – More comprehensive by addressing fairness, bias, privacy, and responsible data handling, thus offering a rounder perspective on XAI.  \n",
            "     – Aligns with the reference answer’s intent and expands on important ethical and accountability factors.  \n",
            "   • Weaknesses of Adaptive Retrieval:  \n",
            "     – The additional detail may be more general than the strictly core explanation. Some readers might prefer the brevity of the Standard Retrieval approach.  \n",
            "\n",
            "5. Overall Evaluation  \n",
            "   • Both responses are accurate, relevant, and closely aligned with the reference answer.  \n",
            "   • The Standard Retrieval response is succinct and directly addresses key points: transparency, trust, and responsibility.  \n",
            "   • The Adaptive Retrieval response covers the same ground but adds depth on ethical and social considerations.  \n",
            "\n",
            "In summary, the Adaptive Retrieval response is more expansive in scope, covering ethical and legal dimensions like bias and privacy, while the Standard Retrieval response is more concise but still covers the main points of XAI’s purpose. Both interpretations fulfill the core requirement of explaining XAI in a way that emphasizes transparency, trust, and responsible AI deployment, with the Adaptive Retrieval providing a slightly richer and more contextualized view.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation comparing adaptive vs standard retrieval\n",
        "# This will process each query using both methods and compare the results\n",
        "evaluation_results = evaluate_adaptive_vs_standard(\n",
        "    pdf_path=pdf_path,                  # Source document for knowledge extraction\n",
        "    test_queries=test_queries,          # List of test queries to evaluate\n",
        "    reference_answers=reference_answers  # Optional ground truth for comparison\n",
        ")\n",
        "\n",
        "# The results will show a detailed comparison between standard retrieval and\n",
        "# adaptive retrieval performance across different query types, highlighting\n",
        "# where adaptive strategies provide improved outcomes\n",
        "print(evaluation_results[\"comparison\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自适应检索：让RAG系统像人类一样\"见机行事\"\n",
        "\n",
        "你发的代码实现了一个\"会思考\"的RAG系统——它能根据问题类型自动切换检索策略，就像人遇到不同问题会用不同思路解答一样。下面我用最通俗的语言拆解这个系统的核心逻辑：\n",
        "\n",
        "\n",
        "### 一、给问题\"贴标签\"：查询分类器\n",
        "\n",
        "想象你去图书馆问问题，图书管理员会先判断你要找什么类型的信息。这个系统的`classify_query`函数就是干这个的：\n",
        "\n",
        "```python\n",
        "def classify_query(query):\n",
        "    # 让AI当\"问题侦探\"，把问题分成4类\n",
        "    # 事实型：比如\"谁发明了电灯？\"（要具体答案）\n",
        "    # 分析型：比如\"为什么手机会影响睡眠？\"（要原因分析）\n",
        "    # 观点型：比如\"你觉得元宇宙有前途吗？\"（要不同看法）\n",
        "    # 上下文型：比如\"之前说的那个算法，能举个例子吗？\"（依赖前面对话）\n",
        "    # 最后只返回类别名称，不啰嗦\n",
        "```\n",
        "\n",
        "举个例子，当用户问\"地球为什么会自转？\"，系统会识别这是**分析型**问题，需要找因果关系的资料。\n",
        "\n",
        "\n",
        "### 二、4种\"解题思路\"：不同问题不同策略\n",
        "\n",
        "#### 1. 事实型问题：像查字典一样精准\n",
        "\n",
        "```python\n",
        "def factual_retrieval_strategy(query):\n",
        "    # 比如用户问\"相对论的提出者是谁？\"\n",
        "    # 第一步：把问题变得更精准，比如变成\"爱因斯坦什么时候提出相对论？\"\n",
        "    # 第二步：用优化后的问题检索，只找最匹配的几段话\n",
        "    # 第三步：让AI给每段话打分（0-10分），只返回最高分的结果\n",
        "```\n",
        "\n",
        "关键技巧：把模糊问题变成精准问题，就像查字典时知道准确词条更容易找到答案。\n",
        "\n",
        "#### 2. 分析型问题：像拼图一样找全面信息\n",
        "\n",
        "```python\n",
        "def analytical_retrieval_strategy(query):\n",
        "    # 比如用户问\"人工智能如何影响就业？\"\n",
        "    # 第一步：把大问题拆成小问题，比如：\n",
        "    #   \"AI对哪些行业影响最大？\"\n",
        "    #   \"AI创造了哪些新职业？\"\n",
        "    #   \"AI导致失业的案例有哪些？\"\n",
        "    # 第二步：每个小问题都找资料，再把结果拼起来\n",
        "    # 第三步：去重（比如不同小问题查到相同内容），保证信息全面不重复\n",
        "```\n",
        "\n",
        "核心逻辑：复杂问题拆分成多个小问题，就像拼图时先找各个小块再拼完整。\n",
        "\n",
        "#### 3. 观点型问题：像辩论赛一样找不同观点\n",
        "\n",
        "```python\n",
        "def opinion_retrieval_strategy(query):\n",
        "    # 比如用户问\"自动驾驶该不该普及？\"\n",
        "    # 第一步：找出不同立场，比如：\n",
        "    #   \"支持普及的理由\"\n",
        "    #   \"反对普及的风险\"\n",
        "    #   \"中立的技术挑战\"\n",
        "    # 第二步：每个立场都找对应的资料\n",
        "    # 第三步：把不同立场的观点整理好，不偏不倚地呈现\n",
        "```\n",
        "\n",
        "独特之处：不直接给答案，而是摆出各方观点，就像辩论赛评委汇总正反方论点。\n",
        "\n",
        "#### 4. 上下文型问题：像聊天一样理解前因后果\n",
        "\n",
        "```python\n",
        "def contextual_retrieval_strategy(query, user_context):\n",
        "    # 比如用户先问\"什么是机器学习\"，接着问\"那深度学习呢？\"\n",
        "    # 系统会记住前面的问题，把当前问题理解为\"深度学习和机器学习的关系\"\n",
        "    # 第一步：如果用户没给上下文，就猜可能的背景（比如推断用户是学生，需要基础解释）\n",
        "    # 第二步：把问题和上下文结合，比如变成\"学生需要知道的深度学习与机器学习区别\"\n",
        "    # 第三步：找资料时优先匹配有上下文关联的内容\n",
        "```\n",
        "\n",
        "关键能力：能记住对话历史，像人类聊天一样理解\"前因后果\"，避免答非所问。\n",
        "\n",
        "\n",
        "### 三、给资料\"打分\"：LLM当裁判\n",
        "\n",
        "```python\n",
        "def score_document_relevance(query, document):\n",
        "    # 比如用户问\"Python怎么写循环\"，查到一段讲Java循环的资料\n",
        "    # LLM会给这段资料打低分（比如2分），因为不相关\n",
        "    # 而查到Python循环的资料会打高分（比如9分）\n",
        "    # 最后只返回高分的资料给用户\n",
        "```\n",
        "\n",
        "这个打分函数是系统的\"火眼金睛\"，能避免把不相关的信息返回给用户，就像老师批改作业时判断答案是否切题。\n",
        "\n",
        "\n",
        "### 四、完整工作流程：从问题到答案的旅程\n",
        "\n",
        "1. **用户提问**：比如\"为什么树叶会变黄？\"\n",
        "2. **问题分类**：系统判断这是**分析型**问题（需要原因分析）\n",
        "3. **策略选择**：启动分析型检索策略\n",
        "4. **拆分成小问题**：\n",
        "   - \"树叶变黄的主要因素有哪些？\"\n",
        "   - \"季节变化如何影响树叶颜色？\"\n",
        "   - \"叶绿素分解的过程是怎样的？\"\n",
        "5. **分别检索**：每个小问题找相关资料\n",
        "6. **整理结果**：把各个小问题的答案拼起来，形成完整解释\n",
        "7. **生成回答**：用自然语言把资料里的信息讲清楚\n",
        "\n",
        "\n",
        "### 五、和普通RAG的区别：为什么自适应更聪明？\n",
        "\n",
        "普通RAG就像用同一把锤子敲所有钉子，而自适应RAG会根据钉子类型选工具：\n",
        "\n",
        "- **普通RAG**：不管什么问题，都用\"关键词匹配\"找资料，可能把不相关的信息返回（比如问\"苹果手机\"却返回\"苹果种植\"的资料）\n",
        "- **自适应RAG**：\n",
        "  - 问事实型问题时，像字典一样精准\n",
        "  - 问分析型问题时，像研究员一样全面\n",
        "  - 问观点型问题时，像辩论会一样中立\n",
        "  - 问上下文问题时，像聊天一样懂前因后果\n",
        "\n",
        "\n",
        "### 六、实际应用场景举例\n",
        "\n",
        "#### 场景1：学生学习\n",
        "\n",
        "- **问题**：\"量子力学的基本原理是什么？\"（事实型）\n",
        "- **系统行为**：\n",
        "  1. 把问题优化为\"量子力学的五大基本原理详解\"\n",
        "  2. 精准找到教材中定义和公式的段落\n",
        "  3. 返回简洁准确的定义，不带多余解释\n",
        "\n",
        "#### 场景2：职场决策\n",
        "\n",
        "- **问题**：\"AI工具该不该引入我们部门？\"（观点型）\n",
        "- **系统行为**：\n",
        "  1. 找出\"支持引入的3个理由\"和\"反对引入的2个风险\"\n",
        "  2. 分别从效率提升、成本节约、员工培训等角度找资料\n",
        "  3. 呈现正反方观点，帮助用户做决策\n",
        "\n",
        "#### 场景3：技术咨询\n",
        "\n",
        "- **问题**：\"之前说的那个推荐算法，在电商场景怎么用？\"（上下文型）\n",
        "- **系统行为**：\n",
        "  1. 记住之前讨论过\"协同过滤算法\"\n",
        "  2. 把当前问题理解为\"协同过滤算法在电商的应用案例\"\n",
        "  3. 找电商推荐系统的具体案例和数据，不重复讲算法原理\n",
        "\n",
        "\n",
        "### 七、总结：让机器像人一样\"灵活思考\"\n",
        "\n",
        "这个自适应RAG系统的核心价值在于：它不是机械地匹配关键词，而是像人类一样理解问题的\"意图\"和\"类型\"，再选择最合适的方法找答案。就像你问朋友问题时，对方会根据问题类型决定是给你查字典、摆事实、讲道理还是回忆之前的对话——这种灵活性让AI的回答更精准、更有用。"
      ],
      "metadata": {
        "id": "O5l9zOCljtJr"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}