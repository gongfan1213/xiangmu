{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "OAoKFr2WxLWm"
      },
      "source": [
        "# Proposition Chunking for Enhanced RAG\n",
        "\n",
        "In this notebook, I implement proposition chunking - an advanced technique to break down documents into atomic, factual statements for more accurate retrieval. Unlike traditional chunking that simply divides text by character count, proposition chunking preserves the semantic integrity of individual facts.\n",
        "\n",
        "Proposition chunking delivers more precise retrieval by:\n",
        "\n",
        "1. Breaking content into atomic, self-contained facts\n",
        "2. Creating smaller, more granular units for retrieval  \n",
        "3. Enabling more precise matching between queries and relevant content\n",
        "4. Filtering out low-quality or incomplete propositions\n",
        "\n",
        "Let's build a complete implementation without relying on LangChain or FAISS."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在本笔记本中，我实现了命题分块，这是一种高级技术，可以将文档分解为原子的事实陈述，以便更准确地检索。与传统的按字符数划分文本不同，命题分块保留了单个事实的语义完整性。命题分块通过以下方式提供更精确的检索：将内容分解为原子的、自包含的事实；为检索创建更小、更细粒度的单元；在查询和相关内容之间实现更精确的匹配；过滤掉低质量或不完整的命题。"
      ],
      "metadata": {
        "id": "_ookwV52xRhh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKd-ZyU_xLWp"
      },
      "source": [
        "## Setting Up the Environment\n",
        "We begin by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PymuPdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z1mKHGmxX0S",
        "outputId": "33feb863-2d99-438b-97b2-f0d50cabd90f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PymuPdf\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PymuPdf\n",
            "Successfully installed PymuPdf-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BUOfsgf7xLWp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7to1tpXlxLWq"
      },
      "source": [
        "## Extracting Text from a PDF File\n",
        "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cmcyqoaPxLWq"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWyjwxiYxLWq"
      },
      "source": [
        "## Chunking the Extracted Text\n",
        "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mas5x3-oxLWr"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=800, overlap=100):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunk dictionaries with text and metadata\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Iterate over the text with the specified chunk size and overlap\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
        "        if chunk:  # Ensure we don't add empty chunks\n",
        "            chunks.append({\n",
        "                \"text\": chunk,  # The chunk text\n",
        "                \"chunk_id\": len(chunks) + 1,  # Unique ID for the chunk\n",
        "                \"start_char\": i,  # Starting character index of the chunk\n",
        "                \"end_char\": i + len(chunk)  # Ending character index of the chunk\n",
        "            })\n",
        "\n",
        "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
        "    return chunks  # Return the list of chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pEFfthYxLWr"
      },
      "source": [
        "## Setting Up the OpenAI API Client\n",
        "We initialize the OpenAI client to generate embeddings and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AnxS47ESxLWr"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"http://xxxxx/v1/\",\n",
        "    api_key=\"skxxxxxxxxxx9\" # Retrieve the API key from environment variables\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh6Fl6B6xLWs"
      },
      "source": [
        "## Simple Vector Store Implementation\n",
        "We'll create a basic vector store to manage document chunks and their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gb6CrerwxLWs"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize lists to store vectors, texts, and metadata\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text content\n",
        "            embedding (List[float]): The embedding vector\n",
        "            metadata (Dict, optional): Additional metadata\n",
        "        \"\"\"\n",
        "        # Append the embedding, text, and metadata to their respective lists\n",
        "        self.vectors.append(np.array(embedding))\n",
        "        self.texts.append(text)\n",
        "        self.metadata.append(metadata or {})\n",
        "\n",
        "    def add_items(self, texts, embeddings, metadata_list=None):\n",
        "        \"\"\"\n",
        "        Add multiple items to the vector store.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of text contents\n",
        "            embeddings (List[List[float]]): List of embedding vectors\n",
        "            metadata_list (List[Dict], optional): List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        # If no metadata list is provided, create an empty dictionary for each text\n",
        "        if metadata_list is None:\n",
        "            metadata_list = [{} for _ in range(len(texts))]\n",
        "\n",
        "        # Add each text, embedding, and metadata to the store\n",
        "        for text, embedding, metadata in zip(texts, embeddings, metadata_list):\n",
        "            self.add_item(text, embedding, metadata)\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "            query_embedding (List[float]): Query embedding vector\n",
        "            k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Top k most similar items\n",
        "        \"\"\"\n",
        "        # Return an empty list if there are no vectors in the store\n",
        "        if not self.vectors:\n",
        "            return []\n",
        "\n",
        "        # Convert query embedding to a numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))\n",
        "\n",
        "        # Sort by similarity in descending order\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Collect the top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],\n",
        "                \"metadata\": self.metadata[idx],\n",
        "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我来详细解读这个 `SimpleVectorStore` 类的代码：\n",
        "\n",
        "## 代码整体功能\n",
        "\n",
        "这是一个**简单的向量存储实现**，用于存储文本及其对应的向量表示，并支持相似性搜索。这是RAG（检索增强生成）系统中的核心组件。\n",
        "\n",
        "---\n",
        "\n",
        "## 类结构分析\n",
        "\n",
        "### 1. **初始化方法 `__init__`**\n",
        "```python\n",
        "def __init__(self):\n",
        "    self.vectors = []      # 存储向量\n",
        "    self.texts = []        # 存储文本\n",
        "    self.metadata = []     # 存储元数据\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 创建三个列表来分别存储向量、文本和元数据\n",
        "- 使用列表结构，简单但效率较低（实际应用中通常使用数据库）\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **添加单个项目 `add_item`**\n",
        "```python\n",
        "def add_item(self, text, embedding, metadata=None):\n",
        "    self.vectors.append(np.array(embedding))\n",
        "    self.texts.append(text)\n",
        "    self.metadata.append(metadata or {})\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 将文本、向量和元数据添加到存储中\n",
        "- 将向量转换为NumPy数组以确保一致性\n",
        "- 如果没有元数据，使用空字典作为默认值\n",
        "\n",
        "**参数说明：**\n",
        "- `text`: 文本内容\n",
        "- `embedding`: 文本的向量表示\n",
        "- `metadata`: 可选的元数据（如文档来源、时间戳等）\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **批量添加项目 `add_items`**\n",
        "```python\n",
        "def add_items(self, texts, embeddings, metadata_list=None):\n",
        "    if metadata_list is None:\n",
        "        metadata_list = [{} for _ in range(len(texts))]\n",
        "    \n",
        "    for text, embedding, metadata in zip(texts, embeddings, metadata_list):\n",
        "        self.add_item(text, embedding, metadata)\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 批量添加多个文本和对应的向量\n",
        "- 如果没有提供元数据列表，为每个文本创建空元数据\n",
        "- 使用 `zip` 函数并行处理三个列表\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **相似性搜索 `similarity_search`**\n",
        "```python\n",
        "def similarity_search(self, query_embedding, k=5):\n",
        "    if not self.vectors:\n",
        "        return []\n",
        "    \n",
        "    query_vector = np.array(query_embedding)\n",
        "    \n",
        "    # 计算余弦相似度\n",
        "    similarities = []\n",
        "    for i, vector in enumerate(self.vectors):\n",
        "        similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "        similarities.append((i, similarity))\n",
        "    \n",
        "    # 按相似度排序\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # 返回前k个结果\n",
        "    results = []\n",
        "    for i in range(min(k, len(similarities))):\n",
        "        idx, score = similarities[i]\n",
        "        results.append({\n",
        "            \"text\": self.texts[idx],\n",
        "            \"metadata\": self.metadata[idx],\n",
        "            \"similarity\": float(score)\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 根据查询向量找到最相似的k个文本\n",
        "- 使用**余弦相似度**计算向量间的相似性\n",
        "- 返回排序后的结果，包含文本、元数据和相似度分数\n",
        "\n",
        "---\n",
        "\n",
        "## 核心算法：余弦相似度\n",
        "\n",
        "```python\n",
        "similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "```\n",
        "\n",
        "**数学原理：**\n",
        "- 余弦相似度 = 向量点积 / (向量A的模 × 向量B的模)\n",
        "- 结果范围：[-1, 1]，1表示完全相同，0表示无关，-1表示完全相反\n",
        "- 适合衡量语义相似性，因为关注方向而非大小\n",
        "\n",
        "---\n",
        "\n",
        "## 使用示例\n",
        "\n",
        "```python\n",
        "# 创建向量存储\n",
        "vector_store = SimpleVectorStore()\n",
        "\n",
        "# 添加文档\n",
        "texts = [\"机器学习很有趣\", \"深度学习是AI的子集\", \"自然语言处理很重要\"]\n",
        "embeddings = [[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5]]\n",
        "metadata = [{\"source\": \"doc1\"}, {\"source\": \"doc2\"}, {\"source\": \"doc3\"}]\n",
        "\n",
        "vector_store.add_items(texts, embeddings, metadata)\n",
        "\n",
        "# 搜索相似文档\n",
        "query_embedding = [0.15, 0.25, 0.35]\n",
        "results = vector_store.similarity_search(query_embedding, k=2)\n",
        "\n",
        "# 输出结果\n",
        "for result in results:\n",
        "    print(f\"文本: {result['text']}\")\n",
        "    print(f\"相似度: {result['similarity']:.3f}\")\n",
        "    print(f\"元数据: {result['metadata']}\")\n",
        "    print(\"---\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 优缺点分析\n",
        "\n",
        "### 优点：\n",
        "1. **简单易懂**：代码结构清晰，易于理解\n",
        "2. **功能完整**：支持添加、搜索等基本功能\n",
        "3. **灵活性**：支持元数据存储\n",
        "\n",
        "### 缺点：\n",
        "1. **性能问题**：使用列表存储，搜索时需要遍历所有向量\n",
        "2. **内存效率低**：所有数据存在内存中\n",
        "3. **扩展性差**：不适合大规模数据\n",
        "4. **缺少优化**：没有使用索引或近似搜索\n",
        "\n",
        "---\n",
        "\n",
        "## 实际应用中的改进\n",
        "\n",
        "在实际的RAG系统中，通常会使用：\n",
        "- **向量数据库**：如Pinecone、Weaviate、Qdrant\n",
        "- **近似搜索**：如HNSW、IVF等算法\n",
        "- **索引优化**：提高搜索效率\n",
        "- **持久化存储**：支持大规模数据\n",
        "\n",
        "这个 `SimpleVectorStore` 是一个很好的教学示例，展示了向量存储的基本原理！"
      ],
      "metadata": {
        "id": "oTC3ERYE07Op"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgkixF8YxLWs"
      },
      "source": [
        "## Creating Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jUSQDp4RxLWs"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"\n",
        "    Create embeddings for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (str or List[str]): Input text(s)\n",
        "        model (str): Embedding model name\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: Embedding vector(s)\n",
        "    \"\"\"\n",
        "    # Handle both string and list inputs\n",
        "    input_texts = texts if isinstance(texts, list) else [texts]\n",
        "\n",
        "    # Process in batches if needed (OpenAI API limits)\n",
        "    batch_size = 100\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Iterate over the input texts in batches\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
        "\n",
        "        # Create embeddings for the current batch\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=batch\n",
        "        )\n",
        "\n",
        "        # Extract embeddings from the response\n",
        "        batch_embeddings = [item.embedding for item in response.data]\n",
        "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
        "\n",
        "    # If input was a single string, return just the first embedding\n",
        "    if isinstance(texts, str):\n",
        "        return all_embeddings[0]\n",
        "\n",
        "    # Otherwise, return all embeddings\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PABo0UBXxLWs"
      },
      "source": [
        "## Proposition Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZl0hrM-xLWs"
      },
      "outputs": [],
      "source": [
        "def generate_propositions(chunk):\n",
        "    \"\"\"\n",
        "    Generate atomic, self-contained propositions from a text chunk.\n",
        "\n",
        "    Args:\n",
        "        chunk (Dict): Text chunk with content and metadata\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of generated propositions\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to generate propositions\n",
        "    system_prompt = \"\"\"Please break down the following text into simple, self-contained propositions.\n",
        "    Ensure that each proposition meets the following criteria:\n",
        "\n",
        "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
        "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
        "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
        "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
        "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\n",
        "\n",
        "    Output ONLY the list of propositions without any additional text or explanations.\"\"\"\n",
        "\n",
        "    # User prompt containing the text chunk to be converted into propositions\n",
        "    user_prompt = f\"Text to convert into propositions:\\n\\n{chunk['text']}\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",  # Using a stronger model for accurate proposition generation\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract propositions from the response\n",
        "    raw_propositions = response.choices[0].message.content.strip().split('\\n')\n",
        "\n",
        "    # Clean up propositions (remove numbering, bullets, etc.)\n",
        "    clean_propositions = []\n",
        "    for prop in raw_propositions:\n",
        "        # Remove numbering (1., 2., etc.) and bullet points\n",
        "        cleaned = re.sub(r'^\\s*(\\d+\\.|\\-|\\*)\\s*', '', prop).strip()\n",
        "        if cleaned and len(cleaned) > 10:  # Simple filter for empty or very short propositions\n",
        "            clean_propositions.append(cleaned)\n",
        "\n",
        "    return clean_propositions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我来详细讲解这个 `generate_propositions` 函数的代码：\n",
        "\n",
        "## 函数整体功能\n",
        "\n",
        "这个函数用于**将文本块分解为原子化的、自包含的命题**。这是RAG系统中一种高级的文本分块技术，比简单的按长度分块更智能。\n",
        "\n",
        "---\n",
        "\n",
        "## 代码结构分析\n",
        "\n",
        "### 1. **函数定义和文档**\n",
        "```python\n",
        "def generate_propositions(chunk):\n",
        "    \"\"\"\n",
        "    Generate atomic, self-contained propositions from a text chunk.\n",
        "    \n",
        "    Args:\n",
        "        chunk (Dict): Text chunk with content and metadata\n",
        "        \n",
        "    Returns:\n",
        "        List[str]: List of generated propositions\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**功能说明：**\n",
        "- 输入：包含文本内容和元数据的字典\n",
        "- 输出：生成的命题列表\n",
        "- 目标：将复杂文本分解为简单、独立的命题\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **系统提示词（System Prompt）**\n",
        "```python\n",
        "system_prompt = \"\"\"Please break down the following text into simple, self-contained propositions.\n",
        "    Ensure that each proposition meets the following criteria:\n",
        "\n",
        "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
        "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
        "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
        "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
        "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\n",
        "\n",
        "    Output ONLY the list of propositions without any additional text or explanations.\"\"\"\n",
        "```\n",
        "\n",
        "**详细解析：**\n",
        "\n",
        "#### 五个核心标准：\n",
        "\n",
        "1. **表达单一事实**：每个命题只陈述一个具体事实或主张\n",
        "   - 例如：❌ \"苹果公司发布了iPhone 15，并且价格很高\"\n",
        "   - ✅ \"苹果公司发布了iPhone 15\" 和 \"iPhone 15价格很高\"\n",
        "\n",
        "2. **无需上下文即可理解**：命题应该是自包含的\n",
        "   - 例如：❌ \"它很贵\"（需要知道\"它\"指什么）\n",
        "   - ✅ \"iPhone 15价格很高\"\n",
        "\n",
        "3. **使用完整名称而非代词**：避免模糊引用\n",
        "   - 例如：❌ \"他发明了电话\"\n",
        "   - ✅ \"亚历山大·格雷厄姆·贝尔发明了电话\"\n",
        "\n",
        "4. **包含相关日期/限定词**：使事实更精确\n",
        "   - 例如：✅ \"苹果公司于2023年9月发布了iPhone 15\"\n",
        "\n",
        "5. **包含单一主谓关系**：避免复杂从句\n",
        "   - 例如：❌ \"虽然价格很高，但iPhone 15很受欢迎\"\n",
        "   - ✅ \"iPhone 15价格很高\" 和 \"iPhone 15很受欢迎\"\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **用户提示词（User Prompt）**\n",
        "```python\n",
        "user_prompt = f\"Text to convert into propositions:\\n\\n{chunk['text']}\"\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 将待处理的文本块插入到提示词中\n",
        "- 使用 f-string 格式化，确保文本正确传递\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **调用语言模型**\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Using a stronger model for accurate proposition generation\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ],\n",
        "    temperature=0\n",
        ")\n",
        "```\n",
        "\n",
        "**参数解析：**\n",
        "- **模型**：使用 Llama-3.2-3B-Instruct，这是一个相对强大的模型\n",
        "- **消息格式**：标准的聊天完成格式，包含系统提示和用户提示\n",
        "- **temperature=0**：确保输出的一致性，减少随机性\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **提取和清理命题**\n",
        "```python\n",
        "raw_propositions = response.choices[0].message.content.strip().split('\\n')\n",
        "\n",
        "clean_propositions = []\n",
        "for prop in raw_propositions:\n",
        "    # Remove numbering (1., 2., etc.) and bullet points\n",
        "    cleaned = re.sub(r'^\\s*(\\d+\\.|\\-|\\*)\\s*', '', prop).strip()\n",
        "    if cleaned and len(cleaned) > 10:  # Simple filter for empty or very short propositions\n",
        "        clean_propositions.append(cleaned)\n",
        "```\n",
        "\n",
        "**处理步骤：**\n",
        "\n",
        "1. **提取原始响应**：从模型输出中获取内容\n",
        "2. **按行分割**：将响应按换行符分割成列表\n",
        "3. **正则表达式清理**：\n",
        "   - `r'^\\s*(\\d+\\.|\\-|\\*)\\s*'` 匹配行首的编号、破折号或星号\n",
        "   - 移除这些格式标记\n",
        "4. **质量过滤**：\n",
        "   - 过滤空字符串\n",
        "   - 过滤过短的命题（少于10个字符）\n",
        "\n",
        "---\n",
        "\n",
        "## 使用示例\n",
        "\n",
        "```python\n",
        "# 示例输入\n",
        "chunk = {\n",
        "    \"text\": \"苹果公司于2023年9月发布了iPhone 15。这款手机采用了A17 Pro芯片，性能比上一代提升了20%。iPhone 15的起售价为799美元，比iPhone 14贵了100美元。\",\n",
        "    \"metadata\": {\"source\": \"tech_news\", \"date\": \"2023-09-12\"}\n",
        "}\n",
        "\n",
        "# 调用函数\n",
        "propositions = generate_propositions(chunk)\n",
        "\n",
        "# 预期输出\n",
        "# [\n",
        "#     \"苹果公司于2023年9月发布了iPhone 15\",\n",
        "#     \"iPhone 15采用了A17 Pro芯片\",\n",
        "#     \"iPhone 15的性能比上一代提升了20%\",\n",
        "#     \"iPhone 15的起售价为799美元\",\n",
        "#     \"iPhone 15比iPhone 14贵了100美元\"\n",
        "# ]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 在RAG系统中的作用\n",
        "\n",
        "### 1. **提高检索精度**\n",
        "- 将复杂文档分解为原子事实\n",
        "- 更精确地匹配用户查询\n",
        "\n",
        "### 2. **改善生成质量**\n",
        "- 提供更具体、更相关的上下文\n",
        "- 减少幻觉和不准确信息\n",
        "\n",
        "### 3. **增强可解释性**\n",
        "- 每个命题都是独立的、可验证的事实\n",
        "- 便于追踪信息来源\n",
        "\n",
        "---\n",
        "\n",
        "## 优缺点分析\n",
        "\n",
        "### 优点：\n",
        "1. **语义完整性**：保持事实的完整性\n",
        "2. **检索精度高**：原子化命题更容易匹配\n",
        "3. **可解释性强**：每个命题都是独立的事实\n",
        "\n",
        "### 缺点：\n",
        "1. **计算成本高**：需要调用语言模型\n",
        "2. **依赖模型质量**：输出质量取决于模型能力\n",
        "3. **可能丢失上下文**：过度分解可能丢失重要关联\n",
        "\n",
        "---\n",
        "\n",
        "## 实际应用场景\n",
        "\n",
        "1. **法律文档分析**：将法律条文分解为具体条款\n",
        "2. **学术论文检索**：将论文分解为具体研究发现\n",
        "3. **新闻事实提取**：将新闻文章分解为具体事实\n",
        "4. **技术文档索引**：将技术文档分解为具体功能点\n",
        "\n",
        "这个函数是RAG系统中文本预处理的高级技术，能够显著提升检索和生成的质量！"
      ],
      "metadata": {
        "id": "Mr8boaHR2Khu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aA8UWw5xLWt"
      },
      "source": [
        "## Quality Checking for Propositions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ao8e-AMZxLWt"
      },
      "outputs": [],
      "source": [
        "def evaluate_proposition(proposition, original_text):\n",
        "    \"\"\"\n",
        "    Evaluate a proposition's quality based on accuracy, clarity, completeness, and conciseness.\n",
        "\n",
        "    Args:\n",
        "        proposition (str): The proposition to evaluate\n",
        "        original_text (str): The original text for comparison\n",
        "\n",
        "    Returns:\n",
        "        Dict: Scores for each evaluation dimension\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to evaluate the proposition\n",
        "    system_prompt = \"\"\"You are an expert at evaluating the quality of propositions extracted from text.\n",
        "    Rate the given proposition on the following criteria (scale 1-10):\n",
        "\n",
        "    - Accuracy: How well the proposition reflects information in the original text\n",
        "    - Clarity: How easy it is to understand the proposition without additional context\n",
        "    - Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)\n",
        "    - Conciseness: Whether the proposition is concise without losing important information\n",
        "\n",
        "    The response must be in valid JSON format with numerical scores for each criterion:\n",
        "    {\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n",
        "    \"\"\"\n",
        "\n",
        "    # User prompt containing the proposition and the original text\n",
        "    user_prompt = f\"\"\"Proposition: {proposition}\n",
        "\n",
        "    Original Text: {original_text}\n",
        "\n",
        "    Please provide your evaluation scores in JSON format.\"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "        scores = json.loads(response.choices[0].message.content.strip())\n",
        "        return scores\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if JSON parsing fails\n",
        "        return {\n",
        "            \"accuracy\": 5,\n",
        "            \"clarity\": 5,\n",
        "            \"completeness\": 5,\n",
        "            \"conciseness\": 5\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我来详细讲解这个 `evaluate_proposition` 函数的代码：\n",
        "\n",
        "## 函数整体功能\n",
        "\n",
        "这个函数用于**评估命题的质量**，通过多个维度对从文本中提取的命题进行评分。这是RAG系统中质量控制的重要组件。\n",
        "\n",
        "---\n",
        "\n",
        "## 代码结构分析\n",
        "\n",
        "### 1. **函数定义和文档**\n",
        "```python\n",
        "def evaluate_proposition(proposition, original_text):\n",
        "    \"\"\"\n",
        "    Evaluate a proposition's quality based on accuracy, clarity, completeness, and conciseness.\n",
        "    \n",
        "    Args:\n",
        "        proposition (str): The proposition to evaluate\n",
        "        original_text (str): The original text for comparison\n",
        "        \n",
        "    Returns:\n",
        "        Dict: Scores for each evaluation dimension\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**功能说明：**\n",
        "- 输入：待评估的命题和原始文本\n",
        "- 输出：包含四个维度评分的字典\n",
        "- 目标：从多个角度评估命题质量\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **系统提示词（System Prompt）**\n",
        "```python\n",
        "system_prompt = \"\"\"You are an expert at evaluating the quality of propositions extracted from text.\n",
        "    Rate the given proposition on the following criteria (scale 1-10):\n",
        "\n",
        "    - Accuracy: How well the proposition reflects information in the original text\n",
        "    - Clarity: How easy it is to understand the proposition without additional context\n",
        "    - Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)\n",
        "    - Conciseness: Whether the proposition is concise without losing important information\n",
        "\n",
        "    The response must be in valid JSON format with numerical scores for each criterion:\n",
        "    {\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**详细解析四个评估维度：**\n",
        "\n",
        "#### 1. **准确性（Accuracy）- 1-10分**\n",
        "- **定义**：命题在多大程度上准确反映了原始文本中的信息\n",
        "- **评估标准**：\n",
        "  - 10分：完全准确，无任何错误或偏差\n",
        "  - 5分：基本准确，但有小错误\n",
        "  - 1分：完全不准确，包含错误信息\n",
        "\n",
        "**示例：**\n",
        "- 原文：\"苹果公司于2023年9月发布了iPhone 15\"\n",
        "- ✅ 准确：\"苹果公司于2023年9月发布了iPhone 15\"（10分）\n",
        "- ❌ 不准确：\"苹果公司于2024年发布了iPhone 15\"（1分）\n",
        "\n",
        "#### 2. **清晰度（Clarity）- 1-10分**\n",
        "- **定义**：命题是否易于理解，无需额外上下文\n",
        "- **评估标准**：\n",
        "  - 10分：非常清晰，任何人都能理解\n",
        "  - 5分：基本清晰，但可能需要一些背景知识\n",
        "  - 1分：非常模糊，难以理解\n",
        "\n",
        "**示例：**\n",
        "- ✅ 清晰：\"苹果公司于2023年9月发布了iPhone 15\"（10分）\n",
        "- ❌ 模糊：\"它发布了它\"（1分）\n",
        "\n",
        "#### 3. **完整性（Completeness）- 1-10分**\n",
        "- **定义**：命题是否包含必要的细节（日期、限定词等）\n",
        "- **评估标准**：\n",
        "  - 10分：包含所有必要细节\n",
        "  - 5分：包含基本信息，但缺少一些细节\n",
        "  - 1分：信息严重不完整\n",
        "\n",
        "**示例：**\n",
        "- ✅ 完整：\"苹果公司于2023年9月发布了iPhone 15\"（10分）\n",
        "- ❌ 不完整：\"苹果公司发布了iPhone\"（5分，缺少时间）\n",
        "\n",
        "#### 4. **简洁性（Conciseness）- 1-10分**\n",
        "- **定义**：命题是否简洁而不丢失重要信息\n",
        "- **评估标准**：\n",
        "  - 10分：非常简洁，无冗余信息\n",
        "  - 5分：基本简洁，但有些冗余\n",
        "  - 1分：过于冗长，包含不必要信息\n",
        "\n",
        "**示例：**\n",
        "- ✅ 简洁：\"苹果公司于2023年9月发布了iPhone 15\"（10分）\n",
        "- ❌ 冗长：\"苹果公司是一家总部位于美国加利福尼亚州库比蒂诺的跨国科技公司，该公司于2023年9月发布了iPhone 15智能手机\"（5分，包含不必要信息）\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **用户提示词（User Prompt）**\n",
        "```python\n",
        "user_prompt = f\"\"\"Proposition: {proposition}\n",
        "\n",
        "    Original Text: {original_text}\n",
        "\n",
        "    Please provide your evaluation scores in JSON format.\"\"\"\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 将待评估的命题和原始文本提供给模型\n",
        "- 要求模型以JSON格式返回评分\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **调用语言模型**\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "```\n",
        "\n",
        "**关键参数：**\n",
        "- **response_format={\"type\": \"json_object\"}**：强制模型返回JSON格式\n",
        "- **temperature=0**：确保输出的一致性\n",
        "- **模型**：使用Llama-3.2-3B-Instruct进行评估\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **解析响应和错误处理**\n",
        "```python\n",
        "try:\n",
        "    scores = json.loads(response.choices[0].message.content.strip())\n",
        "    return scores\n",
        "except json.JSONDecodeError:\n",
        "    # Fallback if JSON parsing fails\n",
        "    return {\n",
        "        \"accuracy\": 5,\n",
        "        \"clarity\": 5,\n",
        "        \"completeness\": 5,\n",
        "        \"conciseness\": 5\n",
        "    }\n",
        "```\n",
        "\n",
        "**错误处理机制：**\n",
        "- 尝试解析JSON响应\n",
        "- 如果解析失败，返回默认的中等评分（5分）\n",
        "- 确保函数不会因为解析错误而崩溃\n",
        "\n",
        "---\n",
        "\n",
        "## 使用示例\n",
        "\n",
        "```python\n",
        "# 示例输入\n",
        "proposition = \"苹果公司于2023年9月发布了iPhone 15\"\n",
        "original_text = \"苹果公司于2023年9月12日在加利福尼亚州库比蒂诺发布了iPhone 15。这款新手机采用了A17 Pro芯片，性能比上一代提升了20%。\"\n",
        "\n",
        "# 调用函数\n",
        "scores = evaluate_proposition(proposition, original_text)\n",
        "\n",
        "# 预期输出\n",
        "# {\n",
        "#     \"accuracy\": 8,      # 基本准确，但缺少具体日期\n",
        "#     \"clarity\": 10,      # 非常清晰\n",
        "#     \"completeness\": 7,  # 缺少具体日期（9月12日）\n",
        "#     \"conciseness\": 10   # 非常简洁\n",
        "# }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 在RAG系统中的作用\n",
        "\n",
        "### 1. **质量控制**\n",
        "- 过滤低质量的命题\n",
        "- 确保检索到的信息准确可靠\n",
        "\n",
        "### 2. **优化排序**\n",
        "- 根据质量分数对命题进行排序\n",
        "- 优先展示高质量的命题\n",
        "\n",
        "### 3. **系统改进**\n",
        "- 监控命题生成的质量\n",
        "- 为模型调优提供反馈\n",
        "\n",
        "---\n",
        "\n",
        "## 评分标准详解\n",
        "\n",
        "### 评分指南：\n",
        "\n",
        "| 分数 | 描述 | 示例 |\n",
        "|------|------|------|\n",
        "| 9-10 | 优秀 | 完全符合标准，质量很高 |\n",
        "| 7-8 | 良好 | 基本符合标准，有小问题 |\n",
        "| 5-6 | 中等 | 部分符合标准，有明显问题 |\n",
        "| 3-4 | 较差 | 基本不符合标准 |\n",
        "| 1-2 | 很差 | 完全不符合标准 |\n",
        "\n",
        "---\n",
        "\n",
        "## 优缺点分析\n",
        "\n",
        "### 优点：\n",
        "1. **多维度评估**：从四个不同角度评估质量\n",
        "2. **标准化评分**：使用1-10分制，便于比较\n",
        "3. **错误处理**：有完善的异常处理机制\n",
        "4. **JSON格式**：便于程序处理\n",
        "\n",
        "### 缺点：\n",
        "1. **主观性**：评分仍有一定主观性\n",
        "2. **计算成本**：需要调用语言模型\n",
        "3. **依赖模型**：评分质量取决于模型能力\n",
        "\n",
        "---\n",
        "\n",
        "## 实际应用场景\n",
        "\n",
        "1. **内容审核**：评估生成内容的质量\n",
        "2. **信息过滤**：过滤低质量的信息\n",
        "3. **质量监控**：监控系统输出质量\n",
        "4. **模型评估**：评估不同模型的表现\n",
        "\n",
        "这个函数是RAG系统中质量控制的重要工具，能够确保提供给用户的信息是高质量、准确可靠的！"
      ],
      "metadata": {
        "id": "en5wGRBM22Lu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZKTiBBexLWt"
      },
      "source": [
        "## Complete Proposition Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NvSHGqj-xLWt"
      },
      "outputs": [],
      "source": [
        "def process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100,\n",
        "                                      quality_thresholds=None):\n",
        "    \"\"\"\n",
        "    Process a document into quality-checked propositions.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        chunk_overlap (int): Overlap between chunks in characters\n",
        "        quality_thresholds (Dict): Threshold scores for proposition quality\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Dict], List[Dict]]: Original chunks and proposition chunks\n",
        "    \"\"\"\n",
        "    # Set default quality thresholds if not provided\n",
        "    if quality_thresholds is None:\n",
        "        quality_thresholds = {\n",
        "            \"accuracy\": 7,\n",
        "            \"clarity\": 7,\n",
        "            \"completeness\": 7,\n",
        "            \"conciseness\": 7\n",
        "        }\n",
        "\n",
        "    # Extract text from the PDF file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Create chunks from the extracted text\n",
        "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
        "\n",
        "    # Initialize a list to store all propositions\n",
        "    all_propositions = []\n",
        "\n",
        "    print(\"Generating propositions from chunks...\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "\n",
        "        # Generate propositions for the current chunk\n",
        "        chunk_propositions = generate_propositions(chunk)\n",
        "        print(f\"Generated {len(chunk_propositions)} propositions\")\n",
        "\n",
        "        # Process each generated proposition\n",
        "        for prop in chunk_propositions:\n",
        "            proposition_data = {\n",
        "                \"text\": prop,\n",
        "                \"source_chunk_id\": chunk[\"chunk_id\"],\n",
        "                \"source_text\": chunk[\"text\"]\n",
        "            }\n",
        "            all_propositions.append(proposition_data)\n",
        "\n",
        "    # Evaluate the quality of the generated propositions\n",
        "    print(\"\\nEvaluating proposition quality...\")\n",
        "    quality_propositions = []\n",
        "\n",
        "    for i, prop in enumerate(all_propositions):\n",
        "        if i % 10 == 0:  # Status update every 10 propositions\n",
        "            print(f\"Evaluating proposition {i+1}/{len(all_propositions)}...\")\n",
        "\n",
        "        # Evaluate the quality of the current proposition\n",
        "        scores = evaluate_proposition(prop[\"text\"], prop[\"source_text\"])\n",
        "        prop[\"quality_scores\"] = scores\n",
        "\n",
        "        # Check if the proposition passes the quality thresholds\n",
        "        passes_quality = True\n",
        "        for metric, threshold in quality_thresholds.items():\n",
        "            if scores.get(metric, 0) < threshold:\n",
        "                passes_quality = False\n",
        "                break\n",
        "\n",
        "        if passes_quality:\n",
        "            quality_propositions.append(prop)\n",
        "        else:\n",
        "            print(f\"Proposition failed quality check: {prop['text'][:50]}...\")\n",
        "\n",
        "    print(f\"\\nRetained {len(quality_propositions)}/{len(all_propositions)} propositions after quality filtering\")\n",
        "\n",
        "    return chunks, quality_propositions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我来详细讲解这个 `process_document_into_propositions` 函数的代码：\n",
        "\n",
        "## 函数整体功能\n",
        "\n",
        "这是一个**文档处理流水线**，将PDF文档转换为经过质量检查的命题。这是RAG系统中的核心预处理组件，实现了从原始文档到高质量知识库的完整转换。\n",
        "\n",
        "---\n",
        "\n",
        "## 函数参数分析\n",
        "\n",
        "### 1. **输入参数**\n",
        "```python\n",
        "def process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100,\n",
        "                                      quality_thresholds=None):\n",
        "```\n",
        "\n",
        "**参数详解：**\n",
        "- **pdf_path**: PDF文件路径\n",
        "- **chunk_size=800**: 每个文本块的大小（字符数）\n",
        "- **chunk_overlap=100**: 文本块之间的重叠字符数\n",
        "- **quality_thresholds=None**: 质量阈值字典\n",
        "\n",
        "---\n",
        "\n",
        "## 代码结构分析\n",
        "\n",
        "### 1. **质量阈值设置**\n",
        "```python\n",
        "if quality_thresholds is None:\n",
        "    quality_thresholds = {\n",
        "        \"accuracy\": 7,\n",
        "        \"clarity\": 7,\n",
        "        \"completeness\": 7,\n",
        "        \"conciseness\": 7\n",
        "    }\n",
        "```\n",
        "\n",
        "**功能：**\n",
        "- 设置默认的质量阈值（1-10分制）\n",
        "- 所有维度都设为7分，这是一个相对严格的标准\n",
        "- 只有达到或超过这些阈值的命题才会被保留\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **文本提取和分块**\n",
        "```python\n",
        "# Extract text from the PDF file\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Create chunks from the extracted text\n",
        "chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
        "```\n",
        "\n",
        "**处理流程：**\n",
        "1. **PDF文本提取**：从PDF文件中提取纯文本\n",
        "2. **文本分块**：将长文本分割成小块，便于处理\n",
        "3. **重叠设计**：确保重要信息不会在分块边界丢失\n",
        "\n",
        "**分块策略示例：**\n",
        "```\n",
        "原文：1000字符\n",
        "chunk_size=800, chunk_overlap=100\n",
        "\n",
        "块1：字符1-800\n",
        "块2：字符700-1500  (与块1重叠100字符)\n",
        "块3：字符1400-2200 (与块2重叠100字符)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **命题生成循环**\n",
        "```python\n",
        "all_propositions = []\n",
        "\n",
        "print(\"Generating propositions from chunks...\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "    \n",
        "    # Generate propositions for the current chunk\n",
        "    chunk_propositions = generate_propositions(chunk)\n",
        "    print(f\"Generated {len(chunk_propositions)} propositions\")\n",
        "    \n",
        "    # Process each generated proposition\n",
        "    for prop in chunk_propositions:\n",
        "        proposition_data = {\n",
        "            \"text\": prop,\n",
        "            \"source_chunk_id\": chunk[\"chunk_id\"],\n",
        "            \"source_text\": chunk[\"text\"]\n",
        "        }\n",
        "        all_propositions.append(proposition_data)\n",
        "```\n",
        "\n",
        "**详细流程：**\n",
        "\n",
        "#### 步骤1：遍历每个文本块\n",
        "- 显示处理进度\n",
        "- 调用 `generate_propositions` 函数生成命题\n",
        "\n",
        "#### 步骤2：构建命题数据结构\n",
        "```python\n",
        "proposition_data = {\n",
        "    \"text\": prop,                    # 命题文本\n",
        "    \"source_chunk_id\": chunk[\"chunk_id\"],  # 来源块ID\n",
        "    \"source_text\": chunk[\"text\"]     # 原始文本（用于质量评估）\n",
        "}\n",
        "```\n",
        "\n",
        "**数据结构设计的好处：**\n",
        "- **可追溯性**：知道每个命题来自哪个文本块\n",
        "- **质量评估**：保留原始文本用于准确性检查\n",
        "- **调试支持**：便于问题定位和系统优化\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **质量评估和过滤**\n",
        "```python\n",
        "print(\"\\nEvaluating proposition quality...\")\n",
        "quality_propositions = []\n",
        "\n",
        "for i, prop in enumerate(all_propositions):\n",
        "    if i % 10 == 0:  # Status update every 10 propositions\n",
        "        print(f\"Evaluating proposition {i+1}/{len(all_propositions)}...\")\n",
        "        \n",
        "    # Evaluate the quality of the current proposition\n",
        "    scores = evaluate_proposition(prop[\"text\"], prop[\"source_text\"])\n",
        "    prop[\"quality_scores\"] = scores\n",
        "    \n",
        "    # Check if the proposition passes the quality thresholds\n",
        "    passes_quality = True\n",
        "    for metric, threshold in quality_thresholds.items():\n",
        "        if scores.get(metric, 0) < threshold:\n",
        "            passes_quality = False\n",
        "            break\n",
        "    \n",
        "    if passes_quality:\n",
        "        quality_propositions.append(prop)\n",
        "    else:\n",
        "        print(f\"Proposition failed quality check: {prop['text'][:50]}...\")\n",
        "```\n",
        "\n",
        "**质量评估流程：**\n",
        "\n",
        "#### 步骤1：批量评估\n",
        "- 每10个命题显示一次进度\n",
        "- 调用 `evaluate_proposition` 进行质量评分\n",
        "\n",
        "#### 步骤2：阈值检查\n",
        "```python\n",
        "passes_quality = True\n",
        "for metric, threshold in quality_thresholds.items():\n",
        "    if scores.get(metric, 0) < threshold:\n",
        "        passes_quality = False\n",
        "        break\n",
        "```\n",
        "\n",
        "**逻辑说明：**\n",
        "- 所有维度都必须达到阈值\n",
        "- 任何一个维度不达标，整个命题就被过滤掉\n",
        "- 使用 `scores.get(metric, 0)` 处理缺失评分的情况\n",
        "\n",
        "#### 步骤3：结果分类\n",
        "- **通过**：添加到 `quality_propositions`\n",
        "- **未通过**：打印失败信息，便于调试\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **结果统计和返回**\n",
        "```python\n",
        "print(f\"\\nRetained {len(quality_propositions)}/{len(all_propositions)} propositions after quality filtering\")\n",
        "\n",
        "return chunks, quality_propositions\n",
        "```\n",
        "\n",
        "**返回数据：**\n",
        "- **chunks**: 原始文本块（用于调试和参考）\n",
        "- **quality_propositions**: 经过质量过滤的命题列表\n",
        "\n",
        "---\n",
        "\n",
        "## 完整的数据流示例\n",
        "\n",
        "```python\n",
        "# 输入：PDF文档\n",
        "pdf_path = \"document.pdf\"\n",
        "\n",
        "# 处理过程\n",
        "chunks, propositions = process_document_into_propositions(\n",
        "    pdf_path,\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100,\n",
        "    quality_thresholds={\n",
        "        \"accuracy\": 8,      # 更严格的准确性要求\n",
        "        \"clarity\": 7,\n",
        "        \"completeness\": 6,  # 稍微宽松的完整性要求\n",
        "        \"conciseness\": 7\n",
        "    }\n",
        ")\n",
        "\n",
        "# 输出示例\n",
        "print(f\"原始文本块数量: {len(chunks)}\")\n",
        "print(f\"生成命题总数: {len(propositions)}\")\n",
        "\n",
        "# 查看第一个命题\n",
        "if propositions:\n",
        "    first_prop = propositions[0]\n",
        "    print(f\"命题文本: {first_prop['text']}\")\n",
        "    print(f\"质量评分: {first_prop['quality_scores']}\")\n",
        "    print(f\"来源块ID: {first_prop['source_chunk_id']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 在RAG系统中的作用\n",
        "\n",
        "### 1. **知识库构建**\n",
        "- 将非结构化文档转换为结构化知识\n",
        "- 为向量数据库提供高质量数据\n",
        "\n",
        "### 2. **质量控制**\n",
        "- 确保知识库中的信息准确可靠\n",
        "- 过滤低质量或错误的信息\n",
        "\n",
        "### 3. **可追溯性**\n",
        "- 每个命题都可以追溯到原始文档\n",
        "- 便于验证和调试\n",
        "\n",
        "---\n",
        "\n",
        "## 性能优化建议\n",
        "\n",
        "### 1. **并行处理**\n",
        "```python\n",
        "# 可以改进为并行处理\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def process_chunk_parallel(chunk):\n",
        "    # 并行处理每个块\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 2. **批量评估**\n",
        "```python\n",
        "# 批量评估多个命题\n",
        "def batch_evaluate_propositions(propositions):\n",
        "    # 减少API调用次数\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 3. **缓存机制**\n",
        "```python\n",
        "# 缓存已处理的文档\n",
        "def get_cached_propositions(pdf_path):\n",
        "    # 避免重复处理\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 优缺点分析\n",
        "\n",
        "### 优点：\n",
        "1. **完整的流水线**：从PDF到高质量命题的完整处理\n",
        "2. **质量控制**：多维度质量评估和过滤\n",
        "3. **可追溯性**：保留完整的来源信息\n",
        "4. **可配置性**：支持自定义参数和阈值\n",
        "\n",
        "### 缺点：\n",
        "1. **计算成本高**：需要多次调用语言模型\n",
        "2. **处理时间长**：特别是大文档\n",
        "3. **依赖外部服务**：需要稳定的API服务\n",
        "\n",
        "这个函数是RAG系统中文档预处理的核心组件，为后续的检索和生成提供了高质量的知识基础！"
      ],
      "metadata": {
        "id": "6tA6qnN93gyO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8VYEJcAxLWt"
      },
      "source": [
        "## Building Vector Stores for Both Approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TV4rp1MGxLWt"
      },
      "outputs": [],
      "source": [
        "def build_vector_stores(chunks, propositions):\n",
        "    \"\"\"\n",
        "    Build vector stores for both chunk-based and proposition-based approaches.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[Dict]): Original document chunks\n",
        "        propositions (List[Dict]): Quality-filtered propositions\n",
        "\n",
        "    Returns:\n",
        "        Tuple[SimpleVectorStore, SimpleVectorStore]: Chunk and proposition vector stores\n",
        "    \"\"\"\n",
        "    # Create vector store for chunks\n",
        "    chunk_store = SimpleVectorStore()\n",
        "\n",
        "    # Extract chunk texts and create embeddings\n",
        "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "    print(f\"Creating embeddings for {len(chunk_texts)} chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunk_texts)\n",
        "\n",
        "    # Add chunks to vector store with metadata\n",
        "    chunk_metadata = [{\"chunk_id\": chunk[\"chunk_id\"], \"type\": \"chunk\"} for chunk in chunks]\n",
        "    chunk_store.add_items(chunk_texts, chunk_embeddings, chunk_metadata)\n",
        "\n",
        "    # Create vector store for propositions\n",
        "    prop_store = SimpleVectorStore()\n",
        "\n",
        "    # Extract proposition texts and create embeddings\n",
        "    prop_texts = [prop[\"text\"] for prop in propositions]\n",
        "    print(f\"Creating embeddings for {len(prop_texts)} propositions...\")\n",
        "    prop_embeddings = create_embeddings(prop_texts)\n",
        "\n",
        "    # Add propositions to vector store with metadata\n",
        "    prop_metadata = [\n",
        "        {\n",
        "            \"type\": \"proposition\",\n",
        "            \"source_chunk_id\": prop[\"source_chunk_id\"],\n",
        "            \"quality_scores\": prop[\"quality_scores\"]\n",
        "        }\n",
        "        for prop in propositions\n",
        "    ]\n",
        "    prop_store.add_items(prop_texts, prop_embeddings, prop_metadata)\n",
        "\n",
        "    return chunk_store, prop_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uHtWZZAxLWu"
      },
      "source": [
        "## Query and Retrieval Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cw1fCt8SxLWu"
      },
      "outputs": [],
      "source": [
        "def retrieve_from_store(query, vector_store, k=5):\n",
        "    \"\"\"\n",
        "    Retrieve relevant items from a vector store based on query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store to search\n",
        "        k (int): Number of results to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved items with scores and metadata\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Search vector store for the top k most similar items\n",
        "    results = vector_store.similarity_search(query_embedding, k=k)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VBOfFyrExLWu"
      },
      "outputs": [],
      "source": [
        "def compare_retrieval_approaches(query, chunk_store, prop_store, k=5):\n",
        "    \"\"\"\n",
        "    Compare chunk-based and proposition-based retrieval for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunk_store (SimpleVectorStore): Chunk-based vector store\n",
        "        prop_store (SimpleVectorStore): Proposition-based vector store\n",
        "        k (int): Number of results to retrieve from each store\n",
        "\n",
        "    Returns:\n",
        "        Dict: Comparison results\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Query: {query} ===\")\n",
        "\n",
        "    # Retrieve results from the proposition-based vector store\n",
        "    print(\"\\nRetrieving with proposition-based approach...\")\n",
        "    prop_results = retrieve_from_store(query, prop_store, k)\n",
        "\n",
        "    # Retrieve results from the chunk-based vector store\n",
        "    print(\"Retrieving with chunk-based approach...\")\n",
        "    chunk_results = retrieve_from_store(query, chunk_store, k)\n",
        "\n",
        "    # Display proposition-based results\n",
        "    print(\"\\n=== Proposition-Based Results ===\")\n",
        "    for i, result in enumerate(prop_results):\n",
        "        print(f\"{i+1}) {result['text']} (Score: {result['similarity']:.4f})\")\n",
        "\n",
        "    # Display chunk-based results\n",
        "    print(\"\\n=== Chunk-Based Results ===\")\n",
        "    for i, result in enumerate(chunk_results):\n",
        "        # Truncate text to keep the output manageable\n",
        "        truncated_text = result['text'][:150] + \"...\" if len(result['text']) > 150 else result['text']\n",
        "        print(f\"{i+1}) {truncated_text} (Score: {result['similarity']:.4f})\")\n",
        "\n",
        "    # Return the comparison results\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"proposition_results\": prop_results,\n",
        "        \"chunk_results\": chunk_results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2UfsqmdxLWu"
      },
      "source": [
        "## Response Generation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Nd92668cxLWu"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, results, result_type=\"proposition\"):\n",
        "    \"\"\"\n",
        "    Generate a response based on retrieved results.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Retrieved items\n",
        "        result_type (str): Type of results ('proposition' or 'chunk')\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Combine retrieved texts into a single context string\n",
        "    context = \"\\n\\n\".join([result[\"text\"] for result in results])\n",
        "\n",
        "    # System prompt to instruct the AI on how to generate the response\n",
        "    system_prompt = f\"\"\"You are an AI assistant answering questions based on retrieved information.\n",
        "Your answer should be based on the following {result_type}s that were retrieved from a knowledge base.\n",
        "If the retrieved information doesn't answer the question, acknowledge this limitation.\"\"\"\n",
        "\n",
        "    # User prompt containing the query and the retrieved context\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "Retrieved {result_type}s:\n",
        "{context}\n",
        "\n",
        "Please answer the query based on the retrieved information.\"\"\"\n",
        "\n",
        "    # Generate the response using the OpenAI client\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    # Return the generated response text\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PujibFqYxLWu"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(query, prop_response, chunk_response, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluate and compare responses from both approaches.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        prop_response (str): Response from proposition-based approach\n",
        "        chunk_response (str): Response from chunk-based approach\n",
        "        reference_answer (str, optional): Reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation analysis\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to evaluate the responses\n",
        "    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
        "    Compare the two responses to the same query, one generated from proposition-based retrieval\n",
        "    and the other from chunk-based retrieval.\n",
        "\n",
        "    Evaluate them based on:\n",
        "    1. Accuracy: Which response provides more factually correct information?\n",
        "    2. Relevance: Which response better addresses the specific query?\n",
        "    3. Conciseness: Which response is more concise while maintaining completeness?\n",
        "    4. Clarity: Which response is easier to understand?\n",
        "\n",
        "    Be specific about the strengths and weaknesses of each approach.\"\"\"\n",
        "\n",
        "    # User prompt containing the query and the responses to be compared\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "    Response from Proposition-Based Retrieval:\n",
        "    {prop_response}\n",
        "\n",
        "    Response from Chunk-Based Retrieval:\n",
        "    {chunk_response}\"\"\"\n",
        "\n",
        "    # If a reference answer is provided, include it in the user prompt for factual checking\n",
        "    if reference_answer:\n",
        "        user_prompt += f\"\"\"\n",
        "\n",
        "    Reference Answer (for factual checking):\n",
        "    {reference_answer}\"\"\"\n",
        "\n",
        "    # Add the final instruction to the user prompt\n",
        "    user_prompt += \"\"\"\n",
        "    Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n",
        "\n",
        "    # Generate the evaluation analysis using the OpenAI client\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated evaluation analysis\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKri3ANxLWu"
      },
      "source": [
        "## Complete End-to-End Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EaFQWTo5xLWu"
      },
      "outputs": [],
      "source": [
        "def run_proposition_chunking_evaluation(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Run a complete evaluation of proposition chunking vs standard chunking.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        test_queries (List[str]): List of test queries\n",
        "        reference_answers (List[str], optional): Reference answers for queries\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    print(\"=== Starting Proposition Chunking Evaluation ===\\n\")\n",
        "\n",
        "    # Process document into propositions and chunks\n",
        "    chunks, propositions = process_document_into_propositions(pdf_path)\n",
        "\n",
        "    # Build vector stores for chunks and propositions\n",
        "    chunk_store, prop_store = build_vector_stores(chunks, propositions)\n",
        "\n",
        "    # Initialize a list to store results for each query\n",
        "    results = []\n",
        "\n",
        "    # Run tests for each query\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\n=== Testing Query {i+1}/{len(test_queries)} ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        # Get retrieval results from both chunk-based and proposition-based approaches\n",
        "        retrieval_results = compare_retrieval_approaches(query, chunk_store, prop_store)\n",
        "\n",
        "        # Generate responses based on the retrieved proposition-based results\n",
        "        print(\"\\nGenerating response from proposition-based results...\")\n",
        "        prop_response = generate_response(\n",
        "            query,\n",
        "            retrieval_results[\"proposition_results\"],\n",
        "            \"proposition\"\n",
        "        )\n",
        "\n",
        "        # Generate responses based on the retrieved chunk-based results\n",
        "        print(\"Generating response from chunk-based results...\")\n",
        "        chunk_response = generate_response(\n",
        "            query,\n",
        "            retrieval_results[\"chunk_results\"],\n",
        "            \"chunk\"\n",
        "        )\n",
        "\n",
        "        # Get reference answer if available\n",
        "        reference = None\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            reference = reference_answers[i]\n",
        "\n",
        "        # Evaluate the generated responses\n",
        "        print(\"\\nEvaluating responses...\")\n",
        "        evaluation = evaluate_responses(query, prop_response, chunk_response, reference)\n",
        "\n",
        "        # Compile results for the current query\n",
        "        query_result = {\n",
        "            \"query\": query,\n",
        "            \"proposition_results\": retrieval_results[\"proposition_results\"],\n",
        "            \"chunk_results\": retrieval_results[\"chunk_results\"],\n",
        "            \"proposition_response\": prop_response,\n",
        "            \"chunk_response\": chunk_response,\n",
        "            \"reference_answer\": reference,\n",
        "            \"evaluation\": evaluation\n",
        "        }\n",
        "\n",
        "        # Append the results to the overall results list\n",
        "        results.append(query_result)\n",
        "\n",
        "        # Print the responses and evaluation for the current query\n",
        "        print(\"\\n=== Proposition-Based Response ===\")\n",
        "        print(prop_response)\n",
        "\n",
        "        print(\"\\n=== Chunk-Based Response ===\")\n",
        "        print(chunk_response)\n",
        "\n",
        "        print(\"\\n=== Evaluation ===\")\n",
        "        print(evaluation)\n",
        "\n",
        "    # Generate overall analysis of the evaluation\n",
        "    print(\"\\n\\n=== Generating Overall Analysis ===\")\n",
        "    overall_analysis = generate_overall_analysis(results)\n",
        "    print(\"\\n\" + overall_analysis)\n",
        "\n",
        "    # Return the evaluation results, overall analysis, and counts of propositions and chunks\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"overall_analysis\": overall_analysis,\n",
        "        \"proposition_count\": len(propositions),\n",
        "        \"chunk_count\": len(chunks)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DlxaAQY6xLWu"
      },
      "outputs": [],
      "source": [
        "def generate_overall_analysis(results):\n",
        "    \"\"\"\n",
        "    Generate an overall analysis of proposition vs chunk approaches.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results from each test query\n",
        "\n",
        "    Returns:\n",
        "        str: Overall analysis\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to generate the overall analysis\n",
        "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
        "    Based on multiple test queries, provide an overall analysis comparing proposition-based retrieval\n",
        "    to chunk-based retrieval for RAG (Retrieval-Augmented Generation) systems.\n",
        "\n",
        "    Focus on:\n",
        "    1. When proposition-based retrieval performs better\n",
        "    2. When chunk-based retrieval performs better\n",
        "    3. The overall strengths and weaknesses of each approach\n",
        "    4. Recommendations for when to use each approach\"\"\"\n",
        "\n",
        "    # Create a summary of evaluations for each query\n",
        "    evaluations_summary = \"\"\n",
        "    for i, result in enumerate(results):\n",
        "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
        "        evaluations_summary += f\"Evaluation Summary: {result['evaluation'][:200]}...\\n\\n\"\n",
        "\n",
        "    # User prompt containing the summary of evaluations\n",
        "    user_prompt = f\"\"\"Based on the following evaluations of proposition-based vs chunk-based retrieval across {len(results)} queries,\n",
        "    provide an overall analysis comparing these two approaches:\n",
        "\n",
        "    {evaluations_summary}\n",
        "\n",
        "    Please provide a comprehensive analysis on the relative strengths and weaknesses of proposition-based\n",
        "    and chunk-based retrieval for RAG systems.\"\"\"\n",
        "\n",
        "    # Generate the overall analysis using the OpenAI client\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated analysis text\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T93AdVWWxLWv"
      },
      "source": [
        "## Evaluation of Proposition Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Mu4xu98xLWv",
        "outputId": "f7ca0576-2238-4bc0-bb19-4706fb83c654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting Proposition Chunking Evaluation ===\n",
            "\n",
            "Created 48 text chunks\n",
            "Generating propositions from chunks...\n",
            "Processing chunk 1/48...\n",
            "Generated 10 propositions\n",
            "Processing chunk 2/48...\n",
            "Generated 18 propositions\n",
            "Processing chunk 3/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 4/48...\n",
            "Generated 13 propositions\n",
            "Processing chunk 5/48...\n",
            "Generated 13 propositions\n",
            "Processing chunk 6/48...\n",
            "Generated 19 propositions\n",
            "Processing chunk 7/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 8/48...\n",
            "Generated 22 propositions\n",
            "Processing chunk 9/48...\n",
            "Generated 23 propositions\n",
            "Processing chunk 10/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 11/48...\n",
            "Generated 15 propositions\n",
            "Processing chunk 12/48...\n",
            "Generated 16 propositions\n",
            "Processing chunk 13/48...\n",
            "Generated 13 propositions\n",
            "Processing chunk 14/48...\n",
            "Generated 15 propositions\n",
            "Processing chunk 15/48...\n",
            "Generated 14 propositions\n",
            "Processing chunk 16/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 17/48...\n",
            "Generated 25 propositions\n",
            "Processing chunk 18/48...\n",
            "Generated 12 propositions\n",
            "Processing chunk 19/48...\n",
            "Generated 18 propositions\n",
            "Processing chunk 20/48...\n",
            "Generated 27 propositions\n",
            "Processing chunk 21/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 22/48...\n",
            "Generated 14 propositions\n",
            "Processing chunk 23/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 24/48...\n",
            "Generated 24 propositions\n",
            "Processing chunk 25/48...\n",
            "Generated 22 propositions\n",
            "Processing chunk 26/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 27/48...\n",
            "Generated 22 propositions\n",
            "Processing chunk 28/48...\n",
            "Generated 23 propositions\n",
            "Processing chunk 29/48...\n",
            "Generated 18 propositions\n",
            "Processing chunk 30/48...\n",
            "Generated 20 propositions\n",
            "Processing chunk 31/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 32/48...\n",
            "Generated 19 propositions\n",
            "Processing chunk 33/48...\n",
            "Generated 20 propositions\n",
            "Processing chunk 34/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 35/48...\n",
            "Generated 15 propositions\n",
            "Processing chunk 36/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 37/48...\n",
            "Generated 24 propositions\n",
            "Processing chunk 38/48...\n",
            "Generated 21 propositions\n",
            "Processing chunk 39/48...\n",
            "Generated 20 propositions\n",
            "Processing chunk 40/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 41/48...\n",
            "Generated 18 propositions\n",
            "Processing chunk 42/48...\n",
            "Generated 23 propositions\n",
            "Processing chunk 43/48...\n",
            "Generated 18 propositions\n",
            "Processing chunk 44/48...\n",
            "Generated 22 propositions\n",
            "Processing chunk 45/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 46/48...\n",
            "Generated 17 propositions\n",
            "Processing chunk 47/48...\n",
            "Generated 16 propositions\n",
            "Processing chunk 48/48...\n",
            "Generated 13 propositions\n",
            "\n",
            "Evaluating proposition quality...\n",
            "Evaluating proposition 1/894...\n",
            "Evaluating proposition 11/894...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-3357160613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Run the evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m evaluation_results = run_proposition_chunking_evaluation(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpdf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtest_queries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_queries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-366330442.py\u001b[0m in \u001b[0;36mrun_proposition_chunking_evaluation\u001b[0;34m(pdf_path, test_queries, reference_answers)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Process document into propositions and chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpropositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_document_into_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Build vector stores for chunks and propositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-3145666415.py\u001b[0m in \u001b[0;36mprocess_document_into_propositions\u001b[0;34m(pdf_path, chunk_size, chunk_overlap, quality_thresholds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Evaluate the quality of the current proposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_proposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quality_scores\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-3680587282.py\u001b[0m in \u001b[0;36mevaluate_proposition\u001b[0;34m(proposition, original_text)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Generate response from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"o1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    973\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Path to the AI information document that will be processed\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Define test queries covering different aspects of AI to evaluate proposition chunking\n",
        "test_queries = [\n",
        "    \"What are the main ethical concerns in AI development?\",\n",
        "    # \"How does explainable AI improve trust in AI systems?\",\n",
        "    # \"What are the key challenges in developing fair AI systems?\",\n",
        "    # \"What role does human oversight play in AI safety?\"\n",
        "]\n",
        "\n",
        "# Reference answers for more thorough evaluation and comparison of results\n",
        "# These provide a ground truth to measure the quality of generated responses\n",
        "reference_answers = [\n",
        "    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n",
        "    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n",
        "    # \"Key challenges in developing fair AI systems include addressing data bias, ensuring diverse representation in training data, creating transparent algorithms, defining fairness across different contexts, and balancing competing fairness criteria.\",\n",
        "    # \"Human oversight plays a critical role in AI safety by monitoring system behavior, verifying outputs, intervening when necessary, setting ethical boundaries, and ensuring AI systems remain aligned with human values and intentions throughout their operation.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = run_proposition_chunking_evaluation(\n",
        "    pdf_path=pdf_path,\n",
        "    test_queries=test_queries,\n",
        "    reference_answers=reference_answers\n",
        ")\n",
        "\n",
        "# Print the overall analysis\n",
        "print(\"\\n\\n=== Overall Analysis ===\")\n",
        "print(evaluation_results[\"overall_analysis\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}