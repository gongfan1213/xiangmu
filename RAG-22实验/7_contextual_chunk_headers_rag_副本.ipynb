{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "tRbB4Vd-N0_F"
      },
      "source": [
        "# Contextual Chunk Headers (CCH) in Simple RAG\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) improves the factual accuracy of language models by retrieving relevant external knowledge before generating a response. However, standard chunking often loses important context, making retrieval less effective.\n",
        "\n",
        "Contextual Chunk Headers (CCH) enhance RAG by prepending high-level context (like document titles or section headers) to each chunk before embedding them. This improves retrieval quality and prevents out-of-context responses.\n",
        "\n",
        "## Steps in this Notebook:\n",
        "\n",
        "1. **Data Ingestion**: Load and preprocess the text data.\n",
        "2. **Chunking with Contextual Headers**: Extract section titles and prepend them to chunks.\n",
        "3. **Embedding Creation**: Convert context-enhanced chunks into numerical representations.\n",
        "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
        "5. **Response Generation**: Use a language model to generate a response from retrieved text.\n",
        "6. **Evaluation**: Assess response accuracy using a scoring system."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 简单RAG中的上下文块头（CCH）技术  \n",
        "\n",
        "检索增强生成（RAG）通过在生成响应前检索相关外部知识，提升语言模型的事实准确性。然而，标准分块方法常丢失重要上下文，导致检索效率低下。  \n",
        "\n",
        "**上下文块头（Contextual Chunk Headers, CCH）** 技术通过在嵌入前为每个文本块添加高层级上下文（如文档标题或章节标题）来增强RAG系统。这一方法可提升检索质量，并避免生成脱离上下文的响应。  \n",
        "\n",
        "\n",
        "### 本笔记本的实现步骤：  \n",
        "1. **数据摄入**：加载并预处理文本数据。  \n",
        "2. **带上下文头的分块**：提取章节标题并将其添加到文本块前。  \n",
        "3. **嵌入创建**：将上下文增强的文本块转换为数值表示。  \n",
        "4. **语义搜索**：基于用户查询检索相关文本块。  \n",
        "5. **响应生成**：使用语言模型从检索到的文本中生成回答。  \n",
        "6. **评估**：使用评分系统评估响应的准确性。"
      ],
      "metadata": {
        "id": "xkECJ7r3Pu6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 简单RAG中的上下文块头（CCH）技术解析  \n",
        "\n",
        "#### 一、CCH技术的核心目标与创新点  \n",
        "检索增强生成（RAG）通过检索外部知识提升语言模型的事实准确性，但传统分块常丢失文档结构信息（如章节标题、段落关系），导致检索结果偏离上下文。**上下文块头（Contextual Chunk Headers, CCH）** 的核心创新是：**在每个文本块前添加高层级上下文（如文档标题、章节头）**，形成“标题+内容”的块结构，从而在嵌入和检索阶段保留文档语义层级。  \n",
        "\n",
        "\n",
        "#### 二、CCH技术的工作原理  \n",
        "1. **结构保留**：  \n",
        "   将文档的层级信息（如“第三章 人工智能核心技术”）作为块头，与具体内容（如“机器学习是AI的基础...”）拼接成新块：  \n",
        "   ```plaintext\n",
        "   [块头] 第三章 人工智能核心技术  \n",
        "   [块内容] 机器学习通过算法从数据中学习模式，分为监督学习、无监督学习...\n",
        "   ```  \n",
        "\n",
        "2. **语义增强**：  \n",
        "   块头提供了内容的主题归属和上下文关系，使嵌入向量同时包含：  \n",
        "   - 块内容的具体语义（如“机器学习”）  \n",
        "   - 块的层级上下文（如“第三章”的主题相关性）  \n",
        "\n",
        "3. **检索优化**：  \n",
        "   当用户查询与块头主题匹配时（如“人工智能核心技术有哪些”），带块头的文本块会因更高的语义相关性被优先检索，减少跨主题误匹配。  \n",
        "\n",
        "\n",
        "#### 三、Notebook实现步骤详解  \n",
        "\n",
        "##### 1. 数据摄入与预处理  \n",
        "```python\n",
        "# 示例：从结构化文档中提取带层级的文本\n",
        "def extract_text_with_headers(doc_path):\n",
        "    \"\"\"从包含标题和内容的文档中提取带层级的文本\"\"\"\n",
        "    # 假设文档结构为：标题1 -> 子标题1.1 -> 内容 -> 子标题1.2 -> 内容...\n",
        "    headers = []  # 存储各级标题\n",
        "    content = []  # 存储内容段落\n",
        "    # 实际实现需根据文档格式（如PDF、Markdown）解析结构\n",
        "    return headers, content\n",
        "```  \n",
        "- **关键逻辑**：解析文档时保留标题层级，区分“章标题”“节标题”“子节标题”等不同粒度的上下文。  \n",
        "\n",
        "\n",
        "##### 2. 带上下文头的分块  \n",
        "```python\n",
        "def chunk_with_context_headers(headers, content, chunk_size=1000, overlap=200):\n",
        "    \"\"\"将文本分块并添加对应的上下文头\"\"\"\n",
        "    chunks = []\n",
        "    current_header = \"\"\n",
        "    \n",
        "    # 遍历标题和内容，建立标题与内容的归属关系\n",
        "    for i, (header, text) in enumerate(zip(headers, content)):\n",
        "        if header:  # 遇到新标题时更新当前上下文头\n",
        "            current_header = header\n",
        "        \n",
        "        # 分块处理内容，并添加当前标题作为块头\n",
        "        for j in range(0, len(text), chunk_size - overlap):\n",
        "            chunk = text[j:j+chunk_size]\n",
        "            chunks.append(f\"[CCH] {current_header}\\n{chunk}\")  # 格式：块头+内容\n",
        "    \n",
        "    return chunks\n",
        "```  \n",
        "- **设计要点**：  \n",
        "  - 每个块以`[CCH] 标题`作为前缀，明确标识上下文来源；  \n",
        "  - 当内容跨章节时，优先使用最近的标题作为块头（如段落同时属于章和节时，使用节标题）。  \n",
        "\n",
        "\n",
        "##### 3. 嵌入生成与语义检索  \n",
        "```python\n",
        "# 嵌入生成（与传统RAG类似，但输入为带CCH的块）\n",
        "def create_cch_embeddings(chunks, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"生成带上下文头的块嵌入\"\"\"\n",
        "    return client.embeddings.create(model=model, input=chunks)\n",
        "\n",
        "# 检索时，CCH块的嵌入包含标题语义，提升相关性\n",
        "def cch_semantic_search(query, chunks, embeddings, k=5):\n",
        "    \"\"\"基于CCH块的语义检索\"\"\"\n",
        "    # 计算查询与块嵌入的相似度（与传统RAG相同）\n",
        "    # 但因块头存在，相似度过滤更精准\n",
        "    return top_k_relevant_chunks\n",
        "```  \n",
        "- **技术优势**：  \n",
        "  块头中的关键词（如“人工智能”“核心技术”）会提升相关查询的相似度得分，使检索结果更聚焦于主题相关的块。  \n",
        "\n",
        "\n",
        "##### 4. 响应生成与评估  \n",
        "```python\n",
        "# 生成响应时，块头信息帮助LLM理解内容的上下文归属\n",
        "system_prompt = \"你是AI助手，基于提供的带标题上下文回答问题。标题用于说明内容主题，回答需结合标题与内容。\"\n",
        "\n",
        "# 评估时，CCH技术的得分通常更高，因回答更符合文档结构\n",
        "def evaluate_cch_response(ai_answer, true_answer, context_chunks):\n",
        "    \"\"\"评估带CCH的响应准确性\"\"\"\n",
        "    # 检查回答是否利用了块头中的主题信息\n",
        "    # 例如：回答“AI核心技术”时，是否引用了标题为“第三章 人工智能核心技术”的块\n",
        "    return accuracy_score\n",
        "```  \n",
        "\n",
        "\n",
        "#### 四、CCH技术与传统分块的对比  \n",
        "| 维度               | 传统分块                          | CCH分块                          |\n",
        "|--------------------|-----------------------------------|-----------------------------------|\n",
        "| **块结构**         | 纯内容（如“机器学习是AI的基础...”） | 标题+内容（如“第三章 AI核心技术\\n机器学习是AI的基础...”） |\n",
        "| **语义完整性**     | 缺乏上下文归属，可能跨主题混淆    | 明确主题归属，减少跨章节误匹配    |\n",
        "| **检索准确率**     | 仅基于内容关键词匹配              | 结合标题主题和内容关键词匹配      |\n",
        "| **回答相关性**     | 可能生成脱离文档结构的回答        | 回答更符合文档章节逻辑            |\n",
        "| **实现成本**       | 低（无需解析文档结构）            | 中（需解析标题层级）              |  \n",
        "\n",
        "\n",
        "#### 五、CCH技术的适用场景  \n",
        "1. **学术文献问答**：  \n",
        "   - 块头为“第四章 实验方法”的内容块，在用户查询“该研究使用了哪些实验方法”时被优先检索。  \n",
        "\n",
        "2. **企业文档检索**：  \n",
        "   - 块头为“政策-报销流程-差旅”的文本块，在用户查询“差旅报销需要哪些凭证”时精准匹配。  \n",
        "\n",
        "3. **多文档RAG系统**：  \n",
        "   - 不同文档的块头可包含文档名称（如“2024年财报.pdf-第二季度营收”），避免跨文档信息混淆。  \n",
        "\n",
        "\n",
        "#### 六、CCH技术的扩展与优化  \n",
        "1. **动态块头生成**：  \n",
        "   - 对无明确标题的文档，使用LLM生成摘要式块头（如“关于机器学习分类方法的介绍”）。  \n",
        "\n",
        "2. **分层块头策略**：  \n",
        "   - 根据标题层级（章→节→子节）赋予不同权重，例如章标题影响全局主题，子节标题影响局部语义。  \n",
        "\n",
        "3. **块头压缩技术**：  \n",
        "   - 对过长标题（如“第三部分 人工智能在金融领域的应用案例分析-银行智能风控系统”），使用LLM生成精简版块头（如“金融领域AI应用-银行风控”），减少嵌入向量中的冗余信息。  \n",
        "\n",
        "\n",
        "#### 七、潜在挑战与解决方案  \n",
        "1. **标题解析误差**：  \n",
        "   - 非结构化文档（如纯文本）的标题识别可能出错（如误将普通段落识别为标题）。  \n",
        "   - **解决方案**：结合正则表达式和LLM分类，提升标题识别准确率。  \n",
        "\n",
        "2. **块头长度控制**：  \n",
        "   - 过长块头（如500字标题）会占用嵌入向量的语义空间，降低内容表征精度。  \n",
        "   - **解决方案**：限制块头长度（如不超过100字），或使用标题嵌入与内容嵌入的融合策略。  \n",
        "\n",
        "3. **多语言支持**：  \n",
        "   - 跨语言文档的标题解析（如中文标题→英文内容）可能导致语义偏差。  \n",
        "   - **解决方案**：使用多语言嵌入模型（如XLM-R），或对标题和内容分别进行语言适配处理。  \n",
        "\n",
        "CCH技术通过显式保留文档结构信息，在不显著增加计算成本的前提下，有效提升了RAG系统的检索质量和回答相关性，尤其适合处理具有明确层级结构的专业文档。"
      ],
      "metadata": {
        "id": "epYIYqK3OC3f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69vVceo2N0_H"
      },
      "source": [
        "## Setting Up the Environment\n",
        "We begin by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "84Cw0SXtN0_H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import fitz\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PymuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeAIWn5NQRn6",
        "outputId": "f0d5e5ee-4794-4844-dd3a-2fa141e657d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PymuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PymuPDF\n",
            "Successfully installed PymuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCBBltM0N0_I"
      },
      "source": [
        "## Extracting Text and Identifying Section Headers\n",
        "We extract text from a PDF while also identifying section titles (potential headers for chunks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6gaXt7KEN0_I"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC0q2oWCN0_J"
      },
      "source": [
        "## Setting Up the OpenAI API Client\n",
        "We initialize the OpenAI client to generate embeddings and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IBnBXE1PN0_J"
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI client with the base URL and API key\n",
        "client = OpenAI(\n",
        "    base_url=\"http://xxx/v1/\",\n",
        "    api_key=\"xxxxxxt9\"  # Retrieve the API key from environment variables\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DwNy7hKN0_K"
      },
      "source": [
        "## Chunking Text with Contextual Headers\n",
        "To improve retrieval, we generate descriptive headers for each chunk using an LLM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "231ejJurN0_K"
      },
      "outputs": [],
      "source": [
        "def generate_chunk_header(chunk, model=\"gpt-4.1\"):\n",
        "    \"\"\"\n",
        "    Generates a title/header for a given text chunk using an LLM.\n",
        "\n",
        "    Args:\n",
        "    chunk (str): The text chunk to summarize as a header.\n",
        "    model (str): The model to be used for generating the header. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\n",
        "\n",
        "    Returns:\n",
        "    str: Generated header/title.\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"Generate a concise and informative title for the given text.\"\n",
        "\n",
        "    # Generate a response from the AI model based on the system prompt and text chunk\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": chunk}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the generated header/title, stripping any leading/trailing whitespace\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 带智能标题的文本分块函数解析与优化\n",
        "\n",
        "这个`chunk_text_with_headers`函数实现了一个关键功能：**自动为每个文本块生成描述性标题**，这是上下文增强检索(CCH)的核心环节。下面从功能设计到性能优化进行全面解析：\n",
        "\n",
        "#### 一、函数核心逻辑与流程\n",
        "```python\n",
        "def chunk_text_with_headers(text, n, overlap):\n",
        "    \"\"\"\n",
        "    将文本分块并为每个块生成描述性标题\n",
        "    \n",
        "    参数:\n",
        "    text (str): 待分块的完整文本\n",
        "    n (int): 每个块的字符长度\n",
        "    overlap (int): 相邻块的重叠字符数\n",
        "    \n",
        "    返回:\n",
        "    List[dict]: 包含{标题, 文本内容}的字典列表\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    \n",
        "    # 1. 按固定步长滑动窗口分块\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        chunk = text[i:i + n]  # 提取文本块\n",
        "        \n",
        "        # 2. 调用LLM生成标题(关键步骤)\n",
        "        header = generate_chunk_header(chunk)\n",
        "        \n",
        "        # 3. 存储带标题的文本块\n",
        "        chunks.append({\"header\": header, \"text\": chunk})\n",
        "    \n",
        "    return chunks\n",
        "```\n",
        "\n",
        "#### 二、标题生成函数`generate_chunk_header`实现\n",
        "虽然代码中未定义该函数，但从调用可以推断其功能：\n",
        "```python\n",
        "def generate_chunk_header(chunk_text):\n",
        "    \"\"\"使用LLM为文本块生成描述性标题\"\"\"\n",
        "    # 构建提示词\n",
        "    prompt = f\"\"\"\n",
        "    为以下文本生成一个简洁的标题(不超过15个词)，准确概括其核心内容：\n",
        "    {chunk_text[:500]}  # 截断过长文本，避免超出token限制\n",
        "    \n",
        "    标题必须：\n",
        "    1. 反映文本的核心主题\n",
        "    2. 不包含多余修饰词\n",
        "    3. 格式为短语而非完整句子\n",
        "    \n",
        "    示例输入: \"机器学习是人工智能的一个分支，它使用算法...\"\n",
        "    示例输出: \"机器学习的定义与应用\"\n",
        "    \"\"\"\n",
        "    \n",
        "    # 调用LLM生成标题\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.2,  # 低随机性，保证标题确定性\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "#### 三、关键技术点分析\n",
        "1. **滑动窗口分块策略**：\n",
        "   - 步长计算：`n - overlap`（如n=1000, overlap=200时，每800字符生成一个新块）\n",
        "   - 边界处理：自动处理文本末尾不足n的情况（如最后一块可能只有500字符）\n",
        "\n",
        "2. **标题生成的LLM参数调优**：\n",
        "   - 模型选择：建议使用`gpt-3.5-turbo`（成本低，速度快）\n",
        "   - temperature=0.2：降低随机性，生成更确定性的标题\n",
        "   - 提示词设计：通过明确约束（如\"不超过15个词\"）控制输出格式\n",
        "\n",
        "3. **性能优化考虑**：\n",
        "   - 批处理：每10-20个块批量调用一次LLM，减少API请求次数\n",
        "   - 缓存机制：相同文本块复用已生成的标题（如使用LRU缓存）\n",
        "\n",
        "#### 四、完整处理流程示例\n",
        "```python\n",
        "# 1. 从PDF提取文本\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# 2. 分块并生成标题\n",
        "text_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
        "\n",
        "# 3. 生成嵌入向量\n",
        "embeddings = create_embeddings([\n",
        "    f\"[标题] {chunk['header']}\\n[内容] {chunk['text']}\"\n",
        "    for chunk in text_chunks\n",
        "])\n",
        "\n",
        "# 4. 示例：打印第一个块\n",
        "print(\"示例标题:\", text_chunks[0]['header'])\n",
        "print(\"示例内容:\", text_chunks[0]['text'][:100], \"...\")\n",
        "```\n",
        "\n",
        "#### 五、输出示例与效果对比\n",
        "**原始文本块**（前100字符）：\n",
        "```\n",
        "\"人工智能(AI)是计算机科学的一个分支，旨在使机器能够模拟人类智能。AI系统通过学习数据模式来执行任务，不需要明确的编程指令...\"\n",
        "```\n",
        "\n",
        "**生成的标题**：\n",
        "```\n",
        "\"人工智能的定义与核心原理\"\n",
        "```\n",
        "\n",
        "**对比传统分块**：\n",
        "| 方法               | 检索\"AI定义\"的相关性 | 上下文连贯性 |\n",
        "|--------------------|----------------------|--------------|\n",
        "| 无标题分块         | 中等（需匹配关键词） | 低           |\n",
        "| 带智能标题分块     | 高（标题直接匹配）   | 高           |\n",
        "\n",
        "#### 六、性能优化方案\n",
        "1. **并行处理**：\n",
        "```python\n",
        "# 使用concurrent.futures实现并行标题生成\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    header = generate_chunk_header(chunk)\n",
        "    return {\"header\": header, \"text\": chunk}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    text_chunks = list(executor.map(process_chunk, chunks))\n",
        "```\n",
        "\n",
        "2. **标题质量提升**：\n",
        "```python\n",
        "# 添加标题质量验证逻辑\n",
        "def validate_header(header, chunk_text):\n",
        "    # 计算标题与文本的语义相似度\n",
        "    header_embedding = create_embeddings(header)\n",
        "    chunk_embedding = create_embeddings(chunk_text[:500])\n",
        "    \n",
        "    similarity = cosine_similarity(header_embedding, chunk_embedding)\n",
        "    return similarity > 0.6  # 设置相似度阈值\n",
        "\n",
        "# 对质量不达标的标题进行二次生成\n",
        "if not validate_header(header, chunk_text):\n",
        "    header = generate_chunk_header(chunk_text, retry=True)\n",
        "```\n",
        "\n",
        "#### 七、潜在问题与解决方案\n",
        "1. **标题生成失败**：\n",
        "   - 捕获API异常，设置默认标题（如\"未分类内容\"）\n",
        "   - 添加重试机制（如使用`tenacity`库）\n",
        "\n",
        "2. **标题重复问题**：\n",
        "   - 维护已生成标题集合，发现重复时添加编号（如\"机器学习基础-1\"）\n",
        "   - 调整提示词，要求标题具有唯一性\n",
        "\n",
        "3. **成本控制**：\n",
        "   - 对短文本块（如<300字符）跳过标题生成，使用前20个字符作为标题\n",
        "   - 实现标题生成的开关参数，允许在非关键场景关闭该功能\n",
        "\n",
        "通过这种带智能标题的分块方法，RAG系统的检索准确率通常可提升20-30%，尤其在处理长文档时效果显著。"
      ],
      "metadata": {
        "id": "MFK0x4jSmzT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kmy5UcQnN0_K"
      },
      "outputs": [],
      "source": [
        "def chunk_text_with_headers(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks text into smaller segments and generates headers.\n",
        "\n",
        "    Args:\n",
        "    text (str): The full text to be chunked.\n",
        "    n (int): The chunk size in characters.\n",
        "    overlap (int): Overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[dict]: A list of dictionaries with 'header' and 'text' keys.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store chunks\n",
        "\n",
        "    # Iterate through the text with the specified chunk size and overlap\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        chunk = text[i:i + n]  # Extract a chunk of text\n",
        "        header = generate_chunk_header(chunk)  # Generate a header for the chunk using LLM\n",
        "        chunks.append({\"header\": header, \"text\": chunk})  # Append the header and chunk to the list\n",
        "\n",
        "    return chunks  # Return the list of chunks with headers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BdIB0PnN0_L"
      },
      "source": [
        "## Extracting and Chunking Text from a PDF File\n",
        "Now, we load the PDF, extract text, and split it into chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Ivqd4ZN0_L",
        "outputId": "99580adc-05fc-474d-bf62-d36978ad17ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Chunk:\n",
            "Header: Introduction and Historical Overview of Artificial Intelligence\n",
            "Content: Understanding Artificial Intelligence \n",
            "Chapter 1: Introduction to Artificial Intelligence \n",
            "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
            "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
            "the project of developing systems endowed with the intellectual processes characteristic of \n",
            "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
            "experience. Over the past few decades, advancements in computing power and data availability \n",
            "have significantly accelerated the development and deployment of AI. \n",
            "Historical Context \n",
            "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
            "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
            "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
            "and symbolic methods. The 1980s saw a rise in exp\n"
          ]
        }
      ],
      "source": [
        "# Define the PDF file path\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Extract text from the PDF file\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Chunk the extracted text with headers\n",
        "# We use a chunk size of 1000 characters and an overlap of 200 characters\n",
        "text_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
        "\n",
        "# Print a sample chunk with its generated header\n",
        "print(\"Sample Chunk:\")\n",
        "print(\"Header:\", text_chunks[0]['header'])\n",
        "print(\"Content:\", text_chunks[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyMOdmCdN0_M"
      },
      "source": [
        "## Creating Embeddings for Headers and Text\n",
        "We create embeddings for both headers and text to improve retrieval accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "smiA_2wPN0_M"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to be embedded.\n",
        "    model (str): The embedding model to be used. Default is \"BAAI/bge-en-icl\".\n",
        "\n",
        "    Returns:\n",
        "    dict: The response containing the embedding for the input text.\n",
        "    \"\"\"\n",
        "    # Create embeddings using the specified model and input text\n",
        "    response = client.embeddings.create(\n",
        "        model=model,\n",
        "        input=text\n",
        "    )\n",
        "    # Return the embedding from the response\n",
        "    return response.data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6tYDwBAN0_M",
        "outputId": "908b231a-1526-4fc5-e4f9-0545241fcf02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 42/42 [02:08<00:00,  3.06s/it]\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for each chunk\n",
        "embeddings = []  # Initialize an empty list to store embeddings\n",
        "\n",
        "# Iterate through each text chunk with a progress bar\n",
        "for chunk in tqdm(text_chunks, desc=\"Generating embeddings\"):\n",
        "    # Create an embedding for the chunk's text\n",
        "    text_embedding = create_embeddings(chunk[\"text\"])\n",
        "    # Create an embedding for the chunk's header\n",
        "    header_embedding = create_embeddings(chunk[\"header\"])\n",
        "    # Append the chunk's header, text, and their embeddings to the list\n",
        "    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YBH9QNcN0_M"
      },
      "source": [
        "## Performing Semantic Search\n",
        "We implement cosine similarity to find the most relevant text chunks for a user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZHMpuvY7N0_M"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "    vec1 (np.ndarray): First vector.\n",
        "    vec2 (np.ndarray): Second vector.\n",
        "\n",
        "    Returns:\n",
        "    float: Cosine similarity score.\n",
        "    \"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ag1Nz5iPN0_N"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, chunks, k=5):\n",
        "    \"\"\"\n",
        "    Searches for the most relevant chunks based on a query.\n",
        "\n",
        "    Args:\n",
        "    query (str): User query.\n",
        "    chunks (List[dict]): List of text chunks with embeddings.\n",
        "    k (int): Number of top results.\n",
        "\n",
        "    Returns:\n",
        "    List[dict]: Top-k most relevant chunks.\n",
        "    \"\"\"\n",
        "    # Create an embedding for the query\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    similarities = []  # Initialize a list to store similarity scores\n",
        "\n",
        "    # Iterate through each chunk to calculate similarity scores\n",
        "    for chunk in chunks:\n",
        "        # Compute cosine similarity between query embedding and chunk text embedding\n",
        "        sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"]))\n",
        "        # Compute cosine similarity between query embedding and chunk header embedding\n",
        "        sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[\"header_embedding\"]))\n",
        "        # Calculate the average similarity score\n",
        "        avg_similarity = (sim_text + sim_header) / 2\n",
        "        # Append the chunk and its average similarity score to the list\n",
        "        similarities.append((chunk, avg_similarity))\n",
        "\n",
        "    # Sort the chunks based on similarity scores in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    # Return the top-k most relevant chunks\n",
        "    return [x[0] for x in similarities[:k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 语义搜索函数解析：基于标题和内容双重匹配的混合检索\n",
        "\n",
        "这个`semantic_search`函数实现了一个关键优化：**同时利用文本块内容和标题的嵌入向量进行检索**，显著提升了语义匹配的准确性。下面从技术实现到性能优化进行详细解析：\n",
        "\n",
        "#### 一、核心算法逻辑\n",
        "```python\n",
        "def semantic_search(query, chunks, k=5):\n",
        "    \"\"\"\n",
        "    基于语义相似度检索最相关的文本块\n",
        "    \n",
        "    参数:\n",
        "    query (str): 用户查询\n",
        "    chunks (List[dict]): 包含文本块和嵌入向量的字典列表\n",
        "    k (int): 返回的结果数量\n",
        "    \n",
        "    返回:\n",
        "    List[dict]: 最相关的k个文本块\n",
        "    \"\"\"\n",
        "    # 1. 生成查询的嵌入向量\n",
        "    query_embedding = create_embeddings(query)\n",
        "    \n",
        "    similarities = []\n",
        "    \n",
        "    # 2. 计算每个文本块的匹配度\n",
        "    for chunk in chunks:\n",
        "        # 2.1 内容相似度: 查询与文本内容的匹配度\n",
        "        sim_text = cosine_similarity(\n",
        "            np.array(query_embedding),\n",
        "            np.array(chunk[\"embedding\"])\n",
        "        )\n",
        "        \n",
        "        # 2.2 标题相似度: 查询与文本标题的匹配度\n",
        "        sim_header = cosine_similarity(\n",
        "            np.array(query_embedding),\n",
        "            np.array(chunk[\"header_embedding\"])\n",
        "        )\n",
        "        \n",
        "        # 2.3 加权平均: 标题相似度权重可调整\n",
        "        avg_similarity = (sim_text + sim_header) / 2\n",
        "        \n",
        "        similarities.append((chunk, avg_similarity))\n",
        "    \n",
        "    # 3. 按相似度排序并返回前k个结果\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in similarities[:k]]\n",
        "```\n",
        "\n",
        "#### 二、算法设计亮点\n",
        "1. **混合相似度计算**：\n",
        "   - 内容相似度：捕获查询与文本细节的匹配\n",
        "   - 标题相似度：捕获查询与文本主题的匹配\n",
        "   - 平均策略：平衡主题相关性和细节匹配度\n",
        "\n",
        "2. **向量计算优化**：\n",
        "   - 使用NumPy数组加速余弦相似度计算\n",
        "   - 可扩展为批量向量计算（适用于大规模数据集）\n",
        "\n",
        "3. **数据结构设计**：\n",
        "   - 每个文本块需预计算两个嵌入向量：\n",
        "     ```python\n",
        "     {\n",
        "         \"header\": \"人工智能的定义\",\n",
        "         \"text\": \"人工智能是...\",\n",
        "         \"embedding\": [...],         # 文本内容的嵌入向量\n",
        "         \"header_embedding\": [...]   # 标题的嵌入向量\n",
        "     }\n",
        "     ```\n",
        "\n",
        "#### 三、性能优化方案\n",
        "1. **索引加速**：\n",
        "```python\n",
        "# 使用FAISS库构建向量索引\n",
        "import faiss\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    \"\"\"构建FAISS索引加速语义搜索\"\"\"\n",
        "    # 合并内容和标题的嵌入向量（可调整权重）\n",
        "    dim = len(chunks[0][\"embedding\"])\n",
        "    index = faiss.IndexFlatIP(dim)  # 内积索引，适合余弦相似度\n",
        "    \n",
        "    # 构建索引\n",
        "    all_vectors = []\n",
        "    for chunk in chunks:\n",
        "        # 可调整标题和内容的权重\n",
        "        vector = 0.6 * np.array(chunk[\"embedding\"]) + 0.4 * np.array(chunk[\"header_embedding\"])\n",
        "        all_vectors.append(vector.astype('float32'))\n",
        "    \n",
        "    index.add(np.vstack(all_vectors))\n",
        "    return index\n",
        "\n",
        "# 优化后的搜索函数\n",
        "def optimized_semantic_search(query, chunks, index, k=5):\n",
        "    query_vector = create_embeddings(query)\n",
        "    query_vector = (0.6 * np.array(query_vector) + 0.4 * np.array(query_vector)).astype('float32')\n",
        "    \n",
        "    # 使用FAISS搜索\n",
        "    distances, indices = index.search(np.array([query_vector]), k)\n",
        "    \n",
        "    # 返回最相似的文本块\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "```\n",
        "\n",
        "2. **缓存机制**：\n",
        "```python\n",
        "# 使用LRU缓存避免重复计算\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=1000)  # 缓存最近1000次查询结果\n",
        "def cached_semantic_search(query, chunks, k=5):\n",
        "    return semantic_search(query, chunks, k)\n",
        "```\n",
        "\n",
        "#### 四、检索效果对比\n",
        "| 查询示例 | 传统检索（仅内容） | 混合检索（内容+标题） |\n",
        "|----------|--------------------|----------------------|\n",
        "| \"AI的定义\" | 可能返回包含\"AI\"但主题不相关的段落 | 优先返回标题为\"人工智能的定义\"的段落 |\n",
        "| \"机器学习应用\" | 需精确匹配\"应用\"关键词 | 标题包含\"应用\"的段落权重更高 |\n",
        "\n",
        "#### 五、扩展应用场景\n",
        "1. **多模态检索**：\n",
        "   - 对图片/表格添加文本标题，参与语义检索\n",
        "   - 示例：\n",
        "     ```python\n",
        "     {\n",
        "         \"header\": \"图3: 2020-2023年AI专利数量增长趋势\",\n",
        "         \"text\": \"该图展示了...\",\n",
        "         \"embedding\": [...],\n",
        "         \"header_embedding\": [...]\n",
        "     }\n",
        "     ```\n",
        "\n",
        "2. **跨语言检索**：\n",
        "   - 使用多语言嵌入模型（如XLM-R）\n",
        "   - 支持查询语言与文档语言不一致的场景\n",
        "\n",
        "3. **知识图谱增强**：\n",
        "   - 将标题映射到知识图谱实体\n",
        "   - 示例：标题\"Transformer架构\"关联到实体`https://dbpedia.org/resource/Transformer_(machine_learning_model)`\n",
        "\n",
        "#### 六、参数调优建议\n",
        "1. **相似度权重调整**：\n",
        "   ```python\n",
        "   # 为标题相似度分配更高权重（适用于主题优先的场景）\n",
        "   avg_similarity = 0.3 * sim_text + 0.7 * sim_header\n",
        "   ```\n",
        "\n",
        "2. **嵌入模型选择**：\n",
        "   - 通用场景：text-embedding-ada-002\n",
        "   - 专业领域：BAAI/bge-large-en（训练时加入领域语料）\n",
        "\n",
        "3. **k值选择**：\n",
        "   - 对话系统：k=3（快速响应）\n",
        "   - 研究助手：k=10（提供更多上下文）\n",
        "\n",
        "通过这种基于标题和内容双重匹配的语义搜索方法，RAG系统的检索准确率通常可提升20-30%，尤其在处理长文档和专业领域内容时效果显著。"
      ],
      "metadata": {
        "id": "IFlxX0whoAL0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_YheScJN0_N"
      },
      "source": [
        "## Running a Query on Extracted Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrSeuwZqN0_N",
        "outputId": "7e4bc4df-3351-40c9-d426-758a8fd61d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Header 1: Key Principles for Building Trustworthy AI: Explainability, Privacy, Accountability, and Transparency\n",
            "Content:\n",
            "systems. Explainable AI (XAI) \n",
            "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
            "fairness and accuracy. \n",
            "Privacy and Data Protection \n",
            "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
            "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
            "and complying with data protection regulations are crucial. \n",
            "Accountability and Responsibility \n",
            "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
            "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
            "developers, deployers, and users of AI systems. \n",
            "Chapter 20: Building Trust in AI \n",
            "Transparency and Explainability \n",
            "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
            "and providing insights into their decision-making processes helps users assess their reliability \n",
            "and fairness. \n",
            "Robustness and Reliability \n",
            "\n",
            "\n",
            "Header 2: Key Challenges in AI: Explainability, Privacy, Job Displacement, Autonomy, and Weaponization\n",
            "Content:\n",
            "inability \n",
            "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
            "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
            "crucial for building trust and accountability. \n",
            " \n",
            " \n",
            "Privacy and Security \n",
            "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
            "Protecting sensitive information and ensuring responsible data handling are essential. \n",
            "Job Displacement \n",
            "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
            "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
            "of AI-driven automation is a key challenge. \n",
            "Autonomy and Control \n",
            "As AI systems become more autonomous, questions arise about control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The p\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load validation data\n",
        "with open('val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "query = data[0]['question']\n",
        "\n",
        "# Retrieve the top 2 most relevant text chunks\n",
        "top_chunks = semantic_search(query, embeddings, k=2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Query:\", query)\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"Header {i+1}: {chunk['header']}\")\n",
        "    print(f\"Content:\\n{chunk['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bE0VF4mN0_N"
      },
      "source": [
        "## Generating a Response Based on Retrieved Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oDDKZDHiN0_N"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt for the AI assistant\n",
        "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
        "\n",
        "def generate_response(system_prompt, user_message, model=\"gpt-4.1\"):\n",
        "    \"\"\"\n",
        "    Generates a response from the AI model based on the system prompt and user message.\n",
        "\n",
        "    Args:\n",
        "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
        "    user_message (str): The user's message or query.\n",
        "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
        "\n",
        "    Returns:\n",
        "    dict: The response from the AI model.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Create the user prompt based on the top chunks\n",
        "user_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\n",
        "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
        "\n",
        "# Generate AI response\n",
        "ai_response = generate_response(system_prompt, user_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZFJmj7UVui3",
        "outputId": "39dcd311-f1dc-48e9-e93a-77236bc6cb63"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-BiJxm4X5q57sWmodqQEkcK5ryALzn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Explainable AI (XAI) techniques aim to make AI decisions more understandable, enabling users to assess their fairness and accuracy. It is considered important because making AI systems understandable and providing insights into their decision-making processes helps users assess their reliability and fairness, which is key to building trust in AI. Additionally, enhancing transparency and explainability is crucial for building trust and accountability, especially since many AI systems, particularly deep learning models, are \"black boxes\" that are difficult to interpret.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1749902870, model='gpt-4.1-2025-04-14', object='chat.completion', service_tier=None, system_fingerprint='fp_07e970ab25', usage=CompletionUsage(completion_tokens=97, prompt_tokens=480, total_tokens=577, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPaQL5HN0_N"
      },
      "source": [
        "## Evaluating the AI Response\n",
        "We compare the AI response with the expected answer and assign a score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDace6rRN0_N",
        "outputId": "788ca879-a96f-4255-c637-798544ac9e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Score: 1\n"
          ]
        }
      ],
      "source": [
        "# Define evaluation system prompt\n",
        "evaluate_system_prompt = \"\"\"You are an intelligent evaluation system.\n",
        "Assess the AI assistant's response based on the provided context.\n",
        "- Assign a score of 1 if the response is very close to the true answer.\n",
        "- Assign a score of 0.5 if the response is partially correct.\n",
        "- Assign a score of 0 if the response is incorrect.\n",
        "Return only the score (0, 0.5, or 1).\"\"\"\n",
        "\n",
        "# Extract the ground truth answer from validation data\n",
        "true_answer = data[0]['ideal_answer']\n",
        "\n",
        "# Construct evaluation prompt\n",
        "evaluation_prompt = f\"\"\"\n",
        "User Query: {query}\n",
        "AI Response: {ai_response}\n",
        "True Answer: {true_answer}\n",
        "{evaluate_system_prompt}\n",
        "\"\"\"\n",
        "\n",
        "# Generate evaluation score\n",
        "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
        "\n",
        "# Print the evaluation score\n",
        "print(\"Evaluation Score:\", evaluation_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXwSdlDlV1ZI",
        "outputId": "89787028-08ce-4bcb-c2ee-3c05ef018e59"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-BiJyIbz3BLiXLTsPZwizopbFf2OVu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1749902902, model='gpt-4.1-2025-04-14', object='chat.completion', service_tier=None, system_fingerprint='fp_07e970ab25', usage=CompletionUsage(completion_tokens=2, prompt_tokens=719, total_tokens=721, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatCompletion响应参数详解：从结构到含义的全面解析  \n",
        "\n",
        "以下是对ChatCompletion响应中各参数的详细拆解，帮助理解LLM交互的技术细节：  \n",
        "\n",
        "\n",
        "#### 一、顶级字段：响应元数据  \n",
        "```python\n",
        "ChatCompletion(\n",
        "    id='chatcmpl-BiJyIbz3BLiXLTsPZwizopbFf2OVu',  # ①\n",
        "    created=1749902902,  # ②\n",
        "    model='gpt-4.1-2025-04-14',  # ③\n",
        "    object='chat.completion',  # ④\n",
        "    service_tier=None,\n",
        "    system_fingerprint='fp_07e970ab25',\n",
        "    # 其他字段...\n",
        ")\n",
        "```  \n",
        "\n",
        "1. **`id`**：  \n",
        "   - **类型**：字符串  \n",
        "   - **含义**：本次API调用的唯一标识符，用于追踪请求记录（如调试、计费查询）。  \n",
        "   - **示例**：`chatcmpl-BiJyIbz3BLiXLTsPZwizopbFf2OVu`（以`chatcmpl-`开头）。  \n",
        "\n",
        "2. **`created`**：  \n",
        "   - **类型**：时间戳（UTC秒）  \n",
        "   - **含义**：响应生成的时间，可用于计算请求耗时。  \n",
        "   - **转换示例**：`1749902902` → `2025年1月23日 14:48:22`（需通过时间转换工具解析）。  \n",
        "\n",
        "3. **`model`**：  \n",
        "   - **类型**：字符串  \n",
        "   - **含义**：生成响应的模型版本。  \n",
        "   - **示例**：`gpt-4.1-2025-04-14`（表示2025年4月14日发布的GPT-4.1版本）。  \n",
        "\n",
        "4. **`object`**：  \n",
        "   - **类型**：字符串  \n",
        "   - **含义**：响应对象的类型，固定为`chat.completion`（表示聊天完成响应）。  \n",
        "\n",
        "\n",
        "#### 二、核心字段：choices（响应内容）  \n",
        "```python\n",
        "choices=[\n",
        "    Choice(\n",
        "        finish_reason='stop',  # ①\n",
        "        index=0,  # ②\n",
        "        logprobs=None,\n",
        "        message=ChatCompletionMessage(  # ③\n",
        "            content='1',\n",
        "            role='assistant',\n",
        "            # 其他message字段...\n",
        "        ),\n",
        "        # 其他Choice字段...\n",
        "    )\n",
        "]\n",
        "```  \n",
        "\n",
        "1. **`finish_reason`**：  \n",
        "   - **类型**：字符串  \n",
        "   - **含义**：模型停止生成的原因，常见值：  \n",
        "     - `stop`：正常生成完毕（遇到结束符或达到最大token数）；  \n",
        "     - `length`：因达到`max_tokens`限制而中断；  \n",
        "     - `function_call`：因调用工具函数而停止（适用于函数调用场景）。  \n",
        "\n",
        "2. **`index`**：  \n",
        "   - **类型**：整数  \n",
        "   - **含义**：响应在多选项中的索引（通常`index=0`，因默认只返回1个结果）。  \n",
        "\n",
        "3. **`message`**：  \n",
        "   - **类型**：ChatCompletionMessage对象  \n",
        "   - **核心子字段**：  \n",
        "     - `role`：角色标识，固定为`assistant`（表示助手响应）；  \n",
        "     - `content`：生成的文本内容（如`'1'`）；  \n",
        "     - `function_call`：若调用了函数，此处包含函数名和参数（本例中为`None`）。  \n",
        "\n",
        "\n",
        "#### 三、内容过滤字段：安全审核结果  \n",
        "```python\n",
        "# 在choices和prompt_filter_results中均有安全过滤字段\n",
        "{\n",
        "    'hate': {'filtered': False, 'severity': 'safe'},  # 仇恨内容检测\n",
        "    'self_harm': {'filtered': False, 'severity': 'safe'},  # 自残内容检测\n",
        "    'sexual': {'filtered': False, 'severity': 'safe'},  # 色情内容检测\n",
        "    'violence': {'filtered': False, 'severity': 'safe'},  # 暴力内容检测\n",
        "    # 其他过滤类型...\n",
        "}\n",
        "```  \n",
        "\n",
        "- **字段含义**：  \n",
        "  - `filtered`：是否触发过滤（`True`表示内容被屏蔽，`False`表示通过）；  \n",
        "  - `severity`：风险等级（如`'safe'`表示安全，`'high'`表示高风险）。  \n",
        "- **作用**：OpenAI的内容安全机制，自动检测并过滤违规内容。  \n",
        "\n",
        "\n",
        "#### 四、用量统计：usage  \n",
        "```python\n",
        "usage=CompletionUsage(\n",
        "    completion_tokens=2,  # ①\n",
        "    prompt_tokens=719,  # ②\n",
        "    total_tokens=721,  # ③\n",
        "    # 其他用量细节...\n",
        ")\n",
        "```  \n",
        "\n",
        "1. **`completion_tokens`**：  \n",
        "   - **含义**：生成响应消耗的token数（本例中为2，即输出`'1'`约2个token）。  \n",
        "\n",
        "2. **`prompt_tokens`**：  \n",
        "   - **含义**：输入提示词消耗的token数（如用户问题、上下文等，本例中为719）。  \n",
        "\n",
        "3. **`total_tokens`**：  \n",
        "   - **含义**：总消耗token数（`prompt_tokens + completion_tokens`，本例中719+2=721）。  \n",
        "- **重要性**：token用量直接影响API计费（如GPT-4.1每1k tokens约0.03美元）。  \n",
        "\n",
        "\n",
        "#### 五、扩展字段：prompt_filter_results  \n",
        "```python\n",
        "prompt_filter_results=[\n",
        "    {\n",
        "        'prompt_index': 0,\n",
        "        'content_filter_results': {\n",
        "            'jailbreak': {'filtered': False, 'detected': False},  # 越狱攻击检测\n",
        "            # 其他过滤类型与choices中的字段一致...\n",
        "        }\n",
        "    }\n",
        "]\n",
        "```  \n",
        "\n",
        "- **作用**：对输入提示词进行安全检测，防止用户通过prompt注入（如诱导模型绕过安全限制）。  \n",
        "- **`jailbreak`字段**：检测是否存在“越狱”尝试（即诱导模型生成违规内容），`detected=False`表示未检测到风险。  \n",
        "\n",
        "\n",
        "#### 六、参数应用场景与实践价值  \n",
        "1. **调试与追踪**：  \n",
        "   - 通过`id`和`created`字段关联请求与响应，排查超时或异常问题。  \n",
        "\n",
        "2. **成本控制**：  \n",
        "   - 监控`usage.total_tokens`统计API消耗，优化提示词长度以降低成本（如精简上下文）。  \n",
        "\n",
        "3. **安全合规**：  \n",
        "   - 检查`content_filter_results`确保输出无违规内容，尤其在医疗、金融等敏感领域。  \n",
        "\n",
        "4. **模型版本管理**：  \n",
        "   - 通过`model`字段确认当前使用的模型版本，避免因版本更新导致的效果波动（如API默认升级时）。  \n",
        "\n",
        "\n",
        "#### 七、典型异常场景处理  \n",
        "1. **`finish_reason='length'`**：  \n",
        "   - **处理**：增加`max_tokens`参数值，或精简输入提示词以允许更多输出。  \n",
        "\n",
        "2. **`content_filter_results['filtered']=True`**：  \n",
        "   - **处理**：修改提示词以消除违规内容，或联系OpenAI调整安全策略（如企业级服务可申请定制过滤规则）。  \n",
        "\n",
        "3. **`usage.total_tokens`异常高**：  \n",
        "   - **优化**：  \n",
        "     - 使用token计数器（如`tiktoken`库）提前估算提示词token数；  \n",
        "     - 对长文本进行分块处理，避免一次性传入过多内容。  \n",
        "\n",
        "\n",
        "#### 八、字段解析代码示例  \n",
        "```python\n",
        "# 解析响应参数的Python代码示例\n",
        "response = client.chat.completions.create(...)  # 假设已获取响应\n",
        "\n",
        "# 提取核心信息\n",
        "print(f\"模型版本: {response.model}\")\n",
        "print(f\"响应内容: {response.choices[0].message.content}\")\n",
        "print(f\"总消耗token: {response.usage.total_tokens}\")\n",
        "\n",
        "# 安全检测结果\n",
        "if any(\n",
        "    filter_result['filtered']\n",
        "    for choice in response.choices\n",
        "    for filter_result in choice.content_filter_results.values()\n",
        "):\n",
        "    print(\"警告：响应包含被过滤的内容\")\n",
        "```  \n",
        "\n",
        "通过理解这些参数，开发者可更精准地控制LLM交互过程，优化成本、安全性和响应质量。"
      ],
      "metadata": {
        "id": "9Gvebh9ol4ei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWAdvrpqo6Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG系统评估模块详解：自动化评分机制与实现原理\n",
        "\n",
        "这段代码实现了RAG系统中至关重要的**自动化评估模块**，通过LLM自身对生成的回答进行评分。这种方法巧妙地利用了模型的理解能力，避免了传统基于规则或简单关键词匹配的评估局限性。\n",
        "\n",
        "#### 一、评估系统的核心设计\n",
        "```python\n",
        "# 定义评估系统的提示词\n",
        "evaluate_system_prompt = \"\"\"You are an intelligent evaluation system.\n",
        "Assess the AI assistant's response based on the provided context.\n",
        "- Assign a score of 1 if the response is very close to the true answer.\n",
        "- Assign a score of 0.5 if the response is partially correct.\n",
        "- Assign a score of 0 if the response is incorrect.\n",
        "Return only the score (0, 0.5, or 1).\"\"\"\n",
        "```\n",
        "\n",
        "#### 关键设计点：\n",
        "1. **角色定位**：将评估者定义为\"intelligent evaluation system\"，强调其专业评估能力\n",
        "2. **评分标准**：\n",
        "   - 1分：回答与标准答案高度一致\n",
        "   - 0.5分：部分正确（如遗漏次要信息但包含关键内容）\n",
        "   - 0分：回答错误或完全脱离上下文\n",
        "3. **输出约束**：强制返回数值评分（避免生成解释性文本）\n",
        "\n",
        "#### 二、评估流程实现\n",
        "```python\n",
        "# 构建评估提示词\n",
        "evaluation_prompt = f\"\"\"\n",
        "User Query: {query}\n",
        "AI Response: {ai_response}\n",
        "True Answer: {true_answer}\n",
        "{evaluate_system_prompt}\n",
        "\"\"\"\n",
        "\n",
        "# 生成评估分数\n",
        "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
        "\n",
        "# 提取评分结果\n",
        "print(\"Evaluation Score:\", evaluation_response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "#### 流程解析：\n",
        "1. **信息输入**：\n",
        "   - 用户原始问题（User Query）\n",
        "   - AI生成的回答（AI Response）\n",
        "   - 标准答案（True Answer）\n",
        "   - 评估规则（evaluate_system_prompt）\n",
        "\n",
        "2. **评估执行**：\n",
        "   - 调用相同的`generate_response`函数，但使用评估专用的提示词\n",
        "   - 模型需根据上述信息输出0、0.5或1\n",
        "\n",
        "3. **结果输出**：\n",
        "   - 直接获取模型返回的数值作为评分结果\n",
        "\n",
        "#### 三、评分机制的工作原理\n",
        "评估模型会执行以下分析步骤：\n",
        "1. **相关性检查**：AI回答是否针对用户问题\n",
        "2. **事实核对**：\n",
        "   - 关键事实是否与标准答案一致\n",
        "   - 是否包含上下文未提供的信息（即产生幻觉）\n",
        "3. **完整性评估**：\n",
        "   - 是否涵盖标准答案的所有要点\n",
        "   - 是否遗漏关键细节\n",
        "\n",
        "#### 四、评估系统的优势\n",
        "1. **语义理解能力**：\n",
        "   - 能识别同义表达（如\"AI\"与\"人工智能\"视为等同）\n",
        "   - 理解复杂逻辑关系（如因果、对比）\n",
        "\n",
        "2. **上下文感知**：\n",
        "   - 区分回答是基于上下文还是模型自身知识\n",
        "   - 严格执行\"仅根据上下文回答\"的约束\n",
        "\n",
        "3. **适应性强**：\n",
        "   - 可评估不同类型的问题（如事实性、解释性）\n",
        "   - 无需为特定领域定制评估规则\n",
        "\n",
        "#### 五、潜在问题与优化方向\n",
        "1. **评分一致性**：\n",
        "   - 同一回答可能因评估时的随机性得到不同分数\n",
        "   - **优化**：设置temperature=0，或多次评估取平均值\n",
        "\n",
        "2. **标准答案偏差**：\n",
        "   - 人工标注的标准答案可能存在主观性\n",
        "   - **优化**：采用多标注者共识，或使用金标准数据集\n",
        "\n",
        "3. **评估成本**：\n",
        "   - 每次评估需额外调用一次LLM\n",
        "   - **优化**：\n",
        "     - 批量评估多个回答\n",
        "     - 使用轻量级模型进行初步评估\n",
        "\n",
        "#### 六、评估结果的应用场景\n",
        "1. **模型选择与调优**：\n",
        "   - 比较不同LLM在特定任务上的表现\n",
        "   - 优化检索参数（如k值、上下文大小）\n",
        "\n",
        "2. **系统监控**：\n",
        "   - 实时监测RAG系统的回答质量\n",
        "   - 识别性能下降的时间点并触发警报\n",
        "\n",
        "3. **迭代改进**：\n",
        "   - 分析低评分回答，针对性改进：\n",
        "     - 增强检索相关性\n",
        "     - 优化提示工程\n",
        "     - 改进分块策略\n",
        "\n",
        "#### 七、扩展实现：更精细的评估指标\n",
        "除了整体评分外，还可扩展评估维度：\n",
        "```python\n",
        "# 扩展评估系统，返回多维度评分\n",
        "expanded_evaluate_prompt = \"\"\"\n",
        "Assess the AI response based on the context:\n",
        "1. Relevance (0-1): How relevant is the response to the query?\n",
        "2. Factuality (0-1): How factual is the response?\n",
        "3. Completeness (0-1): How complete is the response?\n",
        "4. Coherence (0-1): How coherent is the response?\n",
        "\n",
        "Return scores in JSON format:\n",
        "{\n",
        "\"relevance\": 0.8,\n",
        "\"factuality\": 1.0,\n",
        "\"completeness\": 0.6,\n",
        "\"coherence\": 0.9\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 解析多维评分结果\n",
        "scores = json.loads(evaluation_response.choices[0].message.content)\n",
        "print(f\"事实性得分: {scores['factuality']}\")\n",
        "print(f\"完整性得分: {scores['completeness']}\")\n",
        "```\n",
        "\n",
        "这种多维评估能更精确地定位系统弱点，指导针对性优化。\n",
        "\n",
        "#### 八、最佳实践建议\n",
        "1. **评估数据准备**：\n",
        "   - 使用领域内代表性问题\n",
        "   - 每个问题配备高质量标准答案\n",
        "   - 包含多样化的问题类型（事实性、开放性、对比性）\n",
        "\n",
        "2. **评估模型选择**：\n",
        "   - 使用比生成模型更强大的LLM进行评估（如用GPT-4评估GPT-3.5的回答）\n",
        "\n",
        "3. **结果可视化**：\n",
        "   - 定期生成评估报告，跟踪系统性能变化\n",
        "   - 可视化不同维度的评分趋势\n",
        "\n",
        "通过这种自动化评估机制，RAG系统能够持续优化，逐步提升回答质量和用户满意度。"
      ],
      "metadata": {
        "id": "MTXdUtMco8ZK"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}