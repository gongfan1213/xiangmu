{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "GqA4V2C5GWud"
      },
      "source": [
        "# Fusion Retrieval: Combining Vector and Keyword Search\n",
        "\n",
        "In this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
        "\n",
        "## Why Fusion Retrieval Matters\n",
        "\n",
        "Traditional RAG systems typically rely on vector search alone, but this has limitations:\n",
        "\n",
        "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
        "- Keyword search is great for specific terms but lacks semantic understanding\n",
        "- Different queries perform better with different retrieval methods\n",
        "\n",
        "Fusion retrieval gives us the best of both worlds by:\n",
        "\n",
        "- Performing both vector-based and keyword-based retrieval\n",
        "- Normalizing the scores from each approach\n",
        "- Combining them with a weighted formula\n",
        "- Ranking documents based on the combined score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 融合检索：向量与关键词搜索的结合  \n",
        "在本笔记本中，我实现了一个融合检索系统，该系统结合了语义向量搜索与基于关键词的BM25检索的优势。这种方法通过同时捕捉概念相似性和精确关键词匹配来提升检索质量。  \n",
        "\n",
        "\n",
        "#### 为什么融合检索如此重要？  \n",
        "传统的检索增强生成（RAG）系统通常仅依赖向量搜索，但这存在局限性：  \n",
        "\n",
        "- **向量搜索**在语义相似性方面表现出色，但可能遗漏精确的关键词匹配；  \n",
        "- **关键词搜索**擅长处理特定术语，但缺乏语义理解能力；  \n",
        "- 不同的查询在不同的检索方法下表现各异。  \n",
        "\n",
        "融合检索通过以下方式实现优势互补：  \n",
        "1. 同时执行基于向量和基于关键词的检索；  \n",
        "2. 对两种方法的得分进行标准化处理；  \n",
        "3. 通过加权公式将二者结合；  \n",
        "4. 基于组合得分对文档进行排序。"
      ],
      "metadata": {
        "id": "KnowF2ArGhIA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW9H2mIiGWuf"
      },
      "source": [
        "## Setting Up the Environment\n",
        "We begin by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PymuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MPqCWp8GvDs",
        "outputId": "9ce96ed5-f6dd-4630-bd3e-ee27cff96b58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PymuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PymuPDF\n",
            "Successfully installed PymuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QngU2Hy-HExT",
        "outputId": "f4d15144-cd33-4755-e8ea-802d1c1015cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eTmB9sOEGWug"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrWQ2MF-GWug"
      },
      "source": [
        "## Setting Up the OpenAI API Client\n",
        "We initialize the OpenAI client to generate embeddings and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H8eNblpBGWuh"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"http://xxxxxxx0/v1/\",\n",
        "    api_key=\"skxxxxxxxxxxt9\" # Retrieve the API key from environment variables\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XHdV8p0GWuh"
      },
      "source": [
        "## Document Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-HgHOrk8GWuh"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text content from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content\n",
        "    \"\"\"\n",
        "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
        "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
        "    text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document[page_num]  # Get the page object\n",
        "        text += page.get_text()  # Extract text from the page and append to the text string\n",
        "\n",
        "    return text  # Return the extracted text content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O0iMnTfRGWuh"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        chunk_overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunks with text and metadata\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store chunks\n",
        "\n",
        "    # Iterate over the text with the specified chunk size and overlap\n",
        "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
        "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
        "        if chunk:  # Ensure we don't add empty chunks\n",
        "            chunk_data = {\n",
        "                \"text\": chunk,  # The chunk text\n",
        "                \"metadata\": {\n",
        "                    \"start_char\": i,  # Start character index of the chunk\n",
        "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
        "                }\n",
        "            }\n",
        "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
        "\n",
        "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
        "    return chunks  # Return the list of chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EQ6N_wHlGWui"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing extra whitespace and special characters.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
        "    text = text.replace('\\\\t', ' ')\n",
        "    text = text.replace('\\\\n', ' ')\n",
        "\n",
        "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvwvH1iGWui"
      },
      "source": [
        "## Creating Our Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0v8hWk1wGWui"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"\n",
        "    Create embeddings for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (str or List[str]): Input text(s)\n",
        "        model (str): Embedding model name\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: Embedding vectors\n",
        "    \"\"\"\n",
        "    # Handle both string and list inputs\n",
        "    input_texts = texts if isinstance(texts, list) else [texts]\n",
        "\n",
        "    # Process in batches if needed (OpenAI API limits)\n",
        "    batch_size = 100\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Iterate over the input texts in batches\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
        "\n",
        "        # Create embeddings for the current batch\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=batch\n",
        "        )\n",
        "\n",
        "        # Extract embeddings from the response\n",
        "        batch_embeddings = [item.embedding for item in response.data]\n",
        "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
        "\n",
        "    # If input was a string, return just the first embedding\n",
        "    if isinstance(texts, str):\n",
        "        return all_embeddings[0]\n",
        "\n",
        "    # Otherwise return all embeddings\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HvA9QeKJGWui"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store text content\n",
        "        self.metadata = []  # List to store metadata\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text content\n",
        "            embedding (List[float]): The embedding vector\n",
        "            metadata (Dict, optional): Additional metadata\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Append the embedding vector\n",
        "        self.texts.append(text)  # Append the text content\n",
        "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
        "\n",
        "    def add_items(self, items, embeddings):\n",
        "        \"\"\"\n",
        "        Add multiple items to the vector store.\n",
        "\n",
        "        Args:\n",
        "            items (List[Dict]): List of text items\n",
        "            embeddings (List[List[float]]): List of embedding vectors\n",
        "        \"\"\"\n",
        "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
        "            self.add_item(\n",
        "                text=item[\"text\"],  # Extract text from item\n",
        "                embedding=embedding,  # Use corresponding embedding\n",
        "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
        "            )\n",
        "\n",
        "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding with similarity scores.\n",
        "\n",
        "        Args:\n",
        "            query_embedding (List[float]): Query embedding vector\n",
        "            k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            similarity = cosine_similarity([query_vector], [vector])[0][0]  # Compute cosine similarity\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results with scores\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Retrieve text by index\n",
        "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
        "                \"similarity\": float(score)  # Add similarity score\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_all_documents(self):\n",
        "        \"\"\"\n",
        "        Get all documents in the store.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: All documents\n",
        "        \"\"\"\n",
        "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyTW7vbBGWuj"
      },
      "source": [
        "## BM25 Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WNKsDQKwGWuj"
      },
      "outputs": [],
      "source": [
        "def create_bm25_index(chunks):\n",
        "    \"\"\"\n",
        "    Create a BM25 index from the given chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[Dict]): List of text chunks\n",
        "\n",
        "    Returns:\n",
        "        BM25Okapi: A BM25 index\n",
        "    \"\"\"\n",
        "    # Extract text from each chunk\n",
        "    texts = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "    # Tokenize each document by splitting on whitespace\n",
        "    tokenized_docs = [text.split() for text in texts]\n",
        "\n",
        "    # Create the BM25 index using the tokenized documents\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    # Print the number of documents in the BM25 index\n",
        "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
        "\n",
        "    return bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ek6rTRAhGWuj"
      },
      "outputs": [],
      "source": [
        "def bm25_search(bm25, chunks, query, k=5):\n",
        "    \"\"\"\n",
        "    Search the BM25 index with a query.\n",
        "\n",
        "    Args:\n",
        "        bm25 (BM25Okapi): BM25 index\n",
        "        chunks (List[Dict]): List of text chunks\n",
        "        query (str): Query string\n",
        "        k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Top k results with scores\n",
        "    \"\"\"\n",
        "    # Tokenize the query by splitting it into individual words\n",
        "    query_tokens = query.split()\n",
        "\n",
        "    # Get BM25 scores for the query tokens against the indexed documents\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Initialize an empty list to store results with their scores\n",
        "    results = []\n",
        "\n",
        "    # Iterate over the scores and corresponding chunks\n",
        "    for i, score in enumerate(scores):\n",
        "        # Create a copy of the metadata to avoid modifying the original\n",
        "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
        "        # Add index to metadata\n",
        "        metadata[\"index\"] = i\n",
        "\n",
        "        results.append({\n",
        "            \"text\": chunks[i][\"text\"],\n",
        "            \"metadata\": metadata,  # Add metadata with index\n",
        "            \"bm25_score\": float(score)\n",
        "        })\n",
        "\n",
        "    # Sort the results by BM25 score in descending order\n",
        "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
        "\n",
        "    # Return the top k results\n",
        "    return results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LodmDPKyGWuj"
      },
      "source": [
        "## Fusion Retrieval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gs9AtrScGWuj"
      },
      "outputs": [],
      "source": [
        "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Perform fusion retrieval combining vector-based and BM25 search.\n",
        "\n",
        "    Args:\n",
        "        query (str): Query string\n",
        "        chunks (List[Dict]): Original text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of results to return\n",
        "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Top k results based on combined scores\n",
        "    \"\"\"\n",
        "    print(f\"Performing fusion retrieval for query: {query}\")\n",
        "\n",
        "    # Define small epsilon to avoid division by zero\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Get vector search results\n",
        "    query_embedding = create_embeddings(query)  # Create embedding for the query\n",
        "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n",
        "\n",
        "    # Get BM25 search results\n",
        "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
        "\n",
        "    # Create dictionaries to map document index to score\n",
        "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
        "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
        "\n",
        "    # Ensure all documents have scores for both methods\n",
        "    all_docs = vector_store.get_all_documents()\n",
        "    combined_results = []\n",
        "\n",
        "    for i, doc in enumerate(all_docs):\n",
        "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
        "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
        "        combined_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"vector_score\": vector_score,\n",
        "            \"bm25_score\": bm25_score,\n",
        "            \"index\": i\n",
        "        })\n",
        "\n",
        "    # Extract scores as arrays\n",
        "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
        "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
        "\n",
        "    # Normalize scores\n",
        "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
        "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
        "\n",
        "    # Compute combined scores\n",
        "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
        "\n",
        "    # Add combined scores to results\n",
        "    for i, score in enumerate(combined_scores):\n",
        "        combined_results[i][\"combined_score\"] = float(score)\n",
        "\n",
        "    # Sort by combined score (descending)\n",
        "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "\n",
        "    # Return top k results\n",
        "    top_results = combined_results[:k]\n",
        "\n",
        "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
        "    return top_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpP_qkl7GWuk"
      },
      "source": [
        "## Document Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "R9xmOqFrGWuk"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for fusion retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        chunk_overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Clean the extracted text to remove extra whitespace and special characters\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Split the cleaned text into overlapping chunks\n",
        "    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
        "\n",
        "    # Extract the text content from each chunk for embedding creation\n",
        "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "\n",
        "    # Create embeddings for the chunk texts\n",
        "    embeddings = create_embeddings(chunk_texts)\n",
        "\n",
        "    # Initialize the vector store\n",
        "    vector_store = SimpleVectorStore()\n",
        "\n",
        "    # Add the chunks and their embeddings to the vector store\n",
        "    vector_store.add_items(chunks, embeddings)\n",
        "    print(f\"Added {len(chunks)} items to vector store\")\n",
        "\n",
        "    # Create a BM25 index from the chunks\n",
        "    bm25_index = create_bm25_index(chunks)\n",
        "\n",
        "    # Return the chunks, vector store, and BM25 index\n",
        "    return chunks, vector_store, bm25_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVhzHwjgGWuk"
      },
      "source": [
        "## Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4cpE7XRcGWuk"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context from retrieved documents\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
        "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n",
        "\n",
        "    # Format the user prompt with the context and query\n",
        "    user_prompt = f\"\"\"Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Please answer the question based on the provided context.\"\"\"\n",
        "\n",
        "    # Generate the response using the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Specify the model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
        "            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n",
        "        ],\n",
        "        temperature=0.1  # Set the temperature for response generation\n",
        "    )\n",
        "\n",
        "    # Return the generated response\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0h-AyTWGWuk"
      },
      "source": [
        "## Main Retrieval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3qeEdxSOGWuk"
      },
      "outputs": [],
      "source": [
        "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Answer a query using fusion RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results including retrieved documents and response\n",
        "    \"\"\"\n",
        "    # Retrieve documents using fusion retrieval method\n",
        "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It8WqZtvGWuk"
      },
      "source": [
        "## Comparing Retrieval Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ASH192HcGWuk"
      },
      "outputs": [],
      "source": [
        "def vector_only_rag(query, vector_store, k=5):\n",
        "    \"\"\"\n",
        "    Answer a query using only vector-based RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Retrieve documents using vector-based similarity search\n",
        "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dD9i1B7CGWuk"
      },
      "outputs": [],
      "source": [
        "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
        "    \"\"\"\n",
        "    Answer a query using only BM25-based RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results\n",
        "    \"\"\"\n",
        "    # Retrieve documents using BM25 search\n",
        "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu0DHIeLGWuk"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zDmRFrJGGWul"
      },
      "outputs": [],
      "source": [
        "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Compare different retrieval methods for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores in fusion retrieval\n",
        "        reference_answer (str, optional): Reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        Dict: Comparison results\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
        "\n",
        "    # Run vector-only RAG\n",
        "    print(\"\\nRunning vector-only RAG...\")\n",
        "    vector_result = vector_only_rag(query, vector_store, k)\n",
        "\n",
        "    # Run BM25-only RAG\n",
        "    print(\"\\nRunning BM25-only RAG...\")\n",
        "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
        "\n",
        "    # Run fusion RAG\n",
        "    print(\"\\nRunning fusion RAG...\")\n",
        "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
        "\n",
        "    # Compare responses from different retrieval methods\n",
        "    print(\"\\nComparing responses...\")\n",
        "    comparison = evaluate_responses(\n",
        "        query,\n",
        "        vector_result[\"response\"],\n",
        "        bm25_result[\"response\"],\n",
        "        fusion_result[\"response\"],\n",
        "        reference_answer\n",
        "    )\n",
        "\n",
        "    # Return the comparison results\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"vector_result\": vector_result,\n",
        "        \"bm25_result\": bm25_result,\n",
        "        \"fusion_result\": fusion_result,\n",
        "        \"comparison\": comparison\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5kxu4ozbGWul"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluate the responses from different retrieval methods.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_response (str): Response from vector-only RAG\n",
        "        bm25_response (str): Response from BM25-only RAG\n",
        "        fusion_response (str): Response from fusion RAG\n",
        "        reference_answer (str, optional): Reference answer\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation of responses\n",
        "    \"\"\"\n",
        "    # System prompt for the evaluator to guide the evaluation process\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
        "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
        "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
        "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
        "\n",
        "    Evaluate the responses based on:\n",
        "    - Relevance to the query\n",
        "    - Factual correctness\n",
        "    - Comprehensiveness\n",
        "    - Clarity and coherence\"\"\"\n",
        "\n",
        "    # User prompt containing the query and responses\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "    Vector-based response:\n",
        "    {vector_response}\n",
        "\n",
        "    BM25 keyword response:\n",
        "    {bm25_response}\n",
        "\n",
        "    Fusion response:\n",
        "    {fusion_response}\n",
        "    \"\"\"\n",
        "\n",
        "    # Add reference answer to the prompt if provided\n",
        "    if reference_answer:\n",
        "        user_prompt += f\"\"\"\n",
        "            Reference answer:\n",
        "            {reference_answer}\n",
        "        \"\"\"\n",
        "\n",
        "    # Add instructions for detailed comparison to the user prompt\n",
        "    user_prompt += \"\"\"\n",
        "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
        "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Specify the model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
        "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
        "        ],\n",
        "        temperature=0  # Set the temperature for response generation\n",
        "    )\n",
        "\n",
        "    # Return the generated evaluation content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_VRgK4GWul"
      },
      "source": [
        "## Complete Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SdjsEw0jGWul"
      },
      "outputs": [],
      "source": [
        "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate fusion retrieval compared to other methods.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        test_queries (List[str]): List of test queries\n",
        "        reference_answers (List[str], optional): Reference answers\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores in fusion retrieval\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
        "\n",
        "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
        "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
        "\n",
        "    # Initialize a list to store results for each query\n",
        "    results = []\n",
        "\n",
        "    # Iterate over each test query\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        # Get the reference answer if available\n",
        "        reference = None\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            reference = reference_answers[i]\n",
        "\n",
        "        # Compare retrieval methods for the current query\n",
        "        comparison = compare_retrieval_methods(\n",
        "            query,\n",
        "            chunks,\n",
        "            vector_store,\n",
        "            bm25_index,\n",
        "            k=k,\n",
        "            alpha=alpha,\n",
        "            reference_answer=reference\n",
        "        )\n",
        "\n",
        "        # Append the comparison results to the results list\n",
        "        results.append(comparison)\n",
        "\n",
        "        # Print the responses from different retrieval methods\n",
        "        print(\"\\n=== Vector-based Response ===\")\n",
        "        print(comparison[\"vector_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== BM25 Response ===\")\n",
        "        print(comparison[\"bm25_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== Fusion Response ===\")\n",
        "        print(comparison[\"fusion_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== Comparison ===\")\n",
        "        print(comparison[\"comparison\"])\n",
        "\n",
        "    # Generate an overall analysis of the fusion retrieval performance\n",
        "    overall_analysis = generate_overall_analysis(results)\n",
        "\n",
        "    # Return the results and overall analysis\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"overall_analysis\": overall_analysis\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "O0S6OE6YGWul"
      },
      "outputs": [],
      "source": [
        "def generate_overall_analysis(results):\n",
        "    \"\"\"\n",
        "    Generate an overall analysis of fusion retrieval.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results from evaluating queries\n",
        "\n",
        "    Returns:\n",
        "        str: Overall analysis\n",
        "    \"\"\"\n",
        "    # System prompt to guide the evaluation process\n",
        "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
        "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
        "    1. Vector-based retrieval (semantic similarity)\n",
        "    2. BM25 keyword retrieval (keyword matching)\n",
        "    3. Fusion retrieval (combination of both)\n",
        "\n",
        "    Focus on:\n",
        "    1. Types of queries where each approach performs best\n",
        "    2. Overall strengths and weaknesses of each approach\n",
        "    3. How fusion retrieval balances the trade-offs\n",
        "    4. Recommendations for when to use each approach\"\"\"\n",
        "\n",
        "    # Create a summary of evaluations for each query\n",
        "    evaluations_summary = \"\"\n",
        "    for i, result in enumerate(results):\n",
        "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
        "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
        "\n",
        "    # User prompt containing the evaluations summary\n",
        "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries,\n",
        "    provide an overall analysis comparing these three approaches:\n",
        "\n",
        "    {evaluations_summary}\n",
        "\n",
        "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
        "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
        "\n",
        "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated analysis content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j1b5uEIGWul"
      },
      "source": [
        "## Evaluating Fusion Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1jO1XW4GWul",
        "outputId": "e03fbb95-82a7-4818-9ee1-3722347e2e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EVALUATING FUSION RETRIEVAL ===\n",
            "\n",
            "Extracting text from AI_Information.pdf...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 items to vector store\n",
            "Created BM25 index with 42 documents\n",
            "\n",
            "\n",
            "=== Evaluating Query 1/1 ===\n",
            "Query: What are the main applications of transformer models in natural language processing?\n",
            "\n",
            "=== Comparing retrieval methods for query: What are the main applications of transformer models in natural language processing? ===\n",
            "\n",
            "\n",
            "Running vector-only RAG...\n",
            "\n",
            "Running BM25-only RAG...\n",
            "\n",
            "Running fusion RAG...\n",
            "Performing fusion retrieval for query: What are the main applications of transformer models in natural language processing?\n",
            "Retrieved 5 documents with fusion retrieval\n",
            "\n",
            "Comparing responses...\n",
            "\n",
            "=== Vector-based Response ===\n",
            "Acknowledging the limitation that the context provided does not specifically mention transformer models, I cannot provide information on the main applications of transformer models in natural language processing. Transformer models are a type of deep learning model that has gained significant popularity in NLP tasks due to their ability to handle long-range dependencies effectively. Some common applications of transformer models in NLP include machine translation, text generation, sentiment analysis, and language understanding tasks.\n",
            "\n",
            "=== BM25 Response ===\n",
            "The provided context does not specifically mention transformer models in natural language processing. Transformer models are a type of deep learning model that has gained significant popularity in NLP tasks due to their ability to handle long-range dependencies effectively. Transformer models are commonly used in tasks such as language translation, text generation, sentiment analysis, and document summarization.\n",
            "\n",
            "=== Fusion Response ===\n",
            "Acknowledging the limitation that the context provided does not specifically mention transformer models, I cannot provide information on the main applications of transformer models in natural language processing. Transformer models are a type of deep learning model that has gained popularity in NLP tasks due to their ability to handle long-range dependencies effectively. Transformer models are commonly used in tasks such as machine translation, text generation, sentiment analysis, and language understanding.\n",
            "\n",
            "=== Comparison ===\n",
            "**Vector-based response:**\n",
            "- **Relevance to the query:** The response is relevant to the query by mentioning the main applications of transformer models in NLP.\n",
            "- **Factual correctness:** The response correctly identifies some common applications of transformer models in NLP.\n",
            "- **Comprehensiveness:** The response covers several main applications of transformer models in NLP.\n",
            "- **Clarity and coherence:** The response is clear and coherent in presenting the information.\n",
            "\n",
            "**BM25 keyword response:**\n",
            "- **Relevance to the query:** The response is relevant to the query by discussing the applications of transformer models in NLP.\n",
            "- **Factual correctness:** The response correctly identifies common applications of transformer models in NLP.\n",
            "- **Comprehensiveness:** The response covers some main applications of transformer models in NLP.\n",
            "- **Clarity and coherence:** The response is clear and coherent in presenting the information.\n",
            "\n",
            "**Fusion response:**\n",
            "- **Relevance to the query:** The response is relevant to the query by discussing the applications of transformer models in NLP.\n",
            "- **Factual correctness:** The response correctly identifies some common applications of transformer models in NLP.\n",
            "- **Comprehensiveness:** The response covers several main applications of transformer models in NLP.\n",
            "- **Clarity and coherence:** The response is clear and coherent in presenting the information.\n",
            "\n",
            "**Comparison:**\n",
            "- All three responses are relevant to the query and provide factual information about the applications of transformer models in NLP.\n",
            "- The vector-based response and the fusion response are more comprehensive as they cover a wider range of applications of transformer models in NLP compared to the BM25 keyword response.\n",
            "- In terms of clarity and coherence, all three responses are well-structured and easy to understand.\n",
            "\n",
            "**Best Approach:**\n",
            "- In this particular query, the fusion retrieval approach performed the best as it combined both vector-based semantic similarity and keyword matching approaches, resulting in a more comprehensive coverage of the main applications of transformer models in NLP.\n",
            "- The fusion approach leveraged the strengths of both vector-based retrieval and keyword retrieval to provide a more complete and accurate response to the query.\n",
            "- While the vector-based and BM25 keyword approaches were also effective, the fusion approach excelled in terms of comprehensiveness by incorporating both semantic similarity and keyword matching techniques.\n",
            "\n",
            "\n",
            "=== OVERALL ANALYSIS ===\n",
            "\n",
            "**Vector-based Retrieval (Semantic Similarity):**\n",
            "- *Types of Queries where it performs best:* Vector-based retrieval excels in queries that require understanding of the underlying semantic meaning and context. It is effective for queries that involve natural language understanding and require matching the meaning of the query with the content of the documents.\n",
            "- *Strengths:*\n",
            "    - Utilizes semantic similarity to match query and document representations.\n",
            "    - Effective in capturing the context and meaning of the query.\n",
            "    - Can handle synonyms and related terms well.\n",
            "- *Weaknesses:*\n",
            "    - May struggle with out-of-vocabulary terms.\n",
            "    - Requires a large amount of training data for accurate representation learning.\n",
            "    - Sensitivity to noise in the data can affect performance.\n",
            "\n",
            "**BM25 Keyword Retrieval (Keyword Matching):**\n",
            "- *Types of Queries where it performs best:* BM25 keyword retrieval is most effective for queries that rely heavily on specific keywords or terms. It is suitable for queries where the focus is on matching the exact terms present in the query with the documents.\n",
            "- *Strengths:*\n",
            "    - Efficient in handling keyword-based queries.\n",
            "    - Robust to noise and irrelevant terms.\n",
            "    - Effective in scenarios where the query terms are well-defined.\n",
            "- *Weaknesses:*\n",
            "    - Limited in capturing semantic relationships between terms.\n",
            "    - May struggle with synonymy and polysemy.\n",
            "    - Less effective for queries requiring understanding of context.\n",
            "\n",
            "**Fusion Retrieval (Combination of both):**\n",
            "- *Advantages over Individual Methods:*\n",
            "    - **Balanced Approach:** Fusion retrieval combines the strengths of both semantic similarity and keyword matching. It leverages semantic understanding from vector-based retrieval and the efficiency of keyword matching from BM25.\n",
            "    - **Improved Relevance:** By combining both approaches, fusion retrieval can provide more relevant results by considering both semantic relevance and keyword matching.\n",
            "    - **Robustness:** Fusion retrieval can handle a wider range of queries by balancing the trade-offs between semantic understanding and keyword matching.\n",
            "    - **Enhanced Recall and Precision:** The fusion of different retrieval methods can lead to improved recall and precision by leveraging the strengths of each approach.\n",
            "  \n",
            "**Recommendations:**\n",
            "- **Vector-based Retrieval:** Use this approach for queries that require understanding of context, semantics, and natural language understanding.\n",
            "- **BM25 Keyword Retrieval:** Opt for this approach for queries that are keyword-focused and where exact term matching is crucial.\n",
            "- **Fusion Retrieval:** Consider fusion retrieval for a more comprehensive and balanced retrieval approach, especially for queries that can benefit from both semantic relevance and keyword matching. Fusion retrieval is particularly useful when dealing with diverse query types and when aiming for improved overall retrieval performance.\n"
          ]
        }
      ],
      "source": [
        "# Path to PDF document\n",
        "# Path to PDF document containing AI information for knowledge retrieval testing\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Define a single AI-related test query\n",
        "test_queries = [\n",
        "    \"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n",
        "]\n",
        "\n",
        "# Optional reference answer\n",
        "reference_answers = [\n",
        "    \"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n",
        "]\n",
        "\n",
        "# Set parameters\n",
        "k = 5  # Number of documents to retrieve\n",
        "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_fusion_retrieval(\n",
        "    pdf_path=pdf_path,\n",
        "    test_queries=test_queries,\n",
        "    reference_answers=reference_answers,\n",
        "    k=k,\n",
        "    alpha=alpha\n",
        ")\n",
        "\n",
        "# Print overall analysis\n",
        "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
        "print(evaluation_results[\"overall_analysis\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}